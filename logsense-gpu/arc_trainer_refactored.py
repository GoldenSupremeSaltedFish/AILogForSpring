#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Intel Arc GPU è®­ç»ƒå™¨ - é‡æ„ç‰ˆæœ¬
ä½¿ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæé«˜ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
"""

import torch
import torch.nn as nn
import logging
import os
from datetime import datetime
from typing import Dict, Any, Optional

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from arc_models import ModelFactory
from arc_data import DataLoaderFactory, LogPreprocessor
from arc_utils import ArcGPUDetector, TrainerUtils, ModelSaver, MetricsCalculator

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ArcTrainer:
    """Intel Arc GPU è®­ç»ƒå™¨ - é‡æ„ç‰ˆæœ¬"""
    
    def __init__(self, model_type: str = "textcnn", config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('textcnn', 'fasttext')
            config: è®­ç»ƒé…ç½®
        """
        self.model_type = model_type
        self.config = config or self._get_default_config()
        self.device = ArcGPUDetector.get_device()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.label_encoder = None
        self.trainer_utils = TrainerUtils()
        self.model_saver = ModelSaver()
        self.metrics_calculator = MetricsCalculator()
        self.preprocessor = LogPreprocessor()
        
        logger.info(f"ğŸ¯ åˆå§‹åŒ–è®­ç»ƒå™¨ - æ¨¡å‹ç±»å‹: {model_type}")
        logger.info(f"ğŸ–¥ï¸  è®¡ç®—è®¾å¤‡: {self.device}")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'model_config': ModelFactory.get_default_config(self.model_type),
            'training_config': {
                'batch_size': 32,
                'epochs': 10,
                'learning_rate': 0.001,
                'max_length': 128,
                'vocab_size': 10000,
                'test_size': 0.2,
                'val_size': 0.2,
                'random_state': 42
            },
            'data_config': {
                'text_column': 'message',
                'label_column': 'category',
                'normalize_text': True
            }
        }
    
    def load_and_preprocess_data(self, data_path: str) -> tuple:
        """
        åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        Returns:
            è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®åŠ è½½å™¨å’Œæ ‡ç­¾ç¼–ç å™¨
        """
        logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")
        
        # åŠ è½½åŸå§‹æ•°æ®
        texts, labels, label_encoder = DataLoaderFactory.load_csv_data(
            data_path, 
            self.config['data_config']['text_column'],
            self.config['data_config']['label_column']
        )
        
        # é¢„å¤„ç†æ–‡æœ¬
        if self.config['data_config']['normalize_text']:
            texts = self.preprocessor.process_batch(texts, normalize=True)
            logger.info("âœ… æ–‡æœ¬é¢„å¤„ç†å®Œæˆ")
        
        # åˆ†å‰²æ•°æ®
        train_texts, val_texts, test_texts, train_labels, val_labels, test_labels = \
            DataLoaderFactory.split_data(
                texts, labels,
                test_size=self.config['training_config']['test_size'],
                val_size=self.config['training_config']['val_size'],
                random_state=self.config['training_config']['random_state']
            )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = DataLoaderFactory.create_data_loaders(
            train_texts, val_texts, test_texts,
            train_labels, val_labels, test_labels,
            batch_size=self.config['training_config']['batch_size'],
            max_length=self.config['training_config']['max_length'],
            vocab_size=self.config['training_config']['vocab_size']
        )
        
        # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
        self.label_encoder = label_encoder
        
        # æ‰“å°æ•°æ®é›†ä¿¡æ¯
        dataset_info = DataLoaderFactory.get_dataset_info(train_loader, val_loader, test_loader)
        logger.info(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯: {dataset_info}")
        
        return train_loader, val_loader, test_loader
    
    def create_model(self, num_classes: int):
        """åˆ›å»ºæ¨¡å‹"""
        logger.info(f"ğŸ—ï¸ åˆ›å»ºæ¨¡å‹: {self.model_type}")
        
        # æ›´æ–°æ¨¡å‹é…ç½®
        model_config = self.config['model_config'].copy()
        model_config['num_classes'] = num_classes
        
        # åˆ›å»ºæ¨¡å‹
        self.model = ModelFactory.create_model(self.model_type, **model_config)
        self.model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        param_count = self.model.count_parameters()
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ - å‚æ•°æ•°é‡: {param_count:,}")
        logger.info(f"ğŸ“‹ æ¨¡å‹é…ç½®: {self.model.get_config()}")
    
    def train(self, data_path: str, save_dir: str = "results/models"):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹")
        
        # æ£€æŸ¥GPUçŠ¶æ€
        ArcGPUDetector.print_gpu_status()
        
        # åŠ è½½æ•°æ®
        train_loader, val_loader, test_loader = self.load_and_preprocess_data(data_path)
        
        # åˆ›å»ºæ¨¡å‹
        num_classes = len(self.label_encoder)
        self.create_model(num_classes)
        
        # è®¾ç½®è®­ç»ƒç»„ä»¶
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(
            self.model.parameters(), 
            lr=self.config['training_config']['learning_rate']
        )
        
        # è®­ç»ƒå¾ªç¯
        best_val_acc = 0
        training_history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': []
        }
        
        for epoch in range(self.config['training_config']['epochs']):
            logger.info(f"ğŸ“ˆ Epoch {epoch+1}/{self.config['training_config']['epochs']}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.trainer_utils.train_epoch(
                self.model, train_loader, criterion, optimizer, self.device
            )
            
            # éªŒè¯
            val_loss, val_acc = self.trainer_utils.validate_epoch(
                self.model, val_loader, criterion, self.device
            )
            
            # è®°å½•å†å²
            training_history['train_loss'].append(train_loss)
            training_history['train_acc'].append(train_acc)
            training_history['val_loss'].append(val_loss)
            training_history['val_acc'].append(val_acc)
            
            # æ‰“å°è¿›åº¦
            logger.info(f"  è®­ç»ƒ: æŸå¤±={train_loss:.4f}, å‡†ç¡®ç‡={train_acc:.2f}%")
            logger.info(f"  éªŒè¯: æŸå¤±={val_loss:.4f}, å‡†ç¡®ç‡={val_acc:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                self.model_saver.save_model(
                    self.model, self.label_encoder, save_dir, "best",
                    model_type=self.model_type, epoch=epoch+1, val_acc=val_acc
                )
                logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%)")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.model_saver.save_model(
            self.model, self.label_encoder, save_dir, "final",
            model_type=self.model_type, epoch=self.config['training_config']['epochs']
        )
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.metrics_calculator.plot_training_curves(training_history, save_dir)
        
        # æµ‹è¯•é›†è¯„ä¼°
        test_loss, test_acc = self.trainer_utils.validate_epoch(
            self.model, test_loader, criterion, self.device
        )
        logger.info(f"ğŸ§ª æµ‹è¯•é›†è¯„ä¼°: æŸå¤±={test_loss:.4f}, å‡†ç¡®ç‡={test_acc:.2f}%")
        
        logger.info(f"âœ… è®­ç»ƒå®Œæˆ - æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        
        return {
            'best_val_acc': best_val_acc,
            'test_acc': test_acc,
            'training_history': training_history
        }


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ Intel Arc GPU è®­ç»ƒå™¨ - é‡æ„ç‰ˆæœ¬")
    
    # æ£€æŸ¥GPU
    if not ArcGPUDetector.check_arc_gpu():
        logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°Intel Arc GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ArcTrainer(model_type="textcnn")
    
    # è®­ç»ƒå‚æ•°
    data_path = "DATA_OUTPUT/processed_logs.csv"  # æ ¹æ®å®é™…æ•°æ®è·¯å¾„è°ƒæ•´
    save_dir = "results/models"
    
    # å¼€å§‹è®­ç»ƒ
    results = trainer.train(data_path, save_dir)
    
    # æ‰“å°æœ€ç»ˆç»“æœ
    logger.info("ğŸ“Š è®­ç»ƒç»“æœæ€»ç»“:")
    logger.info(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {results['best_val_acc']:.2f}%")
    logger.info(f"   æµ‹è¯•é›†å‡†ç¡®ç‡: {results['test_acc']:.2f}%")


if __name__ == "__main__":
    main() 