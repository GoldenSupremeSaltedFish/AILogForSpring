#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯„ä¼°å·¥å…·æ¨¡å—
"""

from sklearn.metrics import (
    classification_report, accuracy_score, confusion_matrix,
    precision_score, recall_score, f1_score
)
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
from datetime import datetime
import json

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self):
        # ä¼ä¸šçº§ç›®æ ‡æŒ‡æ ‡
        self.enterprise_targets = {
            'accuracy': 0.90,
            'macro_f1': 0.88,
            'weighted_f1': 0.90,
            'recall': 0.85,
            'precision': 0.90
        }
        self.results = {}
    
    def evaluate_model(self, y_test, y_pred, y_pred_proba, label_encoder):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print("=" * 60)
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        precision_macro = precision_score(y_test, y_pred, average='macro')
        recall_macro = recall_score(y_test, y_pred, average='macro')
        f1_macro = f1_score(y_test, y_pred, average='macro')
        
        precision_weighted = precision_score(y_test, y_pred, average='weighted')
        recall_weighted = recall_score(y_test, y_pred, average='weighted')
        f1_weighted = f1_score(y_test, y_pred, average='weighted')
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        precision_per_class = precision_score(y_test, y_pred, average=None)
        recall_per_class = recall_score(y_test, y_pred, average=None)
        f1_per_class = f1_score(y_test, y_pred, average=None)
        
        # å­˜å‚¨ç»“æœ
        self.results = {
            'accuracy': accuracy,
            'precision_macro': precision_macro,
            'recall_macro': recall_macro,
            'f1_macro': f1_macro,
            'precision_weighted': precision_weighted,
            'recall_weighted': recall_weighted,
            'f1_weighted': f1_weighted,
            'precision_per_class': precision_per_class,
            'recall_per_class': recall_per_class,
            'f1_per_class': f1_per_class,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba,
            'label_encoder': label_encoder
        }
        
        # è¾“å‡ºä¼ä¸šçº§è¯„ä¼°è¡¨æ ¼
        self._print_enterprise_metrics()
        
        # è¾“å‡ºè¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print("\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))
        
        return self.results
    
    def _print_enterprise_metrics(self):
        """æ‰“å°ä¼ä¸šçº§è¯„ä¼°æŒ‡æ ‡"""
        print("\nğŸ“Š ä¼ä¸šçº§è¯„ä¼°æŒ‡æ ‡")
        print("=" * 80)
        print("| æŒ‡æ ‡                 | å½“å‰å€¼           | ä¼ä¸šçº§ç›®æ ‡         | çŠ¶æ€    |")
        print("| ------------------ | ------------ | ----------------- | ------ |")
        
        metrics = [
            ('Accuracy', self.results['accuracy'], self.enterprise_targets['accuracy']),
            ('Macro F1', self.results['f1_macro'], self.enterprise_targets['macro_f1']),
            ('Weighted F1', self.results['f1_weighted'], self.enterprise_targets['weighted_f1']),
            ('Macro Recall', self.results['recall_macro'], self.enterprise_targets['recall']),
            ('Macro Precision', self.results['precision_macro'], self.enterprise_targets['precision'])
        ]
        
        for metric_name, current_value, target_value in metrics:
            status = "âœ… ä¼˜ç§€" if current_value >= target_value else "âš ï¸ éœ€æ”¹è¿›"
            print(f"| {metric_name:<20} | {current_value:.4f}         | {target_value:.2f}            | {status:<6} |")
        
        print("=" * 80)
        
        # è®¡ç®—æ€»ä½“è¾¾æ ‡æƒ…å†µ
        achieved_count = sum(1 for _, current, target in metrics if current >= target)
        total_count = len(metrics)
        print(f"\nğŸ¯ æ€»ä½“è¾¾æ ‡æƒ…å†µ: {achieved_count}/{total_count} é¡¹æŒ‡æ ‡è¾¾åˆ°ä¼ä¸šçº§æ ‡å‡†")
        
        if achieved_count == total_count:
            print("ğŸ‰ æ­å–œï¼æ‰€æœ‰æŒ‡æ ‡éƒ½è¾¾åˆ°äº†ä¼ä¸šçº§æ ‡å‡†ï¼")
        elif achieved_count >= total_count * 0.8:
            print("ğŸ‘ è¡¨ç°è‰¯å¥½ï¼Œå¤§éƒ¨åˆ†æŒ‡æ ‡è¾¾åˆ°ä¼ä¸šçº§æ ‡å‡†")
        else:
            print("ğŸ’¡ å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹å‚æ•°æˆ–å¢åŠ è®­ç»ƒæ•°æ®")
    
    def save_results(self, output_dir: str = "logsense-gpu/results"):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜ç»“æœ
        results_file = output_path / f"enhanced_results_{timestamp}.json"
        
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        results_for_json = {}
        for key, value in self.results.items():
            if key == 'label_encoder':
                continue  # è·³è¿‡label_encoder
            if isinstance(value, np.ndarray):
                results_for_json[key] = value.tolist()
            else:
                results_for_json[key] = value
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_for_json, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_plots(output_path, timestamp)
    
    def _generate_plots(self, output_path: Path, timestamp: str):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        plots_dir = output_path / "plots"
        plots_dir.mkdir(exist_ok=True)
        
        label_encoder = self.results['label_encoder']
        
        # æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(12, 10))
        cm = confusion_matrix(self.results['y_test'], self.results['y_pred'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=label_encoder.classes_,
                   yticklabels=label_encoder.classes_)
        plt.title('æ··æ·†çŸ©é˜µ - å¢å¼ºç‰ˆæ¨¡å‹')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.tight_layout()
        plt.savefig(plots_dir / f"confusion_matrix_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # F1åˆ†æ•°å¯¹æ¯”
        plt.figure(figsize=(12, 8))
        classes = label_encoder.classes_
        f1_scores = self.results['f1_per_class']
        
        bars = plt.bar(range(len(classes)), f1_scores, color='skyblue', alpha=0.7)
        plt.xlabel('ç±»åˆ«')
        plt.ylabel('F1åˆ†æ•°')
        plt.title('å„ç±»åˆ«F1åˆ†æ•° - å¢å¼ºç‰ˆæ¨¡å‹')
        plt.xticks(range(len(classes)), classes, rotation=45, ha='right')
        plt.ylim(0, 1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, score) in enumerate(zip(bars, f1_scores)):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(plots_dir / f"f1_scores_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜åˆ°: {plots_dir}") 