#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šå¹³å°è®¡ç®—æ”¯æŒå·¥å…·
åŠŸèƒ½ï¼š
1. æ£€æµ‹å’Œé…ç½®GPU/CPUç¯å¢ƒ
2. è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¡ç®—è®¾å¤‡
3. å†…å­˜å’Œæ€§èƒ½ç›‘æ§
4. è·¨å¹³å°å…¼å®¹æ€§æ”¯æŒ
"""

import os
import platform
import psutil
from typing import Dict, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')


class PlatformDetector:
    """å¹³å°æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.system_info = self._get_system_info()
        self.gpu_info = self._detect_gpu()
        self.memory_info = self._get_memory_info()
        self.cpu_info = self._get_cpu_info()
    
    def _get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            'platform': platform.system(),
            'platform_version': platform.version(),
            'architecture': platform.architecture()[0],
            'processor': platform.processor(),
            'python_version': platform.python_version()
        }
    
    def _detect_gpu(self) -> Dict:
        """æ£€æµ‹GPUä¿¡æ¯"""
        gpu_info = {
            'available': False,
            'type': None,
            'name': None,
            'memory': None,
            'count': 0
        }
        
        # æ£€æµ‹NVIDIA GPU
        try:
            import torch
            if torch.cuda.is_available():
                gpu_info['available'] = True
                gpu_info['type'] = 'NVIDIA'
                gpu_info['name'] = torch.cuda.get_device_name(0)
                gpu_info['memory'] = torch.cuda.get_device_properties(0).total_memory
                gpu_info['count'] = torch.cuda.device_count()
                print(f"âœ… æ£€æµ‹åˆ°NVIDIA GPU: {gpu_info['name']}")
                print(f"   GPUå†…å­˜: {gpu_info['memory'] / 1024**3:.1f} GB")
                print(f"   GPUæ•°é‡: {gpu_info['count']}")
        except ImportError:
            print("âš ï¸  PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹NVIDIA GPU")
        except Exception as e:
            print(f"âš ï¸  GPUæ£€æµ‹å¤±è´¥: {e}")
        
        # æ£€æµ‹å…¶ä»–GPUç±»å‹
        if not gpu_info['available']:
            # å¯ä»¥æ·»åŠ å…¶ä»–GPUæ£€æµ‹é€»è¾‘
            pass
        
        return gpu_info
    
    def _get_memory_info(self) -> Dict:
        """è·å–å†…å­˜ä¿¡æ¯"""
        memory = psutil.virtual_memory()
        return {
            'total': memory.total,
            'available': memory.available,
            'used': memory.used,
            'percent': memory.percent
        }
    
    def _get_cpu_info(self) -> Dict:
        """è·å–CPUä¿¡æ¯"""
        return {
            'count': psutil.cpu_count(),
            'count_logical': psutil.cpu_count(logical=True),
            'freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None,
            'usage': psutil.cpu_percent(interval=1)
        }
    
    def print_system_info(self):
        """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        print("ğŸ–¥ï¸ ç³»ç»Ÿä¿¡æ¯")
        print("=" * 50)
        print(f"æ“ä½œç³»ç»Ÿ: {self.system_info['platform']} {self.system_info['platform_version']}")
        print(f"æ¶æ„: {self.system_info['architecture']}")
        print(f"å¤„ç†å™¨: {self.system_info['processor']}")
        print(f"Pythonç‰ˆæœ¬: {self.system_info['python_version']}")
        
        print(f"\nğŸ’¾ å†…å­˜ä¿¡æ¯")
        print(f"æ€»å†…å­˜: {self.memory_info['total'] / 1024**3:.1f} GB")
        print(f"å¯ç”¨å†…å­˜: {self.memory_info['available'] / 1024**3:.1f} GB")
        print(f"å†…å­˜ä½¿ç”¨ç‡: {self.memory_info['percent']:.1f}%")
        
        print(f"\nğŸ–¥ï¸ CPUä¿¡æ¯")
        print(f"ç‰©ç†æ ¸å¿ƒæ•°: {self.cpu_info['count']}")
        print(f"é€»è¾‘æ ¸å¿ƒæ•°: {self.cpu_info['count_logical']}")
        print(f"CPUä½¿ç”¨ç‡: {self.cpu_info['usage']:.1f}%")
        
        if self.gpu_info['available']:
            print(f"\nğŸ® GPUä¿¡æ¯")
            print(f"ç±»å‹: {self.gpu_info['type']}")
            print(f"åç§°: {self.gpu_info['name']}")
            print(f"GPUå†…å­˜: {self.gpu_info['memory'] / 1024**3:.1f} GB")
            print(f"GPUæ•°é‡: {self.gpu_info['count']}")
        else:
            print(f"\nâš ï¸  GPUä¸å¯ç”¨")
    
    def get_optimal_device(self) -> str:
        """è·å–æœ€ä¼˜è®¡ç®—è®¾å¤‡"""
        if self.gpu_info['available']:
            # æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿ
            gpu_memory_gb = self.gpu_info['memory'] / 1024**3
            if gpu_memory_gb >= 4:  # è‡³å°‘4GB GPUå†…å­˜
                return 'gpu'
            else:
                print(f"âš ï¸  GPUå†…å­˜ä¸è¶³ ({gpu_memory_gb:.1f} GB < 4 GB)ï¼Œä½¿ç”¨CPU")
                return 'cpu'
        else:
            return 'cpu'
    
    def get_recommended_batch_size(self, model_type: str = 'default') -> int:
        """è·å–æ¨èçš„æ‰¹å¤„ç†å¤§å°"""
        if self.gpu_info['available']:
            gpu_memory_gb = self.gpu_info['memory'] / 1024**3
            if gpu_memory_gb >= 8:
                return 64
            elif gpu_memory_gb >= 4:
                return 32
            else:
                return 16
        else:
            # CPUæ¨¡å¼
            cpu_count = self.cpu_info['count']
            if cpu_count >= 8:
                return 16
            elif cpu_count >= 4:
                return 8
            else:
                return 4


class ModelOptimizer:
    """æ¨¡å‹ä¼˜åŒ–å™¨"""
    
    def __init__(self, platform_detector: PlatformDetector):
        self.platform = platform_detector
        self.device = platform_detector.get_optimal_device()
    
    def optimize_for_platform(self, model_config: Dict) -> Dict:
        """æ ¹æ®å¹³å°ä¼˜åŒ–æ¨¡å‹é…ç½®"""
        optimized_config = model_config.copy()
        
        if self.device == 'gpu':
            # GPUä¼˜åŒ–é…ç½®
            optimized_config.update({
                'device': 'gpu',
                'batch_size': self.platform.get_recommended_batch_size(),
                'use_mixed_precision': True,
                'num_workers': min(4, self.platform.cpu_info['count'])
            })
        else:
            # CPUä¼˜åŒ–é…ç½®
            optimized_config.update({
                'device': 'cpu',
                'batch_size': self.platform.get_recommended_batch_size(),
                'use_mixed_precision': False,
                'num_workers': min(2, self.platform.cpu_info['count'])
            })
        
        return optimized_config
    
    def get_training_config(self, model_type: str = 'gradient_boosting') -> Dict:
        """è·å–è®­ç»ƒé…ç½®"""
        base_config = {
            'model_type': model_type,
            'random_state': 42,
            'test_size': 0.2,
            'stratify': True
        }
        
        if model_type == 'gradient_boosting':
            base_config.update({
                'n_estimators': 100,
                'learning_rate': 0.1,
                'max_depth': 6
            })
        elif model_type == 'lightgbm':
            base_config.update({
                'n_estimators': 100,
                'learning_rate': 0.1,
                'max_depth': 6,
                'verbose': -1
            })
        
        return self.optimize_for_platform(base_config)


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.start_time = None
        self.memory_usage = []
        self.cpu_usage = []
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        import time
        self.start_time = time.time()
        self.memory_usage = []
        self.cpu_usage = []
    
    def record_metrics(self):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        if self.start_time is None:
            return
        
        # è®°å½•å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        self.memory_usage.append({
            'timestamp': time.time() - self.start_time,
            'used': memory.used,
            'percent': memory.percent
        })
        
        # è®°å½•CPUä½¿ç”¨
        cpu_percent = psutil.cpu_percent(interval=0.1)
        self.cpu_usage.append({
            'timestamp': time.time() - self.start_time,
            'usage': cpu_percent
        })
    
    def get_summary(self) -> Dict:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.memory_usage:
            return {}
        
        total_time = time.time() - self.start_time
        avg_memory = sum(m['percent'] for m in self.memory_usage) / len(self.memory_usage)
        avg_cpu = sum(c['usage'] for c in self.cpu_usage) / len(self.cpu_usage)
        max_memory = max(m['percent'] for m in self.memory_usage)
        max_cpu = max(c['usage'] for c in self.cpu_usage)
        
        return {
            'total_time': total_time,
            'avg_memory_usage': avg_memory,
            'avg_cpu_usage': avg_cpu,
            'max_memory_usage': max_memory,
            'max_cpu_usage': max_cpu
        }
    
    def print_summary(self):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        summary = self.get_summary()
        if not summary:
            return
        
        print("\nğŸ“Š æ€§èƒ½ç›‘æ§æ‘˜è¦")
        print("=" * 30)
        print(f"æ€»è¿è¡Œæ—¶é—´: {summary['total_time']:.2f} ç§’")
        print(f"å¹³å‡å†…å­˜ä½¿ç”¨: {summary['avg_memory_usage']:.1f}%")
        print(f"å¹³å‡CPUä½¿ç”¨: {summary['avg_cpu_usage']:.1f}%")
        print(f"æœ€å¤§å†…å­˜ä½¿ç”¨: {summary['max_memory_usage']:.1f}%")
        print(f"æœ€å¤§CPUä½¿ç”¨: {summary['max_cpu_usage']:.1f}%")


def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    detector = PlatformDetector()
    detector.print_system_info()
    
    optimizer = ModelOptimizer(detector)
    config = optimizer.get_training_config()
    
    print(f"\nâš™ï¸ æ¨èé…ç½®")
    print("=" * 30)
    print(f"è®¡ç®—è®¾å¤‡: {config['device']}")
    print(f"æ‰¹å¤„ç†å¤§å°: {config['batch_size']}")
    print(f"å·¥ä½œè¿›ç¨‹æ•°: {config['num_workers']}")
    print(f"æ··åˆç²¾åº¦: {config['use_mixed_precision']}")
    
    return detector, optimizer, config


if __name__ == "__main__":
    setup_environment() 