#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Intel Arc GPU æ·±åº¦æ¨¡å‹è®­ç»ƒå™¨
æ”¯æŒTextCNNã€FastTextç­‰è½»é‡çº§æ¨¡å‹åœ¨Arc GPUä¸Šè®­ç»ƒ
"""

import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import joblib
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import logging
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ArcGPUDetector:
    """Intel Arc GPU æ£€æµ‹å™¨"""
    
    @staticmethod
    def check_arc_gpu():
        """æ£€æŸ¥Intel Arc GPUæ˜¯å¦å¯ç”¨"""
        try:
            import intel_extension_for_pytorch as ipex
            if torch.xpu.is_available():
                device_count = torch.xpu.device_count()
                device_name = torch.xpu.get_device_name(0)
                logger.info(f"âœ… æ£€æµ‹åˆ°Intel GPU: {device_name}")
                logger.info(f"   GPUæ•°é‡: {device_count}")
                return True
            else:
                logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°Intel XPUè®¾å¤‡")
                return False
        except ImportError:
            logger.error("âŒ Intel Extension for PyTorchæœªå®‰è£…")
            return False
        except Exception as e:
            logger.error(f"âŒ GPUæ£€æµ‹å¤±è´¥: {e}")
            return False
    
    @staticmethod
    def get_device():
        """è·å–æœ€ä½³è®¡ç®—è®¾å¤‡"""
        if ArcGPUDetector.check_arc_gpu():
            return torch.device("xpu:0")
        else:
            return torch.device("cpu")

class TextCNN(nn.Module):
    """TextCNNæ¨¡å‹ - é€‚åˆæ—¥å¿—åˆ†ç±»"""
    
    def __init__(self, vocab_size: int, embed_dim: int, num_classes: int, 
                 filter_sizes: List[int] = [3, 4, 5], num_filters: int = 128):
        super(TextCNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.convs = nn.ModuleList([
            nn.Conv1d(embed_dim, num_filters, kernel_size=k) 
            for k in filter_sizes
        ])
        self.dropout = nn.Dropout(0.5)
        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)
    
    def forward(self, x):
        embedded = self.embedding(x)
        embedded = embedded.permute(0, 2, 1)
        
        conv_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(embedded))
            pooled = F.max_pool1d(conv_out, conv_out.size(2))
            conv_outputs.append(pooled.squeeze(2))
        
        concatenated = torch.cat(conv_outputs, dim=1)
        dropped = self.dropout(concatenated)
        output = self.fc(dropped)
        return output

class LogDataset(Dataset):
    """æ—¥å¿—æ•°æ®é›†"""
    
    def __init__(self, texts: List[str], labels: List[int], max_length: int = 128):
        self.texts = texts
        self.labels = labels
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        # ç®€å•çš„åˆ†è¯å¤„ç†
        tokens = text.split()[:self.max_length]
        token_ids = [hash(token) % 10000 for token in tokens]
        if len(token_ids) < self.max_length:
            token_ids += [0] * (self.max_length - len(token_ids))
        else:
            token_ids = token_ids[:self.max_length]
        
        return {
            'input_ids': torch.tensor(token_ids, dtype=torch.long),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class ArcGPUTrainer:
    """Intel Arc GPU è®­ç»ƒå™¨"""
    
    def __init__(self, model_type: str = "textcnn", device: Optional[torch.device] = None):
        self.model_type = model_type
        self.device = device or ArcGPUDetector.get_device()
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.label_encoder = None
        self.vocab_size = 10000
        self.embed_dim = 128
        self.max_length = 128
        
        logger.info(f"ğŸ¯ åˆå§‹åŒ–è®­ç»ƒå™¨ - æ¨¡å‹ç±»å‹: {model_type}")
        logger.info(f"ğŸ–¥ï¸  è®¡ç®—è®¾å¤‡: {self.device}")
    
    def create_model(self, num_classes: int):
        """åˆ›å»ºæ¨¡å‹"""
        self.model = TextCNN(
            vocab_size=self.vocab_size,
            embed_dim=self.embed_dim,
            num_classes=num_classes
        )
        
        self.model.to(self.device)
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ - å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters())}")
    
    def load_data(self, data_path: str) -> Tuple[DataLoader, DataLoader, int]:
        """åŠ è½½æ•°æ®"""
        logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")
        
        df = pd.read_csv(data_path)
        texts = df['message'].fillna('').tolist()
        labels = df['category'].tolist()
        
        # æ ‡ç­¾ç¼–ç 
        unique_labels = sorted(list(set(labels)))
        label_to_id = {label: idx for idx, label in enumerate(unique_labels)}
        encoded_labels = [label_to_id[label] for label in labels]
        
        self.label_encoder = {idx: label for label, idx in label_to_id.items()}
        
        # åˆ†å‰²æ•°æ®
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels
        )
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = LogDataset(train_texts, train_labels, self.max_length)
        val_dataset = LogDataset(val_texts, val_labels, self.max_length)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        
        logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ - è®­ç»ƒæ ·æœ¬: {len(train_texts)}, éªŒè¯æ ·æœ¬: {len(val_texts)}")
        logger.info(f"ğŸ·ï¸  ç±»åˆ«æ•°é‡: {len(unique_labels)}")
        
        return train_loader, val_loader, len(unique_labels)
    
    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        progress_bar = tqdm(train_loader, desc="è®­ç»ƒä¸­")
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            self.optimizer.zero_grad()
            outputs = self.model(input_ids)
            loss = self.criterion(outputs, labels)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100 * correct / total:.2f}%'
            })
        
        return total_loss / len(train_loader), 100 * correct / total
    
    def validate(self, val_loader: DataLoader) -> Tuple[float, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        return total_loss / len(val_loader), 100 * correct / total
    
    def train(self, data_path: str, epochs: int = 10, save_dir: str = "results/models"):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹")
        
        # åŠ è½½æ•°æ®
        train_loader, val_loader, num_classes = self.load_data(data_path)
        
        # åˆ›å»ºæ¨¡å‹
        self.create_model(num_classes)
        
        # è®­ç»ƒå¾ªç¯
        best_val_acc = 0
        
        for epoch in range(epochs):
            logger.info(f"ğŸ“ˆ Epoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader)
            
            logger.info(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
            logger.info(f"  éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                self.save_model(save_dir, "best")
                logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%)")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model(save_dir, "final")
        
        logger.info(f"âœ… è®­ç»ƒå®Œæˆ - æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    
    def save_model(self, save_dir: str, suffix: str = ""):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(save_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜PyTorchæ¨¡å‹
        model_path = os.path.join(save_dir, f"arc_gpu_model_{self.model_type}_{suffix}_{timestamp}.pth")
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'model_type': self.model_type,
                'vocab_size': self.vocab_size,
                'embed_dim': self.embed_dim,
                'max_length': self.max_length
            },
            'label_encoder': self.label_encoder
        }, model_path)
        
        # ä¿å­˜ONNXæ¨¡å‹
        self.save_onnx_model(save_dir, timestamp)
        
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    def save_onnx_model(self, save_dir: str, timestamp: str):
        """ä¿å­˜ONNXæ¨¡å‹"""
        try:
            dummy_input = torch.randint(0, self.vocab_size, (1, self.max_length)).to(self.device)
            
            onnx_path = os.path.join(save_dir, f"arc_gpu_model_{self.model_type}_onnx_{timestamp}.onnx")
            torch.onnx.export(
                self.model,
                dummy_input,
                onnx_path,
                input_names=["input"],
                output_names=["output"],
                dynamic_axes={"input": [0, 1], "output": [0]},
                opset_version=11
            )
            logger.info(f"ğŸ’¾ ONNXæ¨¡å‹å·²ä¿å­˜: {onnx_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ ONNXå¯¼å‡ºå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ Intel Arc GPU æ·±åº¦æ¨¡å‹è®­ç»ƒå™¨")
    
    # æ£€æŸ¥GPU
    if not ArcGPUDetector.check_arc_gpu():
        logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°Intel Arc GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ArcGPUTrainer(model_type="textcnn")
    
    # è®­ç»ƒå‚æ•°
    data_path = "DATA_OUTPUT/processed_logs.csv"  # æ ¹æ®å®é™…æ•°æ®è·¯å¾„è°ƒæ•´
    epochs = 10
    save_dir = "results/models"
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(data_path, epochs, save_dir)

if __name__ == "__main__":
    main() 