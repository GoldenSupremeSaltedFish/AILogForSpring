#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆåŸºçº¿æ¨¡å‹è®­ç»ƒè„šæœ¬
åŠŸèƒ½ï¼šä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¾“å‡ºä¼ä¸šçº§è¯„ä¼°æŒ‡æ ‡
"""

import pandas as pd
import numpy as np
import argparse
import sys
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ ç›¸å…³å¯¼å…¥
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, accuracy_score, confusion_matrix,
    precision_score, recall_score, f1_score
)
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# å¯é€‰å¯¼å…¥
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False


class EnhancedBaselineModel:
    """å¢å¼ºç‰ˆåŸºçº¿æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.vectorizer = None
        self.model = None
        self.label_encoder = LabelEncoder()
        self.results = {}
        
        # ä¼ä¸šçº§ç›®æ ‡æŒ‡æ ‡
        self.enterprise_targets = {
            'accuracy': 0.90,
            'macro_f1': 0.88,
            'weighted_f1': 0.90,
            'recall': 0.85,
            'precision': 0.90
        }
    
    def load_and_prepare_data(self, data_file: str, sample_size: int = None):
        """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
        
        df = pd.read_csv(data_file)
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
        
        # æ£€æµ‹æ ‡ç­¾åˆ—å’Œæ–‡æœ¬åˆ—
        label_column = self._detect_label_column(df)
        text_column = self._detect_text_column(df)
        
        if not label_column or not text_column:
            raise ValueError("æœªæ‰¾åˆ°æ ‡ç­¾åˆ—æˆ–æ–‡æœ¬åˆ—")
        
        print(f"ğŸ” ä½¿ç”¨æ ‡ç­¾åˆ—: {label_column}")
        print(f"ğŸ” ä½¿ç”¨æ–‡æœ¬åˆ—: {text_column}")
        
        # è¿‡æ»¤æ•°æ®
        df_filtered = df[df[label_column] != 'other'].copy()
        print(f"ğŸ” è¿‡æ»¤åæ•°æ®: {len(df_filtered)} æ¡è®°å½•")
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        category_counts = df_filtered[label_column].value_counts()
        print("\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in category_counts.items():
            percentage = (count / len(df_filtered)) * 100
            print(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
        
        # å¦‚æœæŒ‡å®šäº†é‡‡æ ·å¤§å°ï¼Œè¿›è¡Œé‡‡æ ·
        if sample_size:
            print(f"\nğŸ¯ è¿›è¡Œé‡‡æ ·ï¼Œæ¯ç±»æœ€å¤š {sample_size} æ¡è®°å½•")
            sampled_data = []
            for category in df_filtered[label_column].unique():
                category_data = df_filtered[df_filtered[label_column] == category]
                if len(category_data) > sample_size:
                    category_data = category_data.sample(n=sample_size, random_state=42)
                sampled_data.append(category_data)
            
            df_filtered = pd.concat(sampled_data, ignore_index=True)
            print(f"ğŸ“Š é‡‡æ ·åæ•°æ®: {len(df_filtered)} æ¡è®°å½•")
        
        return df_filtered, label_column, text_column
    
    def _detect_label_column(self, df: pd.DataFrame) -> str:
        """æ£€æµ‹æ ‡ç­¾åˆ—"""
        possible_labels = ['content_type', 'final_label', 'label', 'category']
        for col in possible_labels:
            if col in df.columns:
                return col
        return None
    
    def _detect_text_column(self, df: pd.DataFrame) -> str:
        """æ£€æµ‹æ–‡æœ¬åˆ—"""
        possible_texts = ['original_log', 'message', 'content', 'text']
        for col in possible_texts:
            if col in df.columns:
                return col
        return None
    
    def create_model(self, model_type: str = 'gradient_boosting'):
        """åˆ›å»ºæ¨¡å‹"""
        if model_type == 'lightgbm' and LIGHTGBM_AVAILABLE:
            self.model = lgb.LGBMClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                num_leaves=31,
                random_state=42,
                verbose=-1
            )
            print("ğŸ¤– ä½¿ç”¨LightGBMæ¨¡å‹")
        elif model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            print("ğŸ¤– ä½¿ç”¨RandomForestæ¨¡å‹")
        else:
            self.model = GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                subsample=0.8,
                random_state=42
            )
            print("ğŸ¤– ä½¿ç”¨GradientBoostingæ¨¡å‹")
    
    def train_model(self, X_train, y_train, X_test, y_test):
        """è®­ç»ƒæ¨¡å‹"""
        print("\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # ç‰¹å¾å·¥ç¨‹
        print("ğŸ“ å‘é‡åŒ–è®­ç»ƒæ•°æ®...")
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.95,
            stop_words='english'
        )
        
        X_train_vec = self.vectorizer.fit_transform(X_train)
        X_test_vec = self.vectorizer.transform(X_test)
        
        print(f"ğŸ“Š ç‰¹å¾ç»´åº¦: {X_train_vec.shape[1]}")
        
        # è®­ç»ƒæ¨¡å‹
        print("ğŸ‹ï¸ è®­ç»ƒæ¨¡å‹...")
        start_time = datetime.now()
        
        self.model.fit(X_train_vec, y_train)
        
        training_time = (datetime.now() - start_time).total_seconds()
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
        
        # é¢„æµ‹
        y_pred = self.model.predict(X_test_vec)
        y_pred_proba = self.model.predict_proba(X_test_vec)
        
        return y_pred, y_pred_proba, X_test_vec
    
    def evaluate_model(self, y_test, y_pred, y_pred_proba):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print("=" * 60)
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        precision_macro = precision_score(y_test, y_pred, average='macro')
        recall_macro = recall_score(y_test, y_pred, average='macro')
        f1_macro = f1_score(y_test, y_pred, average='macro')
        
        precision_weighted = precision_score(y_test, y_pred, average='weighted')
        recall_weighted = recall_score(y_test, y_pred, average='weighted')
        f1_weighted = f1_score(y_test, y_pred, average='weighted')
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        precision_per_class = precision_score(y_test, y_pred, average=None)
        recall_per_class = recall_score(y_test, y_pred, average=None)
        f1_per_class = f1_score(y_test, y_pred, average=None)
        
        # å­˜å‚¨ç»“æœ
        self.results = {
            'accuracy': accuracy,
            'precision_macro': precision_macro,
            'recall_macro': recall_macro,
            'f1_macro': f1_macro,
            'precision_weighted': precision_weighted,
            'recall_weighted': recall_weighted,
            'f1_weighted': f1_weighted,
            'precision_per_class': precision_per_class,
            'recall_per_class': recall_per_class,
            'f1_per_class': f1_per_class,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
        
        # è¾“å‡ºä¼ä¸šçº§è¯„ä¼°è¡¨æ ¼
        self._print_enterprise_metrics()
        
        # è¾“å‡ºè¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print("\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=self.label_encoder.classes_))
        
        return self.results
    
    def _print_enterprise_metrics(self):
        """æ‰“å°ä¼ä¸šçº§è¯„ä¼°æŒ‡æ ‡"""
        print("\nğŸ“Š ä¼ä¸šçº§è¯„ä¼°æŒ‡æ ‡")
        print("=" * 80)
        print("| æŒ‡æ ‡                 | å½“å‰å€¼           | ä¼ä¸šçº§ç›®æ ‡         | çŠ¶æ€    |")
        print("| ------------------ | ------------ | ----------------- | ------ |")
        
        metrics = [
            ('Accuracy', self.results['accuracy'], self.enterprise_targets['accuracy']),
            ('Macro F1', self.results['f1_macro'], self.enterprise_targets['macro_f1']),
            ('Weighted F1', self.results['f1_weighted'], self.enterprise_targets['weighted_f1']),
            ('Macro Recall', self.results['recall_macro'], self.enterprise_targets['recall']),
            ('Macro Precision', self.results['precision_macro'], self.enterprise_targets['precision'])
        ]
        
        for metric_name, current_value, target_value in metrics:
            status = "âœ… ä¼˜ç§€" if current_value >= target_value else "âš ï¸ éœ€æ”¹è¿›"
            print(f"| {metric_name:<20} | {current_value:.4f}         | {target_value:.2f}            | {status:<6} |")
        
        print("=" * 80)
        
        # è®¡ç®—æ€»ä½“è¾¾æ ‡æƒ…å†µ
        achieved_count = sum(1 for _, current, target in metrics if current >= target)
        total_count = len(metrics)
        print(f"\nğŸ¯ æ€»ä½“è¾¾æ ‡æƒ…å†µ: {achieved_count}/{total_count} é¡¹æŒ‡æ ‡è¾¾åˆ°ä¼ä¸šçº§æ ‡å‡†")
        
        if achieved_count == total_count:
            print("ğŸ‰ æ­å–œï¼æ‰€æœ‰æŒ‡æ ‡éƒ½è¾¾åˆ°äº†ä¼ä¸šçº§æ ‡å‡†ï¼")
        elif achieved_count >= total_count * 0.8:
            print("ğŸ‘ è¡¨ç°è‰¯å¥½ï¼Œå¤§éƒ¨åˆ†æŒ‡æ ‡è¾¾åˆ°ä¼ä¸šçº§æ ‡å‡†")
        else:
            print("ğŸ’¡ å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹å‚æ•°æˆ–å¢åŠ è®­ç»ƒæ•°æ®")
    
    def save_results(self, output_dir: str = "logsense-gpu/results"):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜æ¨¡å‹
        models_dir = output_path / "models"
        models_dir.mkdir(exist_ok=True)
        
        import joblib
        model_file = models_dir / f"enhanced_model_{timestamp}.joblib"
        vectorizer_file = models_dir / f"vectorizer_{timestamp}.joblib"
        
        joblib.dump(self.model, model_file)
        joblib.dump(self.vectorizer, vectorizer_file)
        joblib.dump(self.label_encoder, models_dir / f"label_encoder_{timestamp}.joblib")
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_file}")
        
        # ä¿å­˜ç»“æœ
        results_file = output_path / f"enhanced_results_{timestamp}.json"
        import json
        
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        results_for_json = {}
        for key, value in self.results.items():
            if isinstance(value, np.ndarray):
                results_for_json[key] = value.tolist()
            else:
                results_for_json[key] = value
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_for_json, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_plots(output_path, timestamp)
    
    def _generate_plots(self, output_path: Path, timestamp: str):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        plots_dir = output_path / "plots"
        plots_dir.mkdir(exist_ok=True)
        
        # æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(12, 10))
        cm = confusion_matrix(self.results['y_test'], self.results['y_pred'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=self.label_encoder.classes_,
                   yticklabels=self.label_encoder.classes_)
        plt.title('æ··æ·†çŸ©é˜µ - å¢å¼ºç‰ˆæ¨¡å‹')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.tight_layout()
        plt.savefig(plots_dir / f"confusion_matrix_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # F1åˆ†æ•°å¯¹æ¯”
        plt.figure(figsize=(12, 8))
        classes = self.label_encoder.classes_
        f1_scores = self.results['f1_per_class']
        
        bars = plt.bar(range(len(classes)), f1_scores, color='skyblue', alpha=0.7)
        plt.xlabel('ç±»åˆ«')
        plt.ylabel('F1åˆ†æ•°')
        plt.title('å„ç±»åˆ«F1åˆ†æ•° - å¢å¼ºç‰ˆæ¨¡å‹')
        plt.xticks(range(len(classes)), classes, rotation=45, ha='right')
        plt.ylim(0, 1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, score) in enumerate(zip(bars, f1_scores)):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(plots_dir / f"f1_scores_{timestamp}.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜åˆ°: {plots_dir}")
    
    def run_training(self, data_file: str, sample_size: int = None, 
                    model_type: str = 'gradient_boosting', test_size: float = 0.2):
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸ§ª å¼€å§‹å¢å¼ºç‰ˆæ¨¡å‹è®­ç»ƒ")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®
        df, label_column, text_column = self.load_and_prepare_data(data_file, sample_size)
        
        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X = df[text_column].fillna('')
        y = df[label_column]
        
        # ç¼–ç æ ‡ç­¾
        y_encoded = self.label_encoder.fit_transform(y)
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=test_size, random_state=42, stratify=y_encoded
        )
        
        print(f"\nğŸ“Š è®­ç»ƒé›†: {len(X_train)} æ¡è®°å½•")
        print(f"ğŸ“Š æµ‹è¯•é›†: {len(X_test)} æ¡è®°å½•")
        
        # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        self.create_model(model_type)
        y_pred, y_pred_proba, X_test_vec = self.train_model(X_train, y_train, X_test, y_test)
        
        # è¯„ä¼°æ¨¡å‹
        results = self.evaluate_model(y_test, y_pred, y_pred_proba)
        
        # ä¿å­˜ç»“æœ
        self.save_results()
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆåŸºçº¿æ¨¡å‹è®­ç»ƒè„šæœ¬")
    parser.add_argument("--data-file", 
                       default="DATA_OUTPUT/training_data/combined_dataset_20250802_131542.csv",
                       help="è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--sample-size", type=int, default=None,
                       help="æ¯ç±»æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰")
    parser.add_argument("--model-type", 
                       choices=['gradient_boosting', 'lightgbm', 'random_forest'],
                       default='gradient_boosting',
                       help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--test-size", type=float, default=0.2,
                       help="æµ‹è¯•é›†æ¯”ä¾‹")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not Path(args.data_file).exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_file}")
        sys.exit(1)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶è¿è¡Œ
    trainer = EnhancedBaselineModel()
    results = trainer.run_training(
        data_file=args.data_file,
        sample_size=args.sample_size,
        model_type=args.model_type,
        test_size=args.test_size
    )
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main() 