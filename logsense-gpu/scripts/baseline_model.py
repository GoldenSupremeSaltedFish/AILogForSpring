#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°æ ·æœ¬éªŒè¯ + Base æ¨¡å‹è®­ç»ƒ
åŠŸèƒ½ï¼š
1. å°æ ·æœ¬éªŒè¯ï¼ˆ3-5ä¸ªç±»åˆ«ï¼Œæ¯ç±»500æ¡æ ·æœ¬ï¼‰
2. æ„å»ºbaselineæ¨¡å‹ï¼ˆTF-IDF + LightGBM/GradientBoostingï¼‰
3. æ”¯æŒå¤šå¹³å°è®¡ç®—ï¼ˆCPU/GPUï¼‰
4. æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–

ä½¿ç”¨æ–¹æ³•:
python baseline_model.py                           # ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python baseline_model.py --sample-size 300        # è®¾ç½®æ¯ç±»æ ·æœ¬æ•°
python baseline_model.py --model-type lightgbm    # é€‰æ‹©æ¨¡å‹ç±»å‹
python baseline_model.py --gpu                    # å¯ç”¨GPUåŠ é€Ÿ
"""

import pandas as pd
import numpy as np
import argparse
import sys
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
    from sklearn.pipeline import Pipeline
    import matplotlib.pyplot as plt
    import seaborn as sns
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸  scikit-learnæœªå®‰è£…")

# LightGBM
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸  lightgbmæœªå®‰è£…")

# GPUæ”¯æŒ
try:
    import torch
    GPU_AVAILABLE = torch.cuda.is_available()
    if GPU_AVAILABLE:
        print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
except ImportError:
    GPU_AVAILABLE = False
    print("âš ï¸  PyTorchæœªå®‰è£…")


class BaselineModelTrainer:
    """Baselineæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, sample_size: int = 500, model_type: str = 'gradient_boosting'):
        self.sample_size = sample_size
        self.model_type = model_type
        self.vectorizer = None
        self.model = None
        self.classes = None
        
        # æ”¯æŒçš„ç±»åˆ«ï¼ˆé€‰æ‹©3-5ä¸ªä¸»è¦ç±»åˆ«ï¼‰
        self.target_classes = [
            'stack_exception',      # å †æ ˆå¼‚å¸¸
            'connection_issue',     # è¿æ¥é—®é¢˜
            'database_exception',   # æ•°æ®åº“å¼‚å¸¸
            'auth_authorization',   # è®¤è¯æˆæƒ
            'memory_performance'    # å†…å­˜æ€§èƒ½
        ]
        
        print(f"ğŸ¯ ç›®æ ‡ç±»åˆ«: {self.target_classes}")
        print(f"ğŸ“Š æ¯ç±»æ ·æœ¬æ•°: {self.sample_size}")
        print(f"ğŸ¤– æ¨¡å‹ç±»å‹: {model_type}")
    
    def load_and_sample_data(self, data_file: str) -> pd.DataFrame:
        """åŠ è½½æ•°æ®å¹¶è¿›è¡Œå°æ ·æœ¬é‡‡æ ·"""
        try:
            print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
            df = pd.read_csv(data_file)
            print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            if 'text' not in df.columns or 'label' not in df.columns:
                print("âŒ æ•°æ®æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: text, label")
                return pd.DataFrame()
            
            # è¿‡æ»¤ç›®æ ‡ç±»åˆ«
            df_filtered = df[df['label'].isin(self.target_classes)].copy()
            print(f"ğŸ” è¿‡æ»¤åæ•°æ®: {len(df_filtered)} æ¡è®°å½•")
            
            # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
            class_counts = df_filtered['label'].value_counts()
            print("\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
            for class_name, count in class_counts.items():
                print(f"  {class_name}: {count} æ¡")
            
            # å°æ ·æœ¬é‡‡æ ·
            sampled_data = []
            for class_name in self.target_classes:
                class_data = df_filtered[df_filtered['label'] == class_name]
                if len(class_data) >= self.sample_size:
                    # éšæœºé‡‡æ ·
                    sampled = class_data.sample(n=self.sample_size, random_state=42)
                else:
                    # å¦‚æœæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®
                    sampled = class_data
                    print(f"âš ï¸  {class_name} ç±»åˆ«æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨å…¨éƒ¨ {len(sampled)} æ¡")
                
                sampled_data.append(sampled)
            
            # åˆå¹¶é‡‡æ ·æ•°æ®
            df_sampled = pd.concat(sampled_data, ignore_index=True)
            print(f"\nâš–ï¸ é‡‡æ ·åæ•°æ®: {len(df_sampled)} æ¡è®°å½•")
            
            # æœ€ç»ˆç±»åˆ«åˆ†å¸ƒ
            final_counts = df_sampled['label'].value_counts()
            print("\nğŸ“ˆ æœ€ç»ˆç±»åˆ«åˆ†å¸ƒ:")
            for class_name, count in final_counts.items():
                percentage = (count / len(df_sampled)) * 100
                print(f"  {class_name}: {count} æ¡ ({percentage:.1f}%)")
            
            return df_sampled
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        if self.model_type == 'lightgbm' and LIGHTGBM_AVAILABLE:
            # LightGBMæ¨¡å‹
            self.model = lgb.LGBMClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42,
                verbose=-1
            )
            print("ğŸ¤– ä½¿ç”¨LightGBMæ¨¡å‹")
            
        else:
            # GradientBoostingæ¨¡å‹
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            )
            print("ğŸ¤– ä½¿ç”¨GradientBoostingæ¨¡å‹")
        
        # TF-IDFå‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            max_features=3000,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.95
        )
        print("ğŸ“ ä½¿ç”¨TF-IDFå‘é‡åŒ–å™¨")
    
    def train_model(self, X_train: pd.Series, y_train: pd.Series, 
                   X_test: pd.Series, y_test: pd.Series) -> Dict:
        """è®­ç»ƒæ¨¡å‹"""
        print("\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # åˆ›å»ºæ¨¡å‹
        self.create_model()
        
        # å‘é‡åŒ–è®­ç»ƒæ•°æ®
        print("ğŸ“ å‘é‡åŒ–è®­ç»ƒæ•°æ®...")
        X_train_vec = self.vectorizer.fit_transform(X_train)
        X_test_vec = self.vectorizer.transform(X_test)
        
        print(f"ğŸ“Š ç‰¹å¾ç»´åº¦: {X_train_vec.shape[1]}")
        
        # è®­ç»ƒæ¨¡å‹
        print("ğŸ‹ï¸ è®­ç»ƒæ¨¡å‹...")
        start_time = datetime.now()
        self.model.fit(X_train_vec, y_train)
        training_time = (datetime.now() - start_time).total_seconds()
        
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
        
        # é¢„æµ‹
        print("ğŸ”® è¿›è¡Œé¢„æµ‹...")
        y_pred = self.model.predict(X_test_vec)
        y_pred_proba = self.model.predict_proba(X_test_vec)
        
        # è¯„ä¼°ç»“æœ
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)
        
        print(f"\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"å®å¹³å‡F1: {report['macro avg']['f1-score']:.4f}")
        print(f"åŠ æƒå¹³å‡F1: {report['weighted avg']['f1-score']:.4f}")
        
        return {
            'accuracy': accuracy,
            'classification_report': report,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba,
            'training_time': training_time,
            'feature_dim': X_train_vec.shape[1]
        }
    
    def save_model(self, model_dir: Path):
        """ä¿å­˜æ¨¡å‹"""
        try:
            model_dir.mkdir(exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ä¿å­˜æ¨¡å‹
            model_file = model_dir / f"baseline_model_{self.model_type}_{timestamp}.pkl"
            import pickle
            with open(model_file, 'wb') as f:
                pickle.dump({
                    'vectorizer': self.vectorizer,
                    'model': self.model,
                    'classes': self.target_classes,
                    'model_type': self.model_type
                }, f)
            
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_file}")
            
            # ä¿å­˜é…ç½®
            config_file = model_dir / f"model_config_{timestamp}.json"
            import json
            config = {
                'model_type': self.model_type,
                'sample_size': self.sample_size,
                'target_classes': self.target_classes,
                'feature_dim': self.vectorizer.get_feature_names_out().shape[0],
                'training_time': datetime.now().isoformat()
            }
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“‹ é…ç½®å·²ä¿å­˜åˆ°: {config_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
    
    def plot_results(self, y_test: pd.Series, y_pred: np.ndarray, results_dir: Path):
        """ç»˜åˆ¶ç»“æœå›¾è¡¨"""
        try:
            results_dir.mkdir(exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # æ··æ·†çŸ©é˜µ
            plt.figure(figsize=(10, 8))
            cm = confusion_matrix(y_test, y_pred, labels=self.target_classes)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=self.target_classes, 
                       yticklabels=self.target_classes)
            plt.title('æ··æ·†çŸ©é˜µ')
            plt.xlabel('é¢„æµ‹ç±»åˆ«')
            plt.ylabel('çœŸå®ç±»åˆ«')
            plt.tight_layout()
            
            cm_file = results_dir / f"confusion_matrix_{timestamp}.png"
            plt.savefig(cm_file, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {cm_file}")
            
            # ç±»åˆ«å‡†ç¡®ç‡æ¡å½¢å›¾
            report = classification_report(y_test, y_pred, output_dict=True)
            f1_scores = [report[cls]['f1-score'] for cls in self.target_classes]
            
            plt.figure(figsize=(12, 6))
            bars = plt.bar(self.target_classes, f1_scores, color='skyblue')
            plt.title('å„ç±»åˆ«F1åˆ†æ•°')
            plt.xlabel('ç±»åˆ«')
            plt.ylabel('F1åˆ†æ•°')
            plt.xticks(rotation=45)
            plt.ylim(0, 1)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, score in zip(bars, f1_scores):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{score:.3f}', ha='center', va='bottom')
            
            plt.tight_layout()
            
            f1_file = results_dir / f"f1_scores_{timestamp}.png"
            plt.savefig(f1_file, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"ğŸ“ˆ F1åˆ†æ•°å›¾å·²ä¿å­˜åˆ°: {f1_file}")
            
        except Exception as e:
            print(f"âŒ ç»˜åˆ¶å›¾è¡¨å¤±è´¥: {e}")
    
    def run_experiment(self, data_file: str, output_dir: Path = None):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        if output_dir is None:
            output_dir = Path("logsense-gpu/results")
        
        print("ğŸ§ª å¼€å§‹å°æ ·æœ¬éªŒè¯å®éªŒ")
        print("=" * 60)
        
        # åŠ è½½å’Œé‡‡æ ·æ•°æ®
        df = self.load_and_sample_data(data_file)
        if df.empty:
            return False
        
        # æ•°æ®åˆ†å‰²
        X = df['text']
        y = df['label']
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"\nğŸ“Š è®­ç»ƒé›†: {len(X_train)} æ¡è®°å½•")
        print(f"ğŸ“Š æµ‹è¯•é›†: {len(X_test)} æ¡è®°å½•")
        
        # è®­ç»ƒæ¨¡å‹
        results = self.train_model(X_train, y_train, X_test, y_test)
        
        # ä¿å­˜æ¨¡å‹
        model_dir = output_dir / "models"
        self.save_model(model_dir)
        
        # ç»˜åˆ¶ç»“æœ
        results_dir = output_dir / "plots"
        self.plot_results(y_test, results['y_pred'], results_dir)
        
        # ä¿å­˜ç»“æœ
        results_file = output_dir / f"experiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        import json
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'accuracy': results['accuracy'],
                'training_time': results['training_time'],
                'feature_dim': results['feature_dim'],
                'sample_size': self.sample_size,
                'model_type': self.model_type,
                'target_classes': self.target_classes
            }, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ å®éªŒç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å°æ ·æœ¬éªŒè¯ + Baseæ¨¡å‹è®­ç»ƒ")
    parser.add_argument("--data-file", default="DATA_OUTPUT/training_dataset_20250802_013437.csv",
                       help="è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--sample-size", type=int, default=500,
                       help="æ¯ç±»æ ·æœ¬æ•° (é»˜è®¤: 500)")
    parser.add_argument("--model-type", choices=['gradient_boosting', 'lightgbm'], 
                       default='gradient_boosting', help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--output-dir", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--gpu", action="store_true", help="å¯ç”¨GPUåŠ é€Ÿ")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    if not SKLEARN_AVAILABLE:
        print("âŒ è¯·å®‰è£…scikit-learn: pip install scikit-learn")
        sys.exit(1)
    
    if args.model_type == 'lightgbm' and not LIGHTGBM_AVAILABLE:
        print("âŒ è¯·å®‰è£…lightgbm: pip install lightgbm")
        sys.exit(1)
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = Path("logsense-gpu/results")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = BaselineModelTrainer(
        sample_size=args.sample_size,
        model_type=args.model_type
    )
    
    # è¿è¡Œå®éªŒ
    success = trainer.run_experiment(args.data_file, output_dir)
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main() 