#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ¨¡å‹è®­ç»ƒè„šæœ¬ - ä½¿ç”¨æ›´å¤§æ•°æ®é›†
"""

import pandas as pd
import numpy as np
import argparse
import sys
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ ç›¸å…³å¯¼å…¥
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, accuracy_score, confusion_matrix,
    precision_score, recall_score, f1_score
)
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import json

# å¯é€‰å¯¼å…¥
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False


def load_and_prepare_data(data_file: str, sample_size: int = None):
    """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
    print(f"ï¿½ï¿½ åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
    
    df = pd.read_csv(data_file)
    print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
    
    # æ£€æµ‹æ ‡ç­¾åˆ—å’Œæ–‡æœ¬åˆ—
    label_column = None
    text_column = None
    
    possible_labels = ['content_type', 'final_label', 'label', 'category']
    for col in possible_labels:
        if col in df.columns:
            label_column = col
            break
    
    possible_texts = ['original_log', 'message', 'content', 'text']
    for col in possible_texts:
        if col in df.columns:
            text_column = col
            break
    
    if not label_column or not text_column:
        raise ValueError("æœªæ‰¾åˆ°æ ‡ç­¾åˆ—æˆ–æ–‡æœ¬åˆ—")
    
    print(f"ğŸ” ä½¿ç”¨æ ‡ç­¾åˆ—: {label_column}")
    print(f"ğŸ” ä½¿ç”¨æ–‡æœ¬åˆ—: {text_column}")
    
    # è¿‡æ»¤æ•°æ®
    df_filtered = df[df[label_column] != 'other'].copy()
    print(f"ğŸ” è¿‡æ»¤åæ•°æ®: {len(df_filtered)} æ¡è®°å½•")
    
    # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
    category_counts = df_filtered[label_column].value_counts()
    print("\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
    for category, count in category_counts.items():
        percentage = (count / len(df_filtered)) * 100
        print(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
    
    # å¦‚æœæŒ‡å®šäº†é‡‡æ ·å¤§å°ï¼Œè¿›è¡Œé‡‡æ ·
    if sample_size:
        print(f"\nğŸ¯ è¿›è¡Œé‡‡æ ·ï¼Œæ¯ç±»æœ€å¤š {sample_size} æ¡è®°å½•")
        sampled_data = []
        for category in df_filtered[label_column].unique():
            category_data = df_filtered[df_filtered[label_column] == category]
            if len(category_data) > sample_size:
                category_data = category_data.sample(n=sample_size, random_state=42)
            sampled_data.append(category_data)
        
        df_filtered = pd.concat(sampled_data, ignore_index=True)
        print(f"ğŸ“Š é‡‡æ ·åæ•°æ®: {len(df_filtered)} æ¡è®°å½•")
    
    return df_filtered, label_column, text_column


def create_model(model_type: str = 'gradient_boosting'):
    """åˆ›å»ºæ¨¡å‹"""
    if model_type == 'lightgbm' and LIGHTGBM_AVAILABLE:
        model = lgb.LGBMClassifier(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=8,
            num_leaves=31,
            random_state=42,
            verbose=-1
        )
        print("ï¿½ï¿½ ä½¿ç”¨LightGBMæ¨¡å‹")
    elif model_type == 'random_forest':
        model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        print("ğŸ¤– ä½¿ç”¨RandomForestæ¨¡å‹")
    else:
        model = GradientBoostingClassifier(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=8,
            subsample=0.8,
            random_state=42
        )
        print("ï¿½ï¿½ ä½¿ç”¨GradientBoostingæ¨¡å‹")
    
    return model


def train_model(model, X_train, y_train, X_test, y_test):
    """è®­ç»ƒæ¨¡å‹"""
    print("\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    
    # ç‰¹å¾å·¥ç¨‹
    print("ğŸ“ å‘é‡åŒ–è®­ç»ƒæ•°æ®...")
    vectorizer = TfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 3),
        min_df=2,
        max_df=0.95,
        stop_words='english'
    )
    
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)
    
    print(f"ï¿½ï¿½ ç‰¹å¾ç»´åº¦: {X_train_vec.shape[1]}")
    
    # è®­ç»ƒæ¨¡å‹
    print("ğŸ‹ï¸ è®­ç»ƒæ¨¡å‹...")
    start_time = datetime.now()
    
    model.fit(X_train_vec, y_train)
    
    training_time = (datetime.now() - start_time).total_seconds()
    print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    
    # é¢„æµ‹
    y_pred = model.predict(X_test_vec)
    y_pred_proba = model.predict_proba(X_test_vec)
    
    return y_pred, y_pred_proba, vectorizer


def evaluate_model(y_test, y_pred, y_pred_proba, label_encoder):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
    print("=" * 60)
    
    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    precision_macro = precision_score(y_test, y_pred, average='macro')
    recall_macro = recall_score(y_test, y_pred, average='macro')
    f1_macro = f1_score(y_test, y_pred, average='macro')
    
    precision_weighted = precision_score(y_test, y_pred, average='weighted')
    recall_weighted = recall_score(y_test, y_pred, average='weighted')
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    
    # ä¼ä¸šçº§ç›®æ ‡æŒ‡æ ‡
    enterprise_targets = {
        'accuracy': 0.90,
        'macro_f1': 0.88,
        'weighted_f1': 0.90,
        'recall': 0.85,
        'precision': 0.90
    }
    
    # è¾“å‡ºä¼ä¸šçº§è¯„ä¼°è¡¨æ ¼
    print("\nğŸ“Š ä¼ä¸šçº§è¯„ä¼°æŒ‡æ ‡")
    print("=" * 80)
    print("| æŒ‡æ ‡                 | å½“å‰å€¼           | ä¼ä¸šçº§ç›®æ ‡         | çŠ¶æ€    |")
    print("| ------------------ | ------------ | ----------------- | ------ |")
    
    metrics = [
        ('Accuracy', accuracy, enterprise_targets['accuracy']),
        ('Macro F1', f1_macro, enterprise_targets['macro_f1']),
        ('Weighted F1', f1_weighted, enterprise_targets['weighted_f1']),
        ('Macro Recall', recall_macro, enterprise_targets['recall']),
        ('Macro Precision', precision_macro, enterprise_targets['precision'])
    ]
    
    for metric_name, current_value, target_value in metrics:
        status = "âœ… ä¼˜ç§€" if current_value >= target_value else "âš ï¸ éœ€æ”¹è¿›"
        print(f"| {metric_name:<20} | {current_value:.4f}         | {target_value:.2f}            | {status:<6} |")
    
    print("=" * 80)
    
    # è®¡ç®—æ€»ä½“è¾¾æ ‡æƒ…å†µ
    achieved_count = sum(1 for _, current, target in metrics if current >= target)
    total_count = len(metrics)
    print(f"\nğŸ¯ æ€»ä½“è¾¾æ ‡æƒ…å†µ: {achieved_count}/{total_count} é¡¹æŒ‡æ ‡è¾¾åˆ°ä¼ä¸šçº§æ ‡å‡†")
    
    if achieved_count == total_count:
        print("ğŸ‰ æ­å–œï¼æ‰€æœ‰æŒ‡æ ‡éƒ½è¾¾åˆ°äº†ä¼ä¸šçº§æ ‡å‡†ï¼")
    elif achieved_count >= total_count * 0.8:
        print("ğŸ‘ è¡¨ç°è‰¯å¥½ï¼Œå¤§éƒ¨åˆ†æŒ‡æ ‡è¾¾åˆ°ä¼ä¸šçº§æ ‡å‡†")
    else:
        print("ï¿½ï¿½ å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹å‚æ•°æˆ–å¢åŠ è®­ç»ƒæ•°æ®")
    
    # è¾“å‡ºè¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))
    
    return {
        'accuracy': accuracy,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'precision_weighted': precision_weighted,
        'recall_weighted': recall_weighted,
        'f1_weighted': f1_weighted,
        'y_test': y_test,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }


def save_results(model, vectorizer, label_encoder, results, output_dir: str = "logsense-gpu/results"):
    """ä¿å­˜è®­ç»ƒç»“æœ"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜æ¨¡å‹
    models_dir = output_path / "models"
    models_dir.mkdir(exist_ok=True)
    
    model_file = models_dir / f"enhanced_model_{timestamp}.joblib"
    vectorizer_file = models_dir / f"vectorizer_{timestamp}.joblib"
    encoder_file = models_dir / f"label_encoder_{timestamp}.joblib"
    
    joblib.dump(model, model_file)
    joblib.dump(vectorizer, vectorizer_file)
    joblib.dump(label_encoder, encoder_file)
    
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_file}")
    
    # ä¿å­˜ç»“æœ
    results_file = output_path / f"enhanced_results_{timestamp}.json"
    
    # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
    results_for_json = {}
    for key, value in results.items():
        if isinstance(value, np.ndarray):
            results_for_json[key] = value.tolist()
        else:
            results_for_json[key] = value
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results_for_json, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    generate_plots(results, label_encoder, output_path, timestamp)


def generate_plots(results, label_encoder, output_path: Path, timestamp: str):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    plots_dir = output_path / "plots"
    plots_dir.mkdir(exist_ok=True)
    
    # æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(12, 10))
    cm = confusion_matrix(results['y_test'], results['y_pred'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
               xticklabels=label_encoder.classes_,
               yticklabels=label_encoder.classes_)
    plt.title('æ··æ·†çŸ©é˜µ - å¢å¼ºç‰ˆæ¨¡å‹')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.tight_layout()
    plt.savefig(plots_dir / f"confusion_matrix_{timestamp}.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # F1åˆ†æ•°å¯¹æ¯”
    plt.figure(figsize=(12, 8))
    classes = label_encoder.classes_
    f1_scores = f1_score(results['y_test'], results['y_pred'], average=None)
    
    bars = plt.bar(range(len(classes)), f1_scores, color='skyblue', alpha=0.7)
    plt.xlabel('ç±»åˆ«')
    plt.ylabel('F1åˆ†æ•°')
    plt.title('å„ç±»åˆ«F1åˆ†æ•° - å¢å¼ºç‰ˆæ¨¡å‹')
    plt.xticks(range(len(classes)), classes, rotation=45, ha='right')
    plt.ylim(0, 1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, score) in enumerate(zip(bars, f1_scores)):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{score:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(plots_dir / f"f1_scores_{timestamp}.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ï¿½ï¿½ å›¾è¡¨å·²ä¿å­˜åˆ°: {plots_dir}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆæ¨¡å‹è®­ç»ƒè„šæœ¬")
    parser.add_argument("--data-file", 
                       default="DATA_OUTPUT/training_data/combined_dataset_20250802_131542.csv",
                       help="è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--sample-size", type=int, default=None,
                       help="æ¯ç±»æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰")
    parser.add_argument("--model-type", 
                       choices=['gradient_boosting', 'lightgbm', 'random_forest'],
                       default='gradient_boosting',
                       help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--test-size", type=float, default=0.2,
                       help="æµ‹è¯•é›†æ¯”ä¾‹")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not Path(args.data_file).exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_file}")
        sys.exit(1)
    
    print("ğŸ§ª å¼€å§‹å¢å¼ºç‰ˆæ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    try:
        # 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®
        df, label_column, text_column = load_and_prepare_data(args.data_file, args.sample_size)
        
        # 2. å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X = df[text_column].fillna('')
        y = df[label_column]
        
        # 3. ç¼–ç æ ‡ç­¾
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(y)
        
        # 4. åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=args.test_size, random_state=42, stratify=y_encoded
        )
        
        print(f"\nğŸ“Š è®­ç»ƒé›†: {len(X_train)} æ¡è®°å½•")
        print(f"ï¿½ï¿½ æµ‹è¯•é›†: {len(X_test)} æ¡è®°å½•")
        
        # 5. åˆ›å»ºæ¨¡å‹
        model = create_model(args.model_type)
        
        # 6. è®­ç»ƒæ¨¡å‹
        y_pred, y_pred_proba, vectorizer = train_model(model, X_train, y_train, X_test, y_test)
        
        # 7. è¯„ä¼°æ¨¡å‹
        results = evaluate_model(y_test, y_pred, y_pred_proba, label_encoder)
        
        # 8. ä¿å­˜æ¨¡å‹å’Œç»“æœ
        save_results(model, vectorizer, label_encoder, results)
        
        print("\nâœ… è®­ç»ƒå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()