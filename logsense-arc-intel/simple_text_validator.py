#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–æ–‡æœ¬éªŒè¯è„šæœ¬
åªéªŒè¯TextCNNéƒ¨åˆ†ï¼Œä¸åŒ…å«ç»“æ„åŒ–ç‰¹å¾
"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import os
import sys
from collections import Counter
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# å®šä¹‰StructuredFeatureExtractorç±»ï¼ˆç”¨äºåŠ è½½æ¨¡å‹ï¼‰
class StructuredFeatureExtractor:
    def __init__(self, max_tfidf_features=1000):
        self.max_tfidf_features = max_tfidf_features
        self.label_encoders = {}
        self.tfidf_vectorizer = None
        self.scaler = None
        self.feature_names = []

class SimpleTextCNN(nn.Module):
    """ç®€åŒ–çš„TextCNNæ¨¡å‹"""
    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, num_classes, dropout=0.5):
        super(SimpleTextCNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filters, (size, embed_dim)) for size in filter_sizes
        ])
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)
    
    def forward(self, x):
        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]
        x = x.unsqueeze(1)  # [batch_size, 1, seq_len, embed_dim]
        
        conv_outputs = []
        for conv in self.convs:
            conv_out = torch.relu(conv(x))  # [batch_size, num_filters, seq_len-k+1, 1]
            conv_out = conv_out.squeeze(3)  # [batch_size, num_filters, seq_len-k+1]
            pooled = torch.max_pool1d(conv_out, conv_out.size(2))  # [batch_size, num_filters, 1]
            conv_outputs.append(pooled.squeeze(2))  # [batch_size, num_filters]
        
        x = torch.cat(conv_outputs, dim=1)  # [batch_size, len(filter_sizes) * num_filters]
        x = self.dropout(x)
        x = self.fc(x)
        return x

def text_to_sequence(text, vocab, max_len=100):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ—"""
    words = text.lower().split()
    sequence = []
    for word in words[:max_len]:
        sequence.append(vocab.get(word, vocab.get('<UNK>', 1)))
    
    # å¡«å……åˆ°æœ€å¤§é•¿åº¦
    while len(sequence) < max_len:
        sequence.append(vocab.get('<PAD>', 0))
    
    return sequence

class SimpleTextValidator:
    """ç®€åŒ–æ–‡æœ¬éªŒè¯å™¨"""
    
    def __init__(self, model_path):
        self.model_path = model_path
        self.device = torch.device("xpu" if torch.xpu.is_available() else "cpu")
        self.model = None
        self.vocab = None
        self.label_encoder = None
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ” åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # æ·»åŠ å®‰å…¨çš„å…¨å±€å˜é‡
        torch.serialization.add_safe_globals([
            'sklearn.preprocessing._label.LabelEncoder',
            'sklearn.preprocessing._data.StandardScaler', 
            'sklearn.feature_extraction.text.TfidfVectorizer'
        ])
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=False)
        
        # æå–ç»„ä»¶
        self.vocab = checkpoint['vocab']
        self.label_encoder = checkpoint['label_encoder']
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}")
        print(f"   ç±»åˆ«æ•°: {len(self.label_encoder.classes_)}")
        print(f"   ç±»åˆ«: {list(self.label_encoder.classes_)}")
        
        # åˆ›å»ºç®€åŒ–çš„TextCNNæ¨¡å‹
        vocab_size = len(self.vocab)
        num_classes = len(self.label_encoder.classes_)
        
        self.model = SimpleTextCNN(
            vocab_size=vocab_size,
            embed_dim=128,
            num_filters=128,
            filter_sizes=[3, 4, 5],
            num_classes=num_classes,
            dropout=0.5
        )
        
        # åªåŠ è½½æ–‡æœ¬ç¼–ç å™¨çš„æƒé‡
        text_encoder_state_dict = {}
        for key, value in checkpoint['model_state_dict'].items():
            if key.startswith('text_encoder.'):
                # ç§»é™¤'text_encoder.'å‰ç¼€
                new_key = key.replace('text_encoder.', '')
                text_encoder_state_dict[new_key] = value
        
        self.model.load_state_dict(text_encoder_state_dict)
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ–‡æœ¬æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ!")
        
    def load_validation_data(self, data_path):
        """åŠ è½½éªŒè¯æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½éªŒè¯æ•°æ®: {data_path}")
        
        if not os.path.exists(data_path):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return None
        
        df = pd.read_csv(data_path)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ! æ ·æœ¬æ•°: {len(df)}")
        print(f"   åŸå§‹ç±»åˆ«åˆ†å¸ƒ:\n{df['category'].value_counts()}")
        
        # è¿‡æ»¤æ‰è®­ç»ƒæ—¶æ²¡æœ‰çš„ç±»åˆ«
        valid_categories = set(self.label_encoder.classes_)
        df_filtered = df[df['category'].isin(valid_categories)].copy()
        
        print(f"âœ… è¿‡æ»¤åæ•°æ®: æ ·æœ¬æ•°: {len(df_filtered)}")
        print(f"   è¿‡æ»¤åç±»åˆ«åˆ†å¸ƒ:\n{df_filtered['category'].value_counts()}")
        
        return df_filtered
    
    def prepare_features(self, df):
        """å‡†å¤‡ç‰¹å¾"""
        print("ğŸ”§ å‡†å¤‡ç‰¹å¾...")
        
        # æ–‡æœ¬åºåˆ—åŒ–
        text_sequences = []
        for text in df['original_log']:
            sequence = text_to_sequence(text, self.vocab)
            text_sequences.append(sequence)
        
        text_tensor = torch.tensor(text_sequences, dtype=torch.long)
        
        # æ ‡ç­¾
        labels = self.label_encoder.transform(df['category'])
        label_tensor = torch.tensor(labels, dtype=torch.long)
        
        print(f"âœ… ç‰¹å¾å‡†å¤‡å®Œæˆ!")
        print(f"   æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {text_tensor.shape}")
        print(f"   æ ‡ç­¾å½¢çŠ¶: {label_tensor.shape}")
        
        return text_tensor, label_tensor
    
    def validate_model(self, text_tensor, label_tensor):
        """éªŒè¯æ¨¡å‹"""
        print("ğŸ” å¼€å§‹æ¨¡å‹éªŒè¯...")
        
        self.model.eval()
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            # åˆ†æ‰¹å¤„ç†
            batch_size = 32
            num_samples = len(text_tensor)
            
            for i in range(0, num_samples, batch_size):
                end_idx = min(i + batch_size, num_samples)
                
                batch_text = text_tensor[i:end_idx].to(self.device)
                batch_labels = label_tensor[i:end_idx]
                
                outputs = self.model(batch_text)
                predictions = torch.argmax(outputs, dim=1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(batch_labels.numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(all_labels, all_predictions)
        f1 = f1_score(all_labels, all_predictions, average='weighted')
        
        print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
        print(f"   æ€»ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   æ€»ä½“F1åˆ†æ•°: {f1:.4f}")
        
        # åˆ†ç±»æŠ¥å‘Š
        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        report = classification_report(all_labels, all_predictions, 
                                     target_names=self.label_encoder.classes_,
                                     digits=4)
        print(report)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, all_predictions)
        self.plot_confusion_matrix(cm, self.label_encoder.classes_)
        
        return accuracy, f1, all_predictions, all_labels
    
    def plot_confusion_matrix(self, cm, class_names):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=class_names, yticklabels=class_names)
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_path = f"results/plots/simple_text_validation_confusion_matrix_{timestamp}.png"
        os.makedirs(os.path.dirname(plot_path), exist_ok=True)
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜: {plot_path}")
    
    def run_validation(self, data_path):
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        print("=" * 60)
        print("ğŸš€ ç®€åŒ–æ–‡æœ¬æ¨¡å‹éªŒè¯")
        print("=" * 60)
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
        # åŠ è½½æ•°æ®
        df = self.load_validation_data(data_path)
        if df is None:
            return
        
        # å‡†å¤‡ç‰¹å¾
        text_tensor, label_tensor = self.prepare_features(df)
        
        # éªŒè¯æ¨¡å‹
        accuracy, f1, predictions, labels = self.validate_model(text_tensor, label_tensor)
        
        # ä¿å­˜ç»“æœ
        self.save_results(accuracy, f1, predictions, labels, df)
        
        print("\n" + "=" * 60)
        print("âœ… éªŒè¯å®Œæˆ!")
        print("=" * 60)
    
    def save_results(self, accuracy, f1, predictions, labels, df):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = f"results/validation_results_{timestamp}"
        os.makedirs(results_dir, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_df = df.copy()
        results_df['true_label'] = labels
        results_df['predicted_label'] = predictions
        results_df['correct'] = (labels == predictions)
        
        # è½¬æ¢æ ‡ç­¾åç§°
        results_df['true_category'] = self.label_encoder.inverse_transform(labels)
        results_df['predicted_category'] = self.label_encoder.inverse_transform(predictions)
        
        # ä¿å­˜åˆ°CSV
        results_path = os.path.join(results_dir, "validation_results.csv")
        results_df.to_csv(results_path, index=False)
        
        # ä¿å­˜æ‘˜è¦
        summary = {
            'timestamp': timestamp,
            'model_path': self.model_path,
            'total_samples': len(df),
            'accuracy': accuracy,
            'f1_score': f1,
            'class_distribution': df['category'].value_counts().to_dict(),
            'per_class_accuracy': {}
        }
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        for i, class_name in enumerate(self.label_encoder.classes_):
            class_mask = (labels == i)
            if np.sum(class_mask) > 0:
                class_accuracy = (predictions[class_mask] == labels[class_mask]).mean()
                summary['per_class_accuracy'][class_name] = class_accuracy
        
        # ä¿å­˜æ‘˜è¦
        import json
        summary_path = os.path.join(results_dir, "validation_summary.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
        print(f"   - è¯¦ç»†ç»“æœ: validation_results.csv")
        print(f"   - æ‘˜è¦æŠ¥å‘Š: validation_summary.json")

def main():
    """ä¸»å‡½æ•°"""
    # æ¨¡å‹æ–‡ä»¶è·¯å¾„
    model_path = "results/models/feature_enhanced_model_20250812_004934.pth"
    
    # éªŒè¯æ•°æ®è·¯å¾„
    data_path = "data/processed_logs_full.csv"
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = SimpleTextValidator(model_path)
    
    # è¿è¡ŒéªŒè¯
    validator.run_validation(data_path)

if __name__ == "__main__":
    main()
