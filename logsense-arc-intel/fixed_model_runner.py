# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆæ¨¡å‹éªŒè¯è„šæœ¬
ä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„è¯æ±‡è¡¨ï¼Œé¿å…è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…é—®é¢˜
"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import json
import logging
import argparse
import os
from datetime import datetime
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import pickle

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models import ModelFactory
from data import LogPreprocessor
from utils import ArcGPUDetector

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FixedModelRunner:
    """ä¿®å¤ç‰ˆæ¨¡å‹éªŒè¯å™¨"""
    
    def __init__(self, model_path: str, data_path: str):
        self.model_path = model_path
        self.data_path = data_path
        self.device = None
        self.model = None
        self.vocab = None
        self.label_encoder = None
        self.checkpoint = None
        
        # æ£€æµ‹è®¾å¤‡
        self.detect_device()
        
    def detect_device(self):
        """æ£€æµ‹è®¡ç®—è®¾å¤‡"""
        if torch.xpu.is_available():
            self.device = torch.device("xpu:0")
            logger.info("ğŸ® æ£€æµ‹åˆ°Intel Arc GPUï¼Œä½¿ç”¨XPUè®¾å¤‡")
        else:
            self.device = torch.device("cpu")
            logger.warning("âš ï¸ ä½¿ç”¨CPUè®¾å¤‡")
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–ä¿®å¤ç‰ˆæ¨¡å‹éªŒè¯å™¨ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_model_weights(self):
        """åŠ è½½æ¨¡å‹æƒé‡å’Œè¯æ±‡è¡¨"""
        try:
            logger.info("ğŸ“¥ åŠ è½½æ¨¡å‹æƒé‡...")
            
            # åŠ è½½checkpoint
            self.checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
            
            # åŠ è½½è¯æ±‡è¡¨
            if 'vocab' in self.checkpoint:
                self.vocab = self.checkpoint['vocab']
                logger.info(f"ğŸ“š è¯æ±‡è¡¨åŠ è½½å®Œæˆï¼Œå¤§å°: {len(self.vocab)}")
            else:
                logger.error("âŒ æœªæ‰¾åˆ°è¯æ±‡è¡¨")
                raise ValueError("æ¨¡å‹æ–‡ä»¶ä¸­æœªåŒ…å«è¯æ±‡è¡¨")
            
            # åŠ è½½æ ‡ç­¾ç¼–ç å™¨
            if 'label_encoder' in self.checkpoint:
                self.label_encoder = self.checkpoint['label_encoder']
                logger.info(f"ğŸ·ï¸ æ ‡ç­¾ç¼–ç å™¨åŠ è½½å®Œæˆï¼Œç±»åˆ«æ•°: {len(self.label_encoder.classes_)}")
            else:
                logger.error("âŒ æœªæ‰¾åˆ°æ ‡ç­¾ç¼–ç å™¨")
                raise ValueError("æ¨¡å‹æ–‡ä»¶ä¸­æœªåŒ…å«æ ‡ç­¾ç¼–ç å™¨")
            
            logger.info("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {e}")
            raise
    
    def text_to_sequence(self, text: str, vocab: dict, max_length: int = 100) -> list:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ—"""
        words = text.lower().split()
        sequence = []
        
        for word in words[:max_length]:
            if word in vocab:
                sequence.append(vocab[word])
            else:
                sequence.append(vocab.get('<UNK>', 0))
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(sequence) < max_length:
            sequence.append(vocab.get('<PAD>', 0))
        
        return sequence[:max_length]
    
    def extract_structured_features(self, df: pd.DataFrame) -> np.ndarray:
        """æå–ç»“æ„åŒ–ç‰¹å¾"""
        logger.info("ğŸ”§ å¼€å§‹ç‰¹å¾æå–...")
        
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç‰¹å¾æå–å™¨
        preprocessor = LogPreprocessor()
        features = preprocessor.extract_structured_features(df['cleaned_log'].tolist())
        
        logger.info(f"ğŸ”— ç‰¹å¾æå–å®Œæˆï¼Œæ€»ç»´åº¦: {features.shape}")
        return features
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹ç»“æ„"""
        try:
            logger.info("ğŸ”§ åˆ›å»ºæ¨¡å‹ç»“æ„...")
            
            # è·å–æ¨¡å‹é…ç½®
            num_classes = len(self.label_encoder.classes_)
            vocab_size = len(self.vocab)
            
            # åˆ›å»ºæ¨¡å‹
            model_config = {
                'vocab_size': vocab_size,
                'embedding_dim': 128,
                'num_filters': 128,
                'filter_sizes': [3, 4, 5],
                'num_classes': num_classes,
                'dropout': 0.5
            }
            
            self.model = ModelFactory.create_model('textcnn', **model_config)
            self.model.to(self.device)
            
            # åŠ è½½æƒé‡
            self.model.load_state_dict(self.checkpoint['model_state_dict'])
            
            logger.info("âœ… æ¨¡å‹ç»“æ„åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def load_and_prepare_data(self):
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
        try:
            logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {self.data_path}")
            
            # åŠ è½½æ•°æ®
            if self.data_path.endswith('.csv'):
                df = pd.read_csv(self.data_path)
            else:
                raise ValueError("è¯·ä½¿ç”¨CSVæ ¼å¼çš„æ•°æ®æ–‡ä»¶")
            
            # æ•°æ®æ¸…æ´—
            df_cleaned = df.dropna(subset=['cleaned_log', 'category'])
            df_cleaned = df_cleaned[df_cleaned['cleaned_log'].str.strip() != '']
            
            logger.info(f"ğŸ“ˆ æ•°æ®åŠ è½½å®Œæˆï¼Œæ€»è®°å½•æ•°: {len(df_cleaned)}")
            
            # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
            logger.info("ğŸ¯ ç±»åˆ«åˆ†å¸ƒ:")
            category_counts = df_cleaned['category'].value_counts()
            for category, count in category_counts.items():
                logger.info(f"   {category}: {count}")
            
            # æå–ç»“æ„åŒ–ç‰¹å¾
            features = self.extract_structured_features(df_cleaned)
            
            # å‡†å¤‡æ–‡æœ¬åºåˆ—
            texts = [self.text_to_sequence(text, self.vocab) for text in df_cleaned['cleaned_log']]
            
            # å‡†å¤‡æ ‡ç­¾
            labels = self.label_encoder.transform(df_cleaned['category'])
            
            return df_cleaned, texts, features, labels
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def validate_model(self, texts, features, labels, validation_name="validation"):
        """éªŒè¯æ¨¡å‹"""
        try:
            logger.info(f"ğŸ” å¼€å§‹æ¨¡å‹éªŒè¯: {validation_name}")
            
            # å‡†å¤‡æ•°æ®
            text_tensor = torch.tensor(texts, dtype=torch.long)
            feature_tensor = torch.tensor(features, dtype=torch.float32)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            text_tensor = text_tensor.to(self.device)
            feature_tensor = feature_tensor.to(self.device)
            
            # é¢„æµ‹
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(text_tensor, feature_tensor)
                predictions = torch.argmax(outputs, dim=1).cpu().numpy()
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(labels, predictions)
            f1 = f1_score(labels, predictions, average='weighted')
            
            # åˆ†ç±»æŠ¥å‘Š
            class_report = classification_report(
                labels, predictions,
                target_names=self.label_encoder.classes_,
                output_dict=True
            )
            
            # æ··æ·†çŸ©é˜µ
            conf_matrix = confusion_matrix(labels, predictions)
            
            logger.info(f"ğŸ“Š éªŒè¯ç»“æœ - {validation_name}:")
            logger.info(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
            logger.info(f"   F1åˆ†æ•°: {f1:.4f}")
            
            return {
                'validation_name': validation_name,
                'accuracy': accuracy,
                'f1_score': f1,
                'predictions': predictions,
                'true_labels': labels,
                'classification_report': class_report,
                'confusion_matrix': conf_matrix,
                'class_names': self.label_encoder.classes_
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            raise
    
    def save_results(self, results: dict):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        try:
            # åˆ›å»ºç»“æœç›®å½•
            results_dir = Path("final_validation_results")
            results_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜JSONç»“æœ
            json_path = results_dir / f"fixed_validation_results_{timestamp}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': timestamp,
                    'model_path': self.model_path,
                    'data_path': self.data_path,
                    'vocab_size': len(self.vocab),
                    'num_classes': len(self.label_encoder.classes_),
                    'accuracy': results['accuracy'],
                    'f1_score': results['f1_score'],
                    'classification_report': results['classification_report'],
                    'confusion_matrix': results['confusion_matrix'].tolist(),
                    'class_names': results['class_names'].tolist()
                }, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
            report_path = results_dir / f"fixed_detailed_report_{timestamp}.txt"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("ä¿®å¤ç‰ˆæ¨¡å‹éªŒè¯æŠ¥å‘Š\n")
                f.write("=" * 80 + "\n\n")
                f.write(f"éªŒè¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ¨¡å‹è·¯å¾„: {self.model_path}\n")
                f.write(f"æ•°æ®è·¯å¾„: {self.data_path}\n")
                f.write(f"è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}\n")
                f.write(f"ç±»åˆ«æ•°é‡: {len(self.label_encoder.classes_)}\n\n")
                
                f.write("ğŸ“Š éªŒè¯ç»“æœ\n")
                f.write("-" * 40 + "\n")
                f.write(f"å‡†ç¡®ç‡: {results['accuracy']:.4f}\n")
                f.write(f"F1åˆ†æ•°: {results['f1_score']:.4f}\n\n")
                
                f.write("ğŸ¯ åˆ†ç±»æŠ¥å‘Š\n")
                f.write("-" * 40 + "\n")
                for class_name in results['class_names']:
                    if class_name in results['classification_report']:
                        report = results['classification_report'][class_name]
                        f.write(f"\nç±»åˆ«: {class_name}\n")
                        f.write(f"  ç²¾ç¡®ç‡: {report['precision']:.4f}\n")
                        f.write(f"  å¬å›ç‡: {report['recall']:.4f}\n")
                        f.write(f"  F1åˆ†æ•°: {report['f1-score']:.4f}\n")
                        f.write(f"  æ”¯æŒæ•°: {report['support']:.0f}\n")
            
            logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {json_path}")
            logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def run_validation(self):
        """è¿è¡ŒéªŒè¯"""
        try:
            logger.info("ğŸš€ å¼€å§‹ä¿®å¤ç‰ˆæ¨¡å‹éªŒè¯...")
            
            # 1. åŠ è½½æ¨¡å‹æƒé‡ï¼ˆè·å–è¯æ±‡è¡¨å’Œæ ‡ç­¾ç¼–ç å™¨ï¼‰
            self.load_model_weights()
            
            # 2. åŠ è½½å¹¶å‡†å¤‡æ•°æ®
            df, texts, features, labels = self.load_and_prepare_data()
            
            # 3. åˆ›å»ºæ¨¡å‹ç»“æ„
            self.create_model()
            
            # 4. éªŒè¯æ¨¡å‹
            results = self.validate_model(texts, features, labels, "fixed_validation")
            
            # 5. ä¿å­˜ç»“æœ
            self.save_results(results)
            
            logger.info("ğŸ‰ ä¿®å¤ç‰ˆéªŒè¯å®Œæˆï¼")
            
            # æ‰“å°æ‘˜è¦
            print("\n" + "=" * 60)
            print("ğŸ¯ ä¿®å¤ç‰ˆéªŒè¯å®Œæˆæ‘˜è¦")
            print("=" * 60)
            print(f"ğŸ“Š å‡†ç¡®ç‡: {results['accuracy']:.4f}")
            print(f"ğŸ“Š F1åˆ†æ•°: {results['f1_score']:.4f}")
            print(f"ğŸ“ ç»“æœä¿å­˜ä½ç½®: final_validation_results/")
            print("=" * 60)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ä¿®å¤ç‰ˆéªŒè¯å¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä¿®å¤ç‰ˆæ¨¡å‹éªŒè¯è„šæœ¬')
    parser.add_argument('--model_path', required=True, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_path', required=True, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºéªŒè¯å™¨
        runner = FixedModelRunner(args.model_path, args.data_path)
        
        # è¿è¡ŒéªŒè¯
        results = runner.run_validation()
        
    except Exception as e:
        logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
