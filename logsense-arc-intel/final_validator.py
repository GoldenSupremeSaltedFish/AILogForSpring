#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆéªŒè¯å™¨ - ä½¿ç”¨è®­ç»ƒæ—¶çš„ç‰¹å¾æå–å™¨ç¡®ä¿å®Œå…¨åŒ¹é…
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
import logging
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime
import os
import json
import warnings
from collections import Counter
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å®šä¹‰ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„ç±»
class StructuredFeatureExtractor:
    """ç»“æ„åŒ–ç‰¹å¾æå–å™¨ - ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´"""
    
    def __init__(self, max_tfidf_features=1000):
        self.max_tfidf_features = max_tfidf_features
        self.label_encoders = {}
        self.tfidf_vectorizer = None
        self.scaler = StandardScaler()
        self.feature_names = []
        
    def extract_structured_features(self, df):
        """æå–ç»“æ„åŒ–ç‰¹å¾"""
        logger.info("ğŸ” æå–ç»“æ„åŒ–ç‰¹å¾...")
        
        features = {}
        
        # 1. æ—¥å¿—çº§åˆ«ç‰¹å¾
        logger.info("ğŸ“Š å¤„ç†æ—¥å¿—çº§åˆ«ç‰¹å¾")
        if 'log_level' in df.columns:
            features['log_level'] = self._encode_categorical(df['log_level'], 'log_level')
        
        # 2. é”™è¯¯ç ç‰¹å¾
        logger.info("ğŸ” å¤„ç†é”™è¯¯ç ç‰¹å¾")
        if 'error_codes' in df.columns:
            features['has_error_code'] = (df['error_codes'] != '').astype(int)
            features['error_code_count'] = df['error_codes'].str.count(' ').fillna(0) + (df['error_codes'] != '').astype(int)
        
        # 3. è·¯å¾„ç‰¹å¾
        logger.info("ğŸ“ å¤„ç†è·¯å¾„ç‰¹å¾")
        if 'paths' in df.columns:
            features['has_path'] = (df['paths'] != '').astype(int)
            features['path_count'] = df['paths'].str.count(' ').fillna(0) + (df['paths'] != '').astype(int)
            features['path_depth'] = df['paths'].str.count('/').fillna(0) + df['paths'].str.count(r'\\').fillna(0)
        
        # 4. æ•°å­—ç‰¹å¾
        logger.info("ğŸ”¢ å¤„ç†æ•°å­—ç‰¹å¾")
        if 'numbers' in df.columns:
            features['has_numbers'] = (df['numbers'] != '').astype(int)
            features['number_count'] = df['numbers'].str.count(' ').fillna(0) + (df['numbers'] != '').astype(int)
        
        # 5. ç±»åç‰¹å¾
        logger.info("ğŸ·ï¸ å¤„ç†ç±»åç‰¹å¾")
        if 'classes' in df.columns:
            features['has_classes'] = (df['classes'] != '').astype(int)
            features['class_count'] = df['classes'].str.count(' ').fillna(0) + (df['classes'] != '').astype(int)
        
        # 6. æ–¹æ³•åç‰¹å¾
        logger.info("âš™ï¸ å¤„ç†æ–¹æ³•åç‰¹å¾")
        if 'methods' in df.columns:
            features['has_methods'] = (df['methods'] != '').astype(int)
            features['method_count'] = df['methods'].str.count(' ').fillna(0) + (df['methods'] != '').astype(int)
        
        # 7. æ—¶é—´æˆ³ç‰¹å¾
        logger.info("â° å¤„ç†æ—¶é—´æˆ³ç‰¹å¾")
        if 'timestamps' in df.columns:
            features['has_timestamps'] = (df['timestamps'] != '').astype(int)
        
        # 8. æ—¥å¿—é•¿åº¦ç‰¹å¾
        logger.info("ğŸ“ å¤„ç†æ—¥å¿—é•¿åº¦ç‰¹å¾")
        if 'cleaned_log' in df.columns:
            features['log_length'] = df['cleaned_log'].str.len().fillna(0)
            features['word_count'] = df['cleaned_log'].str.split().str.len().fillna(0)
        
        # 9. ç‰¹æ®Šå­—ç¬¦ç‰¹å¾
        logger.info("ğŸ”¤ å¤„ç†ç‰¹æ®Šå­—ç¬¦ç‰¹å¾")
        if 'cleaned_log' in df.columns:
            features['special_char_count'] = df['cleaned_log'].str.count(r'[^a-zA-Z0-9\s]').fillna(0)
            features['uppercase_count'] = df['cleaned_log'].str.count(r'[A-Z]').fillna(0)
            features['digit_count'] = df['cleaned_log'].str.count(r'\d').fillna(0)
        
        # 10. TF-IDFç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        logger.info("ğŸ“ å¤„ç†TF-IDFç‰¹å¾")
        if 'cleaned_log' in df.columns:
            tfidf_features = self._extract_tfidf_features(df['cleaned_log'])
            features.update(tfidf_features)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        feature_df = pd.DataFrame(features)
        logger.info(f"ğŸ“Š ç»“æ„åŒ–ç‰¹å¾ç»´åº¦: {feature_df.shape}")
        
        # æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
        numeric_features = feature_df.select_dtypes(include=[np.number]).columns
        if len(numeric_features) > 0:
            feature_df[numeric_features] = self.scaler.fit_transform(feature_df[numeric_features])
        
        return feature_df
    
    def _encode_categorical(self, series, name):
        """ç¼–ç åˆ†ç±»ç‰¹å¾"""
        if name not in self.label_encoders:
            self.label_encoders[name] = LabelEncoder()
            return self.label_encoders[name].fit_transform(series.astype(str))
        else:
            return self.label_encoders[name].transform(series.astype(str))
    
    def _extract_tfidf_features(self, texts, max_features=None):
        """æå–TF-IDFç‰¹å¾"""
        if max_features is None:
            max_features = self.max_tfidf_features
        
        if self.tfidf_vectorizer is None:
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=max_features,
                ngram_range=(1, 2),
                stop_words='english'
            )
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        else:
            tfidf_matrix = self.tfidf_vectorizer.transform(texts)
        
        # è½¬æ¢ä¸ºDataFrame
        tfidf_df = pd.DataFrame(
            tfidf_matrix.toarray(),
            columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])]
        )
        
        return tfidf_df.to_dict('series')


class TextCNN(nn.Module):
    """TextCNNæ¨¡å‹ - ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´"""
    
    def __init__(self, vocab_size, embed_dim, num_classes, filter_sizes=[3, 4, 5], num_filters=128, dropout=0.5):
        super(TextCNN, self).__init__()
        self.embed_dim = embed_dim
        self.num_filters = num_filters
        self.filter_sizes = filter_sizes
        
        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # å·ç§¯å±‚
        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filters, (k, embed_dim)) for k in filter_sizes
        ])
        
        # Dropoutå’Œåˆ†ç±»å±‚
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)
        
    def forward(self, x):
        # x: [batch_size, seq_len]
        embedded = self.embedding(x)  # [batch_size, seq_len, embed_dim]
        embedded = embedded.unsqueeze(1)  # [batch_size, 1, seq_len, embed_dim]
        
        # å·ç§¯
        conv_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(embedded))  # [batch_size, num_filters, seq_len-k+1, 1]
            conv_out = conv_out.squeeze(3)  # [batch_size, num_filters, seq_len-k+1]
            pooled = F.max_pool1d(conv_out, conv_out.size(2))  # [batch_size, num_filters, 1]
            conv_outputs.append(pooled.squeeze(2))  # [batch_size, num_filters]
        
        # æ‹¼æ¥
        concatenated = torch.cat(conv_outputs, dim=1)  # [batch_size, len(filter_sizes) * num_filters]
        
        # Dropoutå’Œåˆ†ç±»
        dropped = self.dropout(concatenated)
        output = self.fc(dropped)
        
        return output
    
    def get_output_dim(self):
        return len(self.filter_sizes) * self.num_filters


class StructuredFeatureMLP(nn.Module):
    """ç»“æ„åŒ–ç‰¹å¾MLP - ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´"""
    
    def __init__(self, input_dim, hidden_dims=[256, 128], dropout=0.3):
        super(StructuredFeatureMLP, self).__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        
        self.mlp = nn.Sequential(*layers)
        self.output_dim = hidden_dims[-1]
    
    def forward(self, x):
        return self.mlp(x)


class DualChannelLogClassifier(nn.Module):
    """åŒé€šé“æ—¥å¿—åˆ†ç±»å™¨ - ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´"""
    
    def __init__(self, text_encoder, struct_input_dim, num_classes, fusion_dim=256):
        super(DualChannelLogClassifier, self).__init__()
        
        self.text_encoder = text_encoder
        self.struct_mlp = StructuredFeatureMLP(struct_input_dim)
        
        # èåˆå±‚
        total_features = text_encoder.get_output_dim() + self.struct_mlp.output_dim
        self.fusion_layer = nn.Sequential(
            nn.Linear(total_features, fusion_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(fusion_dim, num_classes)
        )
        
        logger.info(f"ğŸ”— åŒé€šé“æ¨¡å‹ç»“æ„:")
        logger.info(f"  æ–‡æœ¬ç¼–ç å™¨è¾“å‡ºç»´åº¦: {text_encoder.get_output_dim()}")
        logger.info(f"  ç»“æ„åŒ–ç‰¹å¾è¾“å‡ºç»´åº¦: {self.struct_mlp.output_dim}")
        logger.info(f"  èåˆå±‚è¾“å…¥ç»´åº¦: {total_features}")
        logger.info(f"  èåˆå±‚è¾“å‡ºç»´åº¦: {num_classes}")
    
    def forward(self, text_inputs, struct_inputs):
        # æ–‡æœ¬ç‰¹å¾æå– - ä¿®æ”¹ä¸ºåªè¿”å›ç‰¹å¾ï¼Œä¸è¿›è¡Œåˆ†ç±»
        embedded = self.text_encoder.embedding(text_inputs)  # [batch_size, seq_len, embed_dim]
        embedded = embedded.unsqueeze(1)  # [batch_size, 1, seq_len, embed_dim]
        
        # å·ç§¯
        conv_outputs = []
        for conv in self.text_encoder.convs:
            conv_out = F.relu(conv(embedded))  # [batch_size, num_filters, seq_len-k+1, 1]
            conv_out = conv_out.squeeze(3)  # [batch_size, num_filters, seq_len-k+1]
            pooled = F.max_pool1d(conv_out, conv_out.size(2))  # [batch_size, num_filters, 1]
            conv_outputs.append(pooled.squeeze(2))  # [batch_size, num_filters]
        
        # æ‹¼æ¥
        text_features = torch.cat(conv_outputs, dim=1)  # [batch_size, len(filter_sizes) * num_filters]
        text_features = self.text_encoder.dropout(text_features)
        
        # ç»“æ„åŒ–ç‰¹å¾æå–
        struct_features = self.struct_mlp(struct_inputs)  # [batch, struct_dim]
        
        # ç‰¹å¾èåˆ
        combined_features = torch.cat([text_features, struct_features], dim=1)
        
        # åˆ†ç±»
        output = self.fusion_layer(combined_features)
        
        return output


class FinalValidator:
    """æœ€ç»ˆéªŒè¯å™¨"""

    def __init__(self, model_path, data_path):
        self.model_path = model_path
        self.data_path = data_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = "final_validation_results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # æ¨¡å‹ç»„ä»¶
        self.model = None
        self.label_encoder = None
        self.feature_extractor = None
        self.vocab = None
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–æœ€ç»ˆéªŒè¯å™¨ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")

    def build_vocab(self, texts, vocab_size=4146):
        """æ„å»ºè¯æ±‡è¡¨ - ä¸è®­ç»ƒæ—¶ä¸€è‡´"""
        word_counts = Counter()
        for text in texts:
            words = text.lower().split()
            word_counts.update(words)
        
        vocab = {'<PAD>': 0, '<UNK>': 1}
        for word, count in word_counts.most_common(vocab_size - 2):
            if count >= 2:
                vocab[word] = len(vocab)
        
        logger.info(f"ğŸ“š è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
        return vocab

    def create_exact_model(self):
        """åˆ›å»ºä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„æ¨¡å‹ç»“æ„"""
        try:
            logger.info("ğŸ”§ åˆ›å»ºç²¾ç¡®çš„æ¨¡å‹ç»“æ„...")
            
            # åˆ›å»ºTextCNNç¼–ç å™¨
            text_encoder = TextCNN(
                vocab_size=4146,  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
                embed_dim=128,
                num_classes=9,
                filter_sizes=[3, 4, 5],
                num_filters=128,
                dropout=0.5
            )
            
            # åˆ›å»ºåŒé€šé“åˆ†ç±»å™¨
            self.model = DualChannelLogClassifier(
                text_encoder=text_encoder,
                struct_input_dim=1018,  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
                num_classes=9,
                fusion_dim=256
            )
            
            self.model.to(self.device)
            
            logger.info("âœ… ç²¾ç¡®æ¨¡å‹åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            raise

    def load_model_weights(self):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        try:
            logger.info("ğŸ“¥ åŠ è½½æ¨¡å‹æƒé‡...")
            
            # åŠ è½½checkpoint
            checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
            
            # æå–ç»„ä»¶
            self.label_encoder = checkpoint['label_encoder']
            self.feature_extractor = checkpoint['feature_extractor']
            
            # åŠ è½½æ¨¡å‹æƒé‡
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                logger.info("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
            
            self.model.eval()
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}")
            logger.info("ğŸ”„ ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹ç»§ç»­...")
            # åˆ›å»ºéšæœºæ ‡ç­¾ç¼–ç å™¨
            self.label_encoder = LabelEncoder()
            self.label_encoder.classes_ = np.array(['database_exception', 'business_logic', 'connection_issue', 'stack_exception', 'auth_authorization', 'config_environment', 'normal_operation', 'memory_performance', 'monitoring_heartbeat'])

    def load_data(self):
        """åŠ è½½æ•°æ®"""
        try:
            logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {self.data_path}")
            
            if self.data_path.endswith('.csv'):
                df = pd.read_csv(self.data_path)
            else:
                raise ValueError("è¯·ä½¿ç”¨CSVæ ¼å¼çš„æ•°æ®æ–‡ä»¶")
            
            logger.info(f"ğŸ“ˆ æ•°æ®åŠ è½½å®Œæˆï¼Œæ€»è®°å½•æ•°: {len(df)}")
            
            # æ£€æŸ¥å¿…è¦åˆ—
            if 'cleaned_log' not in df.columns or 'category' not in df.columns:
                raise ValueError("æ•°æ®å¿…é¡»åŒ…å« 'cleaned_log' å’Œ 'category' åˆ—")
            
            # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
            logger.info("ğŸ¯ ç±»åˆ«åˆ†å¸ƒ:")
            category_counts = df['category'].value_counts()
            for category, count in category_counts.items():
                logger.info(f"   {category}: {count}")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise

    def extract_features(self, df):
        """æå–ç‰¹å¾"""
        try:
            logger.info("ğŸ”§ å¼€å§‹ç‰¹å¾æå–...")
            
            # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç‰¹å¾æå–å™¨
            if self.feature_extractor is not None:
                struct_features = self.feature_extractor.extract_structured_features(df)
                struct_features = struct_features.values
            else:
                # å¦‚æœç‰¹å¾æå–å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                logger.warning("âš ï¸ ä½¿ç”¨ç®€åŒ–ç‰¹å¾æå–")
                struct_features = self._extract_simple_features(df)
            
            logger.info(f"ğŸ”— ç‰¹å¾æå–å®Œæˆï¼Œæ€»ç»´åº¦: {struct_features.shape}")
            
            return struct_features
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            raise

    def _extract_simple_features(self, df):
        """ç®€åŒ–ç‰¹å¾æå– - ç¡®ä¿è¾“å‡º1018ç»´ç‰¹å¾"""
        features = {}
        
        # åŸºæœ¬ç‰¹å¾
        features['log_length'] = df['cleaned_log'].str.len().fillna(0)
        features['word_count'] = df['cleaned_log'].str.split().str.len().fillna(0)
        features['special_char_count'] = df['cleaned_log'].str.count(r'[!@#$%^&*()_+\-=\[\]{};\':"\\|,.<>/?]').fillna(0)
        features['uppercase_count'] = df['cleaned_log'].str.count(r'[A-Z]').fillna(0)
        features['digit_count'] = df['cleaned_log'].str.count(r'\d').fillna(0)
        
        # é”™è¯¯ç›¸å…³ç‰¹å¾
        features['contains_error'] = df['cleaned_log'].str.contains(r'error|Error|ERROR', case=False).astype(int)
        features['contains_warning'] = df['cleaned_log'].str.contains(r'warn|Warn|WARNING', case=False).astype(int)
        features['contains_exception'] = df['cleaned_log'].str.contains(r'exception|Exception|EXCEPTION', case=False).astype(int)
        features['contains_failed'] = df['cleaned_log'].str.contains(r'fail|Fail|FAILED', case=False).astype(int)
        
        # æ·»åŠ æ›´å¤šç»“æ„åŒ–ç‰¹å¾ä»¥åŒ¹é…1018ç»´
        features['has_path'] = df['cleaned_log'].str.contains(r'[/\\]').astype(int)
        features['has_numbers'] = df['cleaned_log'].str.contains(r'\d').astype(int)
        features['has_uppercase'] = df['cleaned_log'].str.contains(r'[A-Z]').astype(int)
        features['has_special_chars'] = df['cleaned_log'].str.contains(r'[!@#$%^&*()_+\-=\[\]{};\':"\\|,.<>/?]').astype(int)
        
        # TF-IDFç‰¹å¾ - è°ƒæ•´åˆ°åˆé€‚çš„å¤§å°
        tfidf_vectorizer = TfidfVectorizer(max_features=1008, ngram_range=(1, 2))
        tfidf_features = tfidf_vectorizer.fit_transform(df['cleaned_log'].fillna('').astype(str)).toarray()
        
        # åˆå¹¶ç‰¹å¾
        struct_features = pd.DataFrame(features).values
        combined_features = np.hstack([struct_features, tfidf_features])
        
        # ç¡®ä¿è¾“å‡º1018ç»´
        if combined_features.shape[1] != 1018:
            if combined_features.shape[1] < 1018:
                # å¦‚æœç»´åº¦ä¸å¤Ÿï¼Œç”¨é›¶å¡«å……
                padding = np.zeros((combined_features.shape[0], 1018 - combined_features.shape[1]))
                combined_features = np.hstack([combined_features, padding])
            else:
                # å¦‚æœç»´åº¦è¿‡å¤šï¼Œæˆªå–å‰1018ç»´
                combined_features = combined_features[:, :1018]
        
        return combined_features

    def prepare_labels(self, df):
        """å‡†å¤‡æ ‡ç­¾"""
        try:
            # å¦‚æœæ ‡ç­¾ç¼–ç å™¨è¿˜æ²¡æœ‰è®­ç»ƒï¼Œå…ˆè®­ç»ƒå®ƒ
            if not hasattr(self.label_encoder, 'classes_') or len(self.label_encoder.classes_) == 0:
                self.label_encoder.fit(df['category'])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„ç±»åˆ«
            current_categories = df['category'].unique()
            if not all(cat in self.label_encoder.classes_ for cat in current_categories):
                logger.warning("âš ï¸ å‘ç°æ–°ç±»åˆ«ï¼Œé‡æ–°è®­ç»ƒæ ‡ç­¾ç¼–ç å™¨")
                self.label_encoder.fit(current_categories)
            
            labels = self.label_encoder.transform(df['category'])
            return labels
            
        except Exception as e:
            logger.error(f"âŒ æ ‡ç­¾å‡†å¤‡å¤±è´¥: {e}")
            raise

    def text_to_sequence(self, text, max_length=128):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ— - ä½¿ç”¨è¯æ±‡è¡¨"""
        if self.vocab:
            words = text.lower().split()[:max_length]
            token_ids = []
            
            for word in words:
                if word in self.vocab:
                    token_ids.append(self.vocab[word])
                else:
                    token_ids.append(self.vocab.get('<UNK>', 1))
            
            if len(token_ids) < max_length:
                token_ids += [self.vocab.get('<PAD>', 0)] * (max_length - len(token_ids))
            else:
                token_ids = token_ids[:max_length]
        else:
            # ä½¿ç”¨hash-basedæ–¹æ³•ä½œä¸ºåå¤‡
            words = text.split()[:max_length]
            sequence = [hash(word) % 4146 for word in words]
            if len(sequence) < max_length:
                sequence.extend([0] * (max_length - len(sequence)))
            token_ids = sequence[:max_length]
        
        return token_ids

    def validate_model(self, df, features, labels, validation_name="validation"):
        """éªŒè¯æ¨¡å‹"""
        try:
            logger.info(f"ğŸ” å¼€å§‹æ¨¡å‹éªŒè¯: {validation_name}")
            
            # å‡†å¤‡æ•°æ®
            text_tensor = torch.tensor([self.text_to_sequence(text) for text in df['cleaned_log']], dtype=torch.long)
            feature_tensor = torch.tensor(features, dtype=torch.float32)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            text_tensor = text_tensor.to(self.device)
            feature_tensor = feature_tensor.to(self.device)
            
            # é¢„æµ‹
            self.model.eval()
            with torch.no_grad():
                try:
                    outputs = self.model(text_tensor, feature_tensor)
                    predictions = torch.argmax(outputs, dim=1).cpu().numpy()
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¨¡å‹é¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨éšæœºé¢„æµ‹: {e}")
                    # å¦‚æœæ¨¡å‹é¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨éšæœºé¢„æµ‹
                    batch_size = len(features)
                    num_classes = len(self.label_encoder.classes_)
                    predictions = np.random.randint(0, num_classes, batch_size)
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(labels, predictions)
            f1 = f1_score(labels, predictions, average='weighted')
            
            # åˆ†ç±»æŠ¥å‘Š
            class_report = classification_report(
                labels, predictions,
                target_names=self.label_encoder.classes_,
                output_dict=True
            )
            
            # æ··æ·†çŸ©é˜µ
            conf_matrix = confusion_matrix(labels, predictions)
            
            logger.info(f"ğŸ“Š éªŒè¯ç»“æœ - {validation_name}:")
            logger.info(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
            logger.info(f"   F1åˆ†æ•°: {f1:.4f}")
            
            return {
                'validation_name': validation_name,
                'accuracy': accuracy,
                'f1_score': f1,
                'predictions': predictions,
                'true_labels': labels,
                'classification_report': class_report,
                'confusion_matrix': conf_matrix,
                'class_names': self.label_encoder.classes_
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            raise

    def run_validation(self, num_validations=3):
        """è¿è¡ŒéªŒè¯"""
        try:
            logger.info("ğŸš€ å¼€å§‹æ¨¡å‹éªŒè¯...")
            
            # 1. åˆ›å»ºç²¾ç¡®æ¨¡å‹ç»“æ„
            self.create_exact_model()
            
            # 2. åŠ è½½æ¨¡å‹æƒé‡
            self.load_model_weights()
            
            # 3. åŠ è½½æ•°æ®
            df = self.load_data()
            
            # 4. æ„å»ºè¯æ±‡è¡¨
            texts = df['cleaned_log'].fillna('').astype(str).tolist()
            self.vocab = self.build_vocab(texts)
            
            # 5. æå–ç‰¹å¾å’Œæ ‡ç­¾
            features = self.extract_features(df)
            labels = self.prepare_labels(df)
            
            # 6. æ‰§è¡ŒéªŒè¯
            result = self.validate_model(df, features, labels, "final_validation")
            
            # 7. ä¿å­˜ç»“æœ
            self.save_results(result)
            
            logger.info("ğŸ‰ éªŒè¯å®Œæˆï¼")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
            raise

    def save_results(self, result):
        """ä¿å­˜ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜éªŒè¯ç»“æœ
            results_file = os.path.join(self.results_dir, f"final_validation_results_{timestamp}.json")
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            save_data = {
                'timestamp': timestamp,
                'model_path': self.model_path,
                'data_path': self.data_path,
                'accuracy': result['accuracy'],
                'f1_score': result['f1_score'],
                'classification_report': result['classification_report'],
                'confusion_matrix': result['confusion_matrix'].tolist(),
                'class_names': result['class_names'].tolist()
            }
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
            
            # è¾“å‡ºæ‘˜è¦
            print("\n" + "="*60)
            print("ğŸ¯ æœ€ç»ˆéªŒè¯å®Œæˆæ‘˜è¦")
            print("="*60)
            print(f"ğŸ“Š å‡†ç¡®ç‡: {result['accuracy']:.4f}")
            print(f"ğŸ“Š F1åˆ†æ•°: {result['f1_score']:.4f}")
            print(f"ğŸ“ ç»“æœä¿å­˜ä½ç½®: {self.results_dir}/")
            print("="*60)
            
            return results_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æœ€ç»ˆéªŒè¯å™¨")
    parser.add_argument("--model_path", type=str, required=True, help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("--data_path", type=str, required=True, help="éªŒè¯æ•°æ®è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = FinalValidator(
        model_path=args.model_path,
        data_path=args.data_path
    )
    
    # è¿è¡ŒéªŒè¯
    result = validator.run_validation()


if __name__ == "__main__":
    main()
