#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‡†å¤‡å®Œæ•´æ•°æ®é›† - åˆå¹¶æ‰€æœ‰11ç§ç±»åˆ«çš„æ—¥å¿—
"""

import os
import pandas as pd
import logging
from pathlib import Path
from typing import Dict, List

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç±»åˆ«æ˜ å°„
CATEGORY_MAPPING = {
    '01_å †æ ˆå¼‚å¸¸_stack_exception': 'stack_exception',
    '02_æ•°æ®åº“å¼‚å¸¸_database_exception': 'database_exception',
    '03_è¿æ¥é—®é¢˜_connection_issue': 'connection_issue',
    '04_è®¤è¯æˆæƒ_auth_authorization': 'auth_authorization',
    '05_é…ç½®ç¯å¢ƒ_config_environment': 'config_environment',
    '06_ä¸šåŠ¡é€»è¾‘_business_logic': 'business_logic',
    '07_æ­£å¸¸æ“ä½œ_normal_operation': 'normal_operation',
    '08_ç›‘æ§å¿ƒè·³_monitoring_heartbeat': 'monitoring_heartbeat',
    '09_å†…å­˜æ€§èƒ½_memory_performance': 'memory_performance',
    '10_è¶…æ—¶é”™è¯¯_timeout': 'timeout',
    '11_SpringBootå¯åŠ¨å¤±è´¥_spring_boot_startup_failure': 'spring_boot_startup_failure'
}

def load_category_data(data_output_path: str) -> pd.DataFrame:
    """åŠ è½½æ‰€æœ‰ç±»åˆ«çš„æ•°æ®"""
    logger.info(f"ğŸ“‚ å¼€å§‹åŠ è½½æ•°æ®: {data_output_path}")
    
    all_data = []
    
    for folder_name, category in CATEGORY_MAPPING.items():
        folder_path = os.path.join(data_output_path, folder_name)
        
        if not os.path.exists(folder_path):
            logger.warning(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {folder_path}")
            continue
            
        logger.info(f"ğŸ“ å¤„ç†ç±»åˆ«: {category} ({folder_name})")
        
        # æŸ¥æ‰¾CSVæ–‡ä»¶
        csv_files = list(Path(folder_path).glob("*.csv"))
        
        if not csv_files:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°CSVæ–‡ä»¶: {folder_path}")
            continue
            
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                logger.info(f"  ğŸ“„ åŠ è½½æ–‡ä»¶: {csv_file.name} - {len(df)} æ¡è®°å½•")
                
                # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
                if 'original_log' not in df.columns:
                    logger.warning(f"âš ï¸ æ–‡ä»¶ç¼ºå°‘original_logåˆ—: {csv_file.name}")
                    continue
                
                # æ·»åŠ ç±»åˆ«ä¿¡æ¯
                df['category'] = category
                all_data.append(df)
                
            except Exception as e:
                logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {csv_file.name} - {e}")
                continue
    
    if not all_data:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®")
        return pd.DataFrame()
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    combined_df = pd.concat(all_data, ignore_index=True)
    logger.info(f"âœ… æ•°æ®åˆå¹¶å®Œæˆ - æ€»è®¡: {len(combined_df)} æ¡è®°å½•")
    
    return combined_df

def analyze_data_distribution(df: pd.DataFrame) -> Dict:
    """åˆ†ææ•°æ®åˆ†å¸ƒ"""
    logger.info("ğŸ“Š åˆ†ææ•°æ®åˆ†å¸ƒ:")
    
    category_counts = df['category'].value_counts()
    total_samples = len(df)
    
    distribution = {}
    for category, count in category_counts.items():
        percentage = (count / total_samples) * 100
        distribution[category] = {
            'count': count,
            'percentage': percentage
        }
        logger.info(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
    
    # æ£€æŸ¥æ•°æ®ä¸å¹³è¡¡
    max_count = category_counts.max()
    min_count = category_counts.min()
    imbalance_ratio = max_count / min_count
    
    logger.info(f"ğŸ“ˆ æ•°æ®ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1")
    
    return {
        'total_samples': total_samples,
        'category_distribution': distribution,
        'imbalance_ratio': imbalance_ratio,
        'num_categories': len(category_counts)
    }

def clean_and_prepare_data(df: pd.DataFrame) -> pd.DataFrame:
    """æ¸…æ´—å’Œå‡†å¤‡æ•°æ®"""
    logger.info("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—")
    
    # è®°å½•åŸå§‹æ•°æ®é‡
    original_count = len(df)
    
    # ç§»é™¤ç©ºå€¼
    df_cleaned = df.dropna(subset=['original_log', 'category'])
    
    # ç§»é™¤ç©ºå­—ç¬¦ä¸²
    df_cleaned = df_cleaned[df_cleaned['original_log'].str.strip() != '']
    
    # ç§»é™¤é‡å¤æ•°æ®
    df_cleaned = df_cleaned.drop_duplicates(subset=['original_log'])
    
    # è®°å½•æ¸…æ´—åçš„æ•°æ®é‡
    cleaned_count = len(df_cleaned)
    removed_count = original_count - cleaned_count
    
    logger.info(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ:")
    logger.info(f"  åŸå§‹æ•°æ®: {original_count} æ¡")
    logger.info(f"  æ¸…æ´—åæ•°æ®: {cleaned_count} æ¡")
    logger.info(f"  ç§»é™¤æ•°æ®: {removed_count} æ¡")
    
    return df_cleaned

def save_processed_data(df: pd.DataFrame, output_path: str):
    """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    df.to_csv(output_path, index=False, encoding='utf-8')
    logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜: {output_path}")
    
    # ä¿å­˜æ•°æ®ä¿¡æ¯
    info_path = output_path.replace('.csv', '_info.txt')
    with open(info_path, 'w', encoding='utf-8') as f:
        f.write(f"æ•°æ®æ–‡ä»¶ä¿¡æ¯\n")
        f.write(f"============\n")
        f.write(f"æ–‡ä»¶è·¯å¾„: {output_path}\n")
        f.write(f"æ€»è®°å½•æ•°: {len(df)}\n")
        f.write(f"ç±»åˆ«æ•°é‡: {df['category'].nunique()}\n")
        f.write(f"åˆ—å: {list(df.columns)}\n\n")
        
        f.write(f"ç±»åˆ«åˆ†å¸ƒ:\n")
        category_counts = df['category'].value_counts()
        for category, count in category_counts.items():
            percentage = (count / len(df)) * 100
            f.write(f"  {category}: {count} æ¡ ({percentage:.1f}%)\n")
    
    logger.info(f"ğŸ“‹ æ•°æ®ä¿¡æ¯å·²ä¿å­˜: {info_path}")

def main():
    """ä¸»å‡½æ•°"""
    # æ•°æ®è·¯å¾„
    data_output_path = r"C:\Users\30871\Desktop\AILogForSpring\DATA_OUTPUT"
    output_path = "data/processed_logs_full.csv"
    
    logger.info("ğŸš€ å¼€å§‹å‡†å¤‡å®Œæ•´æ•°æ®é›†")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(data_output_path):
        logger.error(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_output_path}")
        return
    
    # åŠ è½½æ‰€æœ‰ç±»åˆ«æ•°æ®
    df = load_category_data(data_output_path)
    
    if df.empty:
        logger.error("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®")
        return
    
    # åˆ†ææ•°æ®åˆ†å¸ƒ
    data_info = analyze_data_distribution(df)
    
    # æ¸…æ´—æ•°æ®
    df_cleaned = clean_and_prepare_data(df)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    save_processed_data(df_cleaned, output_path)
    
    logger.info("âœ… æ•°æ®å‡†å¤‡å®Œæˆ!")
    logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
    logger.info(f"ğŸ“Š æœ€ç»ˆæ•°æ®: {len(df_cleaned)} æ¡è®°å½•, {df_cleaned['category'].nunique()} ä¸ªç±»åˆ«")

if __name__ == "__main__":
    main() 