#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç† - æ¸…ç†æ—¥å¿—æ•°æ®ï¼Œç§»é™¤GitHubå…ƒæ•°æ®
"""

import os
import pandas as pd
import numpy as np
import logging
import re
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def clean_log_content(text):
    """æ¸…ç†æ—¥å¿—å†…å®¹"""
    if pd.isna(text) or text == '':
        return ''
    
    text = str(text)
    
    # ç§»é™¤GitHubå…ƒæ•°æ®
    patterns = [
        r'github\.com/[^,\s]+',
        r'https://github\.com/[^,\s]+',
        r'github_issue',
        r'unknown,github_issue',
        r'[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]+',
        r'[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+,\d+',
        r'https://github\.com/[^,\s]+/issues/\d+',
    ]
    
    for pattern in patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'&[a-zA-Z]+;', '', text)
    text = re.sub(r'\[.*?\]', '', text)
    
    # æ¸…ç†å¤šä½™å­—ç¬¦
    text = re.sub(r'^,+|,+$', '', text)
    text = re.sub(r'^"+|"+$', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text

def is_valid_log(text):
    """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ—¥å¿—å†…å®¹"""
    if pd.isna(text) or text == '':
        return False
    
    text = str(text).lower()
    log_keywords = ['error', 'warn', 'info', 'debug', 'exception', 'java', 'spring', 'at ', 'caused by']
    has_keyword = any(keyword in text for keyword in log_keywords)
    has_length = len(text) > 20
    
    return has_keyword or has_length

def process_category_data(category_path, category_name):
    """å¤„ç†å•ä¸ªç±»åˆ«çš„æ•°æ®"""
    logger.info(f"ğŸ“ å¤„ç†ç±»åˆ«: {category_name}")
    
    all_data = []
    
    # éå†ç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶
    for csv_file in Path(category_path).glob("*.csv"):
        try:
            df = pd.read_csv(csv_file)
            logger.info(f"  ğŸ“„ è¯»å–æ–‡ä»¶: {csv_file.name} - {len(df)} æ¡è®°å½•")
            
            # æ¸…ç†æ•°æ®
            cleaned_data = []
            for idx, row in df.iterrows():
                original_log = row.get('original_log', '')
                
                # æ¸…ç†æ—¥å¿—å†…å®¹
                cleaned_log = clean_log_content(original_log)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ—¥å¿—
                if is_valid_log(cleaned_log):
                    cleaned_data.append({
                        'original_log': cleaned_log,
                        'category': category_name,
                        'log_level': row.get('log_level', 'UNKNOWN'),
                        'content_type': row.get('content_type', ''),
                        'priority': row.get('priority', ''),
                        'source_file': csv_file.name
                    })
            
            logger.info(f"  âœ… æ¸…ç†å: {len(cleaned_data)} æ¡æœ‰æ•ˆè®°å½•")
            all_data.extend(cleaned_data)
            
        except Exception as e:
            logger.error(f"  âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {csv_file.name} - {e}")
    
    return pd.DataFrame(all_data)

def main():
    """ä¸»å‡½æ•°"""
    data_path = r"C:\Users\30871\Desktop\AILogForSpring\DATA_OUTPUT"
    output_path = "data/processed_logs_cleaned.csv"
    
    logger.info("ğŸ¯ æ•°æ®é¢„å¤„ç† - æ¸…ç†æ—¥å¿—æ•°æ®")
    logger.info(f"ğŸ“ è¾“å…¥è·¯å¾„: {data_path}")
    logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
    
    # æ£€æŸ¥è¾“å…¥è·¯å¾„
    if not os.path.exists(data_path):
        logger.error(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {data_path}")
        return
    
    # ç±»åˆ«æ˜ å°„
    category_mapping = {
        '01_å †æ ˆå¼‚å¸¸_stack_exception': 'stack_exception',
        '02_æ•°æ®åº“å¼‚å¸¸_database_exception': 'database_exception',
        '03_è¿æ¥é—®é¢˜_connection_issue': 'connection_issue',
        '04_è®¤è¯æˆæƒ_auth_authorization': 'auth_authorization',
        '05_é…ç½®ç¯å¢ƒ_config_environment': 'config_environment',
        '06_ä¸šåŠ¡é€»è¾‘_business_logic': 'business_logic',
        '07_æ­£å¸¸æ“ä½œ_normal_operation': 'normal_operation',
        '08_ç›‘æ§å¿ƒè·³_monitoring_heartbeat': 'monitoring_heartbeat',
        '09_å†…å­˜æ€§èƒ½_memory_performance': 'memory_performance',
        '10_è¶…æ—¶é”™è¯¯_timeout': 'timeout',
        '11_SpringBootå¯åŠ¨å¤±è´¥_spring_boot_startup_failure': 'spring_boot_startup_failure'
    }
    
    all_cleaned_data = []
    
    # å¤„ç†æ¯ä¸ªç±»åˆ«
    for folder_name, category in category_mapping.items():
        folder_path = os.path.join(data_path, folder_name)
        
        if not os.path.exists(folder_path):
            logger.warning(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {folder_path}")
            continue
        
        # å¤„ç†è¯¥ç±»åˆ«çš„æ•°æ®
        category_df = process_category_data(folder_path, category)
        
        if len(category_df) > 0:
            all_cleaned_data.append(category_df)
            logger.info(f"âœ… {category}: {len(category_df)} æ¡è®°å½•")
        else:
            logger.warning(f"âš ï¸ {category}: æ²¡æœ‰æœ‰æ•ˆè®°å½•")
    
    if not all_cleaned_data:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
        return
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    combined_df = pd.concat(all_cleaned_data, ignore_index=True)
    
    # æ•°æ®ç»Ÿè®¡
    logger.info("ğŸ“Š é¢„å¤„ç†ç»“æœç»Ÿè®¡:")
    category_counts = combined_df['category'].value_counts()
    for category, count in category_counts.items():
        logger.info(f"  {category}: {count} æ¡")
    
    # ä¿å­˜æ¸…ç†åçš„æ•°æ®
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    combined_df.to_csv(output_path, index=False, encoding='utf-8')
    
    logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ! ä¿å­˜åˆ°: {output_path}")
    logger.info(f"ğŸ“Š æ€»è®°å½•æ•°: {len(combined_df)}")
    
    # æ˜¾ç¤ºä¸€äº›æ¸…ç†åçš„æ ·æœ¬
    logger.info("ğŸ“ æ¸…ç†åçš„æ ·æœ¬:")
    for i, row in combined_df.head(5).iterrows():
        logger.info(f"  {i+1}. {row['category']}: {row['original_log'][:100]}...")

if __name__ == "__main__":
    main() 