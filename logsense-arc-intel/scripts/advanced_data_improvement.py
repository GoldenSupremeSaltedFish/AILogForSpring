#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ•°æ®æ”¹è¿›ç­–ç•¥
"""

import os
import pandas as pd
import numpy as np
import logging
from collections import Counter
import random
import json

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedDataAugmenter:
    def __init__(self):
        self.synonyms = {
            'error': ['exception', 'failure', 'issue', 'problem', 'bug'],
            'failed': ['failed', 'error', 'exception', 'problem', 'broken'],
            'exception': ['error', 'failure', 'issue', 'problem', 'crash'],
            'connection': ['link', 'connect', 'network', 'socket', 'channel'],
            'database': ['db', 'datastore', 'repository', 'storage', 'table'],
            'timeout': ['timeout', 'expired', 'timed_out', 'overdue', 'deadline'],
            'memory': ['ram', 'storage', 'cache', 'buffer', 'heap'],
            'performance': ['speed', 'efficiency', 'throughput', 'latency', 'response'],
            'authentication': ['auth', 'login', 'verify', 'validate', 'check'],
            'authorization': ['permission', 'access', 'rights', 'privilege', 'role'],
            'configuration': ['config', 'setting', 'parameter', 'option', 'property'],
            'environment': ['env', 'surrounding', 'context', 'condition', 'setup'],
            'business': ['commercial', 'enterprise', 'corporate', 'trade', 'service'],
            'logic': ['reasoning', 'algorithm', 'process', 'method', 'function'],
            'operation': ['action', 'task', 'process', 'function', 'work'],
            'monitoring': ['watch', 'observe', 'track', 'supervise', 'check'],
            'heartbeat': ['pulse', 'signal', 'alive', 'status', 'ping'],
            'startup': ['boot', 'launch', 'initialize', 'begin', 'start'],
            'spring': ['spring', 'framework', 'boot', 'application', 'context']
        }
        
        self.insert_words = [
            'error', 'exception', 'failed', 'timeout', 'connection',
            'database', 'memory', 'performance', 'authentication',
            'authorization', 'configuration', 'environment', 'business',
            'logic', 'operation', 'monitoring', 'heartbeat', 'startup'
        ]
        
        self.delete_prob = 0.15
        self.insert_prob = 0.25
        self.replace_prob = 0.3
        self.swap_prob = 0.1
    
    def augment_text_advanced(self, text: str, num_augmentations: int = 5) -> list:
        augmented_texts = []
        
        for _ in range(num_augmentations):
            augmented = text
            
            # éšæœºåˆ é™¤å•è¯
            if random.random() < self.delete_prob:
                words = augmented.split()
                if len(words) > 3:
                    delete_idx = random.randint(0, len(words) - 1)
                    words.pop(delete_idx)
                    augmented = ' '.join(words)
            
            # éšæœºæ’å…¥å•è¯
            if random.random() < self.insert_prob:
                words = augmented.split()
                if len(words) > 0:
                    insert_word = random.choice(self.insert_words)
                    insert_idx = random.randint(0, len(words))
                    words.insert(insert_idx, insert_word)
                    augmented = ' '.join(words)
            
            # éšæœºæ›¿æ¢å•è¯
            if random.random() < self.replace_prob:
                words = augmented.split()
                for i, word in enumerate(words):
                    if word.lower() in self.synonyms and random.random() < 0.4:
                        synonyms = self.synonyms[word.lower()]
                        words[i] = random.choice(synonyms)
                augmented = ' '.join(words)
            
            # éšæœºäº¤æ¢ç›¸é‚»å•è¯
            if random.random() < self.swap_prob:
                words = augmented.split()
                if len(words) > 1:
                    swap_idx = random.randint(0, len(words) - 2)
                    words[swap_idx], words[swap_idx + 1] = words[swap_idx + 1], words[swap_idx]
                    augmented = ' '.join(words)
            
            if augmented != text:
                augmented_texts.append(augmented)
        
        return augmented_texts
    
    def augment_category_advanced(self, df: pd.DataFrame, category: str, target_count: int = 500) -> pd.DataFrame:
        category_df = df[df['category'] == category].copy()
        current_count = len(category_df)
        
        if current_count >= target_count:
            logger.info(f"  {category}: å·²æœ‰ {current_count} æ¡ï¼Œæ— éœ€å¢å¼º")
            return category_df
        
        needed_count = target_count - current_count
        logger.info(f"  {category}: å½“å‰ {current_count} æ¡ï¼Œéœ€è¦å¢åŠ åˆ° {target_count} æ¡")
        
        augmented_data = []
        
        # å¯¹ç°æœ‰æ ·æœ¬è¿›è¡Œå¢å¼º
        for idx, row in category_df.iterrows():
            text = row['original_log']
            augmented_texts = self.augment_text_advanced(text, num_augmentations=3)
            
            for aug_text in augmented_texts:
                if len(augmented_data) < needed_count:
                    new_row = row.copy()
                    new_row['original_log'] = aug_text
                    new_row['augmented'] = True
                    augmented_data.append(new_row)
        
        # å¦‚æœè¿˜ä¸å¤Ÿï¼Œç»§ç»­å¢å¼º
        while len(augmented_data) < needed_count and len(category_df) > 0:
            sample_row = category_df.sample(n=1).iloc[0]
            text = sample_row['original_log']
            augmented_texts = self.augment_text_advanced(text, num_augmentations=2)
            
            for aug_text in augmented_texts:
                if len(augmented_data) < needed_count:
                    new_row = sample_row.copy()
                    new_row['original_log'] = aug_text
                    new_row['augmented'] = True
                    augmented_data.append(new_row)
        
        # åˆå¹¶åŸå§‹æ•°æ®å’Œå¢å¼ºæ•°æ®
        augmented_df = pd.DataFrame(augmented_data)
        result_df = pd.concat([category_df, augmented_df], ignore_index=True)
        
        logger.info(f"  {category}: å¢å¼ºå {len(result_df)} æ¡")
        return result_df

def improve_data_advanced(input_path: str, output_path: str, target_samples: int = 500):
    logger.info("ğŸš€ å¼€å§‹é«˜çº§æ•°æ®æ”¹è¿›")
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv(input_path)
    logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {len(df)} æ¡è®°å½•")
    
    # æ•°æ®æ¸…æ´—
    df_cleaned = df.dropna(subset=['original_log', 'category'])
    df_cleaned = df_cleaned[df_cleaned['original_log'].str.strip() != '']
    
    # è¿‡æ»¤æ‰æ ·æœ¬å¤ªå°‘çš„ç±»åˆ«
    category_counts = df_cleaned['category'].value_counts()
    min_samples = 10
    valid_categories = category_counts[category_counts >= min_samples].index.tolist()
    df_filtered = df_cleaned[df_cleaned['category'].isin(valid_categories)]
    
    logger.info(f"ğŸ“Š è¿‡æ»¤åæ•°æ®: {len(df_filtered)} æ¡è®°å½•")
    logger.info(f"ğŸ“Š æœ‰æ•ˆç±»åˆ«: {len(valid_categories)} ä¸ª")
    
    # é«˜çº§æ•°æ®å¢å¼º
    logger.info("ğŸ”§ å¼€å§‹é«˜çº§æ•°æ®å¢å¼º")
    augmenter = AdvancedDataAugmenter()
    enhanced_dfs = []
    
    for category in valid_categories:
        category_df = df_filtered[df_filtered['category'] == category].copy()
        enhanced_df = augmenter.augment_category_advanced(category_df, category, target_samples)
        enhanced_dfs.append(enhanced_df)
    
    # åˆå¹¶å¢å¼ºåçš„æ•°æ®
    enhanced_df = pd.concat(enhanced_dfs, ignore_index=True)
    logger.info(f"ğŸ“Š å¢å¼ºåæ•°æ®: {len(enhanced_df)} æ¡è®°å½•")
    
    # ä¿å­˜æ”¹è¿›åçš„æ•°æ®
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    enhanced_df.to_csv(output_path, index=False, encoding='utf-8')
    
    # åˆ†ææœ€ç»ˆåˆ†å¸ƒ
    final_counts = enhanced_df['category'].value_counts()
    logger.info("ğŸ“Š æœ€ç»ˆæ•°æ®åˆ†å¸ƒ:")
    for category, count in final_counts.items():
        logger.info(f"  {category}: {count} æ¡")
    
    # è®¡ç®—ä¸å¹³è¡¡æ¯”ä¾‹
    max_count = final_counts.max()
    min_count = final_counts.min()
    imbalance_ratio = max_count / min_count
    
    logger.info(f"ğŸ“ˆ æœ€ç»ˆä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1")
    logger.info(f"âœ… é«˜çº§æ•°æ®æ”¹è¿›å®Œæˆ! ä¿å­˜åˆ°: {output_path}")
    
    return enhanced_df

def main():
    input_path = "data/processed_logs_full.csv"
    output_path = "data/processed_logs_advanced.csv"
    target_samples = 500
    
    logger.info("ğŸ¯ é«˜çº§æ•°æ®æ”¹è¿›ç­–ç•¥")
    logger.info(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_path}")
    logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
    logger.info(f"ğŸ¯ ç›®æ ‡æ¯ç±»æ ·æœ¬æ•°: {target_samples}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_path):
        logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    # æ‰§è¡Œé«˜çº§æ•°æ®æ”¹è¿›
    improved_df = improve_data_advanced(input_path, output_path, target_samples)
    
    logger.info("âœ… é«˜çº§æ•°æ®æ”¹è¿›å®Œæˆ!")

if __name__ == "__main__":
    main() 