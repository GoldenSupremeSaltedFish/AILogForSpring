#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ”¹è¿›ç­–ç•¥ - æ•°æ®å¢å¼ºå’Œè¿‡é‡‡æ ·
"""

import os
import pandas as pd
import numpy as np
import logging
from collections import Counter
from typing import List, Dict, Tuple
import random
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DataAugmenter:
    """æ•°æ®å¢å¼ºå™¨"""
    
    def __init__(self):
        # åŒä¹‰è¯è¯å…¸
        self.synonyms = {
            'error': ['exception', 'failure', 'issue', 'problem'],
            'failed': ['failed', 'error', 'exception', 'problem'],
            'exception': ['error', 'failure', 'issue', 'problem'],
            'connection': ['link', 'connect', 'network', 'socket'],
            'database': ['db', 'datastore', 'repository', 'storage'],
            'timeout': ['timeout', 'expired', 'timed_out', 'overdue'],
            'memory': ['ram', 'storage', 'cache', 'buffer'],
            'performance': ['speed', 'efficiency', 'throughput', 'latency'],
            'authentication': ['auth', 'login', 'verify', 'validate'],
            'authorization': ['permission', 'access', 'rights', 'privilege'],
            'configuration': ['config', 'setting', 'parameter', 'option'],
            'environment': ['env', 'surrounding', 'context', 'condition'],
            'business': ['commercial', 'enterprise', 'corporate', 'trade'],
            'logic': ['reasoning', 'algorithm', 'process', 'method'],
            'operation': ['action', 'task', 'process', 'function'],
            'monitoring': ['watch', 'observe', 'track', 'supervise'],
            'heartbeat': ['pulse', 'signal', 'alive', 'status'],
            'startup': ['boot', 'launch', 'initialize', 'begin'],
            'spring': ['spring', 'framework', 'boot', 'application']
        }
        
        # æ’å…¥è¯åˆ—è¡¨
        self.insert_words = [
            'error', 'exception', 'failed', 'timeout', 'connection',
            'database', 'memory', 'performance', 'authentication',
            'authorization', 'configuration', 'environment', 'business',
            'logic', 'operation', 'monitoring', 'heartbeat', 'startup'
        ]
        
        # åˆ é™¤æ¦‚ç‡
        self.delete_prob = 0.1
        # æ’å…¥æ¦‚ç‡
        self.insert_prob = 0.15
        # æ›¿æ¢æ¦‚ç‡
        self.replace_prob = 0.2
    
    def augment_text(self, text: str, num_augmentations: int = 3) -> List[str]:
        """å¢å¼ºå•ä¸ªæ–‡æœ¬"""
        augmented_texts = []
        
        for _ in range(num_augmentations):
            augmented = text
            
            # éšæœºåˆ é™¤å•è¯
            if random.random() < self.delete_prob:
                words = augmented.split()
                if len(words) > 3:
                    delete_idx = random.randint(0, len(words) - 1)
                    words.pop(delete_idx)
                    augmented = ' '.join(words)
            
            # éšæœºæ’å…¥å•è¯
            if random.random() < self.insert_prob:
                words = augmented.split()
                if len(words) > 0:
                    insert_word = random.choice(self.insert_words)
                    insert_idx = random.randint(0, len(words))
                    words.insert(insert_idx, insert_word)
                    augmented = ' '.join(words)
            
            # éšæœºæ›¿æ¢å•è¯
            if random.random() < self.replace_prob:
                words = augmented.split()
                for i, word in enumerate(words):
                    if word.lower() in self.synonyms and random.random() < 0.3:
                        synonyms = self.synonyms[word.lower()]
                        words[i] = random.choice(synonyms)
                augmented = ' '.join(words)
            
            if augmented != text:
                augmented_texts.append(augmented)
        
        return augmented_texts
    
    def augment_category(self, df: pd.DataFrame, category: str, target_count: int = 300) -> pd.DataFrame:
        """å¢å¼ºç‰¹å®šç±»åˆ«çš„æ•°æ®"""
        category_df = df[df['category'] == category].copy()
        current_count = len(category_df)
        
        if current_count >= target_count:
            logger.info(f"  {category}: å·²æœ‰ {current_count} æ¡ï¼Œæ— éœ€å¢å¼º")
            return category_df
        
        needed_count = target_count - current_count
        logger.info(f"  {category}: å½“å‰ {current_count} æ¡ï¼Œéœ€è¦å¢åŠ åˆ° {target_count} æ¡")
        
        augmented_data = []
        
        # å¯¹ç°æœ‰æ ·æœ¬è¿›è¡Œå¢å¼º
        for idx, row in category_df.iterrows():
            text = row['original_log']
            augmented_texts = self.augment_text(text, num_augmentations=2)
            
            for aug_text in augmented_texts:
                if len(augmented_data) < needed_count:
                    new_row = row.copy()
                    new_row['original_log'] = aug_text
                    new_row['augmented'] = True
                    augmented_data.append(new_row)
        
        # å¦‚æœè¿˜ä¸å¤Ÿï¼Œç»§ç»­å¢å¼º
        while len(augmented_data) < needed_count and len(category_df) > 0:
            sample_row = category_df.sample(n=1).iloc[0]
            text = sample_row['original_log']
            augmented_texts = self.augment_text(text, num_augmentations=1)
            
            for aug_text in augmented_texts:
                if len(augmented_data) < needed_count:
                    new_row = sample_row.copy()
                    new_row['original_log'] = aug_text
                    new_row['augmented'] = True
                    augmented_data.append(new_row)
        
        # åˆå¹¶åŸå§‹æ•°æ®å’Œå¢å¼ºæ•°æ®
        augmented_df = pd.DataFrame(augmented_data)
        result_df = pd.concat([category_df, augmented_df], ignore_index=True)
        
        logger.info(f"  {category}: å¢å¼ºå {len(result_df)} æ¡")
        return result_df


class DataBalancer:
    """æ•°æ®å¹³è¡¡å™¨"""
    
    def __init__(self, target_samples_per_class: int = 300):
        self.target_samples_per_class = target_samples_per_class
    
    def balance_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¹³è¡¡æ•°æ®"""
        logger.info("ğŸ”„ å¼€å§‹æ•°æ®å¹³è¡¡")
        
        # åˆ†æå½“å‰åˆ†å¸ƒ
        category_counts = df['category'].value_counts()
        logger.info("ğŸ“Š åŸå§‹æ•°æ®åˆ†å¸ƒ:")
        for category, count in category_counts.items():
            logger.info(f"  {category}: {count} æ¡")
        
        # ç¡®å®šç›®æ ‡æ•°é‡
        max_count = category_counts.max()
        target_count = min(self.target_samples_per_class, max_count)
        logger.info(f"ğŸ¯ ç›®æ ‡æ¯ç±»æ ·æœ¬æ•°: {target_count}")
        
        balanced_dfs = []
        
        for category in df['category'].unique():
            category_df = df[df['category'] == category].copy()
            current_count = len(category_df)
            
            if current_count >= target_count:
                # å¦‚æœæ ·æœ¬è¶³å¤Ÿï¼Œéšæœºé‡‡æ ·
                balanced_df = category_df.sample(n=target_count, random_state=42)
                logger.info(f"  {category}: é‡‡æ · {len(balanced_df)} æ¡")
            else:
                # å¦‚æœæ ·æœ¬ä¸è¶³ï¼Œè¿‡é‡‡æ ·
                balanced_df = self._oversample_category(category_df, target_count)
                logger.info(f"  {category}: è¿‡é‡‡æ ·åˆ° {len(balanced_df)} æ¡")
            
            balanced_dfs.append(balanced_df)
        
        # åˆå¹¶æ‰€æœ‰ç±»åˆ«
        balanced_df = pd.concat(balanced_dfs, ignore_index=True)
        
        # åˆ†æå¹³è¡¡åçš„åˆ†å¸ƒ
        balanced_counts = balanced_df['category'].value_counts()
        logger.info("ğŸ“Š å¹³è¡¡åæ•°æ®åˆ†å¸ƒ:")
        for category, count in balanced_counts.items():
            logger.info(f"  {category}: {count} æ¡")
        
        return balanced_df
    
    def _oversample_category(self, category_df: pd.DataFrame, target_count: int) -> pd.DataFrame:
        """è¿‡é‡‡æ ·ç‰¹å®šç±»åˆ«"""
        current_count = len(category_df)
        
        if current_count == 0:
            return category_df
        
        # è®¡ç®—éœ€è¦é‡å¤çš„æ¬¡æ•°
        repeat_times = target_count // current_count
        remainder = target_count % current_count
        
        # é‡å¤æ•°æ®
        repeated_df = pd.concat([category_df] * repeat_times, ignore_index=True)
        
        # æ·»åŠ å‰©ä½™æ ·æœ¬
        if remainder > 0:
            extra_samples = category_df.sample(n=remainder, random_state=42)
            repeated_df = pd.concat([repeated_df, extra_samples], ignore_index=True)
        
        return repeated_df


def improve_data(input_path: str, output_path: str, target_samples: int = 300):
    """æ”¹è¿›æ•°æ®"""
    logger.info("ğŸš€ å¼€å§‹æ•°æ®æ”¹è¿›")
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv(input_path)
    logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {len(df)} æ¡è®°å½•")
    
    # æ•°æ®æ¸…æ´—
    df_cleaned = df.dropna(subset=['original_log', 'category'])
    df_cleaned = df_cleaned[df_cleaned['original_log'].str.strip() != '']
    
    # è¿‡æ»¤æ‰æ ·æœ¬å¤ªå°‘çš„ç±»åˆ«
    category_counts = df_cleaned['category'].value_counts()
    min_samples = 10
    valid_categories = category_counts[category_counts >= min_samples].index.tolist()
    df_filtered = df_cleaned[df_cleaned['category'].isin(valid_categories)]
    
    logger.info(f"ğŸ“Š è¿‡æ»¤åæ•°æ®: {len(df_filtered)} æ¡è®°å½•")
    logger.info(f"ğŸ“Š æœ‰æ•ˆç±»åˆ«: {len(valid_categories)} ä¸ª")
    
    # æ•°æ®å¢å¼º
    logger.info("ğŸ”§ å¼€å§‹æ•°æ®å¢å¼º")
    augmenter = DataAugmenter()
    enhanced_dfs = []
    
    for category in valid_categories:
        category_df = df_filtered[df_filtered['category'] == category].copy()
        enhanced_df = augmenter.augment_category(category_df, category, target_samples)
        enhanced_dfs.append(enhanced_df)
    
    # åˆå¹¶å¢å¼ºåçš„æ•°æ®
    enhanced_df = pd.concat(enhanced_dfs, ignore_index=True)
    logger.info(f"ğŸ“Š å¢å¼ºåæ•°æ®: {len(enhanced_df)} æ¡è®°å½•")
    
    # æ•°æ®å¹³è¡¡
    logger.info("âš–ï¸ å¼€å§‹æ•°æ®å¹³è¡¡")
    balancer = DataBalancer(target_samples_per_class=target_samples)
    balanced_df = balancer.balance_data(enhanced_df)
    
    # ä¿å­˜æ”¹è¿›åçš„æ•°æ®
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    balanced_df.to_csv(output_path, index=False, encoding='utf-8')
    
    # åˆ†ææœ€ç»ˆåˆ†å¸ƒ
    final_counts = balanced_df['category'].value_counts()
    logger.info("ğŸ“Š æœ€ç»ˆæ•°æ®åˆ†å¸ƒ:")
    for category, count in final_counts.items():
        logger.info(f"  {category}: {count} æ¡")
    
    # è®¡ç®—ä¸å¹³è¡¡æ¯”ä¾‹
    max_count = final_counts.max()
    min_count = final_counts.min()
    imbalance_ratio = max_count / min_count
    
    logger.info(f"ğŸ“ˆ æœ€ç»ˆä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1")
    logger.info(f"âœ… æ•°æ®æ”¹è¿›å®Œæˆ! ä¿å­˜åˆ°: {output_path}")
    
    return balanced_df


def main():
    """ä¸»å‡½æ•°"""
    input_path = "data/processed_logs_full.csv"
    output_path = "data/processed_logs_improved.csv"
    target_samples = 300
    
    logger.info("ğŸ¯ æ•°æ®æ”¹è¿›ç­–ç•¥")
    logger.info(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_path}")
    logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
    logger.info(f"ğŸ¯ ç›®æ ‡æ¯ç±»æ ·æœ¬æ•°: {target_samples}")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_path):
        logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    # æ‰§è¡Œæ•°æ®æ”¹è¿›
    improved_df = improve_data(input_path, output_path, target_samples)
    
    logger.info("âœ… æ•°æ®æ”¹è¿›å®Œæˆ!")


if __name__ == "__main__":
    main() 