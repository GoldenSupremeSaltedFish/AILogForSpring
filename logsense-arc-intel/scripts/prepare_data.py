#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å‡†å¤‡è„šæœ¬
åˆå¹¶æ‰€æœ‰åˆ†ç±»å¥½çš„æ—¥å¿—æ•°æ®ï¼Œåˆ›å»ºè®­ç»ƒé›†
"""

import pandas as pd
import os
import logging
from pathlib import Path
from typing import List, Dict, Any

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def load_category_data(data_dir: str) -> Dict[str, pd.DataFrame]:
    """
    åŠ è½½æ‰€æœ‰åˆ†ç±»æ•°æ®
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
    Returns:
        åˆ†ç±»æ•°æ®å­—å…¸
    """
    category_data = {}
    
    # å®šä¹‰åˆ†ç±»æ˜ å°„
    category_mapping = {
        '01_å †æ ˆå¼‚å¸¸_stack_exception': 'stack_exception',
        '02_æ•°æ®åº“å¼‚å¸¸_database_exception': 'database_exception', 
        '03_è¿æ¥é—®é¢˜_connection_issue': 'connection_issue'
    }
    
    for folder_name, category in category_mapping.items():
        folder_path = os.path.join(data_dir, folder_name)
        if os.path.exists(folder_path):
            logger.info(f"ğŸ“‚ åŠ è½½åˆ†ç±»: {category}")
            
            # è¯»å–è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶
            csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
            dfs = []
            
            for csv_file in csv_files:
                file_path = os.path.join(folder_path, csv_file)
                try:
                    df = pd.read_csv(file_path)
                    # æ·»åŠ åˆ†ç±»æ ‡ç­¾
                    df['category'] = category
                    dfs.append(df)
                    logger.info(f"   âœ… åŠ è½½æ–‡ä»¶: {csv_file} ({len(df)} æ¡è®°å½•)")
                except Exception as e:
                    logger.warning(f"   âš ï¸ è·³è¿‡æ–‡ä»¶: {csv_file} - {e}")
            
            if dfs:
                # åˆå¹¶è¯¥åˆ†ç±»çš„æ‰€æœ‰æ•°æ®
                category_df = pd.concat(dfs, ignore_index=True)
                category_data[category] = category_df
                logger.info(f"   ğŸ“Š åˆ†ç±» {category} æ€»è®¡: {len(category_df)} æ¡è®°å½•")
    
    return category_data


def create_balanced_dataset(category_data: Dict[str, pd.DataFrame], 
                          max_samples_per_category: int = 1500) -> pd.DataFrame:
    """
    åˆ›å»ºå¹³è¡¡çš„æ•°æ®é›†
    Args:
        category_data: åˆ†ç±»æ•°æ®å­—å…¸
        max_samples_per_category: æ¯ä¸ªåˆ†ç±»çš„æœ€å¤§æ ·æœ¬æ•°
    Returns:
        å¹³è¡¡çš„æ•°æ®é›†
    """
    balanced_dfs = []
    
    for category, df in category_data.items():
        if len(df) > max_samples_per_category:
            # éšæœºé‡‡æ ·
            df_sampled = df.sample(n=max_samples_per_category, random_state=42)
            logger.info(f"ğŸ“Š {category}: é‡‡æ · {len(df_sampled)} æ¡ (åŸå§‹ {len(df)} æ¡)")
        else:
            df_sampled = df
            logger.info(f"ğŸ“Š {category}: ä½¿ç”¨å…¨éƒ¨ {len(df_sampled)} æ¡")
        
        balanced_dfs.append(df_sampled)
    
    # åˆå¹¶æ‰€æœ‰åˆ†ç±»æ•°æ®
    final_df = pd.concat(balanced_dfs, ignore_index=True)
    
    # ç¡®ä¿æœ‰messageåˆ—
    if 'message' not in final_df.columns:
        # å°è¯•æ‰¾åˆ°åŒ…å«æ—¥å¿—å†…å®¹çš„åˆ—
        possible_columns = ['log', 'content', 'text', 'message', 'æ—¥å¿—', 'å†…å®¹']
        for col in possible_columns:
            if col in final_df.columns:
                final_df['message'] = final_df[col]
                break
        
        if 'message' not in final_df.columns:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„åˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºmessage
            first_col = final_df.columns[0]
            final_df['message'] = final_df[first_col].astype(str)
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°messageåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—: {first_col}")
    
    return final_df


def save_processed_data(df: pd.DataFrame, output_path: str):
    """
    ä¿å­˜å¤„ç†åçš„æ•°æ®
    Args:
        df: æ•°æ®æ¡†
        output_path: è¾“å‡ºè·¯å¾„
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # ä¿å­˜æ•°æ®
    df.to_csv(output_path, index=False, encoding='utf-8')
    logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜: {output_path}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    logger.info("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    logger.info(f"   æ€»æ ·æœ¬æ•°: {len(df)}")
    logger.info(f"   åˆ†ç±»æ•°: {df['category'].nunique()}")
    
    category_counts = df['category'].value_counts()
    for category, count in category_counts.items():
        logger.info(f"   {category}: {count} æ¡")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®")
    
    # æ•°æ®ç›®å½•è·¯å¾„
    data_dir = "../DATA_OUTPUT"
    output_path = "data/processed_logs.csv"
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(data_dir):
        logger.error(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åŠ è½½æ‰€æœ‰åˆ†ç±»æ•°æ®
    category_data = load_category_data(data_dir)
    
    if not category_data:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•åˆ†ç±»æ•°æ®")
        return
    
    # åˆ›å»ºå¹³è¡¡æ•°æ®é›†
    logger.info("âš–ï¸ åˆ›å»ºå¹³è¡¡æ•°æ®é›†...")
    balanced_df = create_balanced_dataset(category_data, max_samples_per_category=1500)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    save_processed_data(balanced_df, output_path)
    
    logger.info("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
    logger.info(f"ğŸ¯ å¯ä»¥å¼€å§‹è®­ç»ƒ: python scripts/train.py --model textcnn --data {output_path}")


if __name__ == "__main__":
    main() 