#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„æ•°æ®å¹³è¡¡å™¨ - ä½¿ç”¨æ›´æ™ºèƒ½çš„é‡‡æ ·ç­–ç•¥
"""

import pandas as pd
import numpy as np
import logging
from collections import Counter
from sklearn.utils import resample
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ImprovedDataBalancer:
    """æ”¹è¿›çš„æ•°æ®å¹³è¡¡å™¨"""
    
    def __init__(self, target_samples_per_class=500):
        self.target_samples_per_class = target_samples_per_class
    
    def load_and_analyze_data(self, data_path: str):
        """åŠ è½½å¹¶åˆ†ææ•°æ®"""
        logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")
        df = pd.read_csv(data_path)
        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
        
        # æ•°æ®æ¸…æ´—
        df_cleaned = df.dropna(subset=['original_log', 'category'])
        df_cleaned = df_cleaned[df_cleaned['original_log'].str.strip() != '']
        
        # åˆ†æç±»åˆ«åˆ†å¸ƒ
        category_counts = df_cleaned['category'].value_counts()
        logger.info("ğŸ“Š åŸå§‹æ•°æ®åˆ†å¸ƒ:")
        for category, count in category_counts.items():
            percentage = (count / len(df_cleaned)) * 100
            logger.info(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
        
        return df_cleaned, category_counts
    
    def smart_balance_data(self, df: pd.DataFrame, category_counts: pd.Series):
        """æ™ºèƒ½å¹³è¡¡æ•°æ®"""
        logger.info(f"ğŸ¯ ç›®æ ‡æ¯ç±»æ ·æœ¬æ•°: {self.target_samples_per_class}")
        
        balanced_dfs = []
        
        for category in category_counts.index:
            category_df = df[df['category'] == category].copy()
            current_count = len(category_df)
            
            logger.info(f"ğŸ“Š å¤„ç†ç±»åˆ«: {category} (å½“å‰: {current_count} æ¡)")
            
            if current_count < self.target_samples_per_class:
                # ä¸Šé‡‡æ · - ä½¿ç”¨æ›´æ™ºèƒ½çš„ç­–ç•¥
                if current_count > 0:
                    # è®¡ç®—éœ€è¦é‡å¤çš„æ¬¡æ•°
                    repeat_times = self.target_samples_per_class // current_count
                    remainder = self.target_samples_per_class % current_count
                    
                    # é‡å¤é‡‡æ ·
                    repeated_samples = []
                    for _ in range(repeat_times):
                        repeated_samples.append(category_df)
                    
                    # æ·»åŠ å‰©ä½™æ ·æœ¬
                    if remainder > 0:
                        remainder_samples = category_df.sample(n=remainder, random_state=42)
                        repeated_samples.append(remainder_samples)
                    
                    # åˆå¹¶å¹¶æ‰“ä¹±
                    oversampled = pd.concat(repeated_samples, ignore_index=True)
                    oversampled = oversampled.sample(frac=1, random_state=42).reset_index(drop=True)
                    
                    logger.info(f"  âœ… æ™ºèƒ½ä¸Šé‡‡æ ·åˆ° {len(oversampled)} æ¡")
                    balanced_dfs.append(oversampled)
                else:
                    logger.warning(f"  âš ï¸ ç±»åˆ« {category} æ— æ ·æœ¬ï¼Œè·³è¿‡")
                    continue
            elif current_count > self.target_samples_per_class:
                # ä¸‹é‡‡æ · - ä½¿ç”¨åˆ†å±‚é‡‡æ ·
                undersampled = category_df.sample(
                    n=self.target_samples_per_class, 
                    random_state=42
                )
                logger.info(f"  âœ… ä¸‹é‡‡æ ·åˆ° {self.target_samples_per_class} æ¡")
                balanced_dfs.append(undersampled)
            else:
                # ä¿æŒåŸæ ·
                logger.info(f"  âœ… ä¿æŒ {current_count} æ¡")
                balanced_dfs.append(category_df)
        
        # åˆå¹¶æ‰€æœ‰ç±»åˆ«
        balanced_df = pd.concat(balanced_dfs, ignore_index=True)
        
        # æ‰“ä¹±æ•°æ®
        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        logger.info(f"ğŸ“Š å¹³è¡¡åæ•°æ®: {len(balanced_df)} æ¡è®°å½•")
        
        # éªŒè¯å¹³è¡¡ç»“æœ
        final_counts = balanced_df['category'].value_counts()
        logger.info("ğŸ“Š å¹³è¡¡ååˆ†å¸ƒ:")
        for category, count in final_counts.items():
            percentage = (count / len(balanced_df)) * 100
            logger.info(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
        
        return balanced_df
    
    def create_improved_data(self, data_path: str, output_path: str):
        """åˆ›å»ºæ”¹è¿›çš„å¹³è¡¡æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹åˆ›å»ºæ”¹è¿›çš„å¹³è¡¡æ•°æ®")
        
        # åŠ è½½æ•°æ®
        df, category_counts = self.load_and_analyze_data(data_path)
        
        # æ™ºèƒ½å¹³è¡¡æ•°æ®
        balanced_df = self.smart_balance_data(df, category_counts)
        
        # ä¿å­˜ç»“æœ
        balanced_df.to_csv(output_path, index=False)
        logger.info(f"ğŸ’¾ ä¿å­˜åˆ°: {output_path}")
        
        return balanced_df


def main():
    """ä¸»å‡½æ•°"""
    balancer = ImprovedDataBalancer(target_samples_per_class=500)
    
    input_path = "data/processed_logs_final_cleaned.csv"
    output_path = "data/processed_logs_improved_balanced.csv"
    
    balanced_df = balancer.create_improved_data(input_path, output_path)
    
    logger.info("âœ… æ”¹è¿›æ•°æ®å¹³è¡¡å®Œæˆ!")


if __name__ == "__main__":
    main() 