#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„Intel Arc GPU è®­ç»ƒè„šæœ¬ - è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜
"""

import argparse
import torch
import torch.nn as nn
import logging
import os
from datetime import datetime
from typing import Dict, Any
import numpy as np
from collections import Counter
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, confusion_matrix, classification_report

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import ModelFactory
from data import LogPreprocessor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ImprovedLogDataset(torch.utils.data.Dataset):
    """æ”¹è¿›çš„æ—¥å¿—æ•°æ®é›† - è§£å†³Tokenizeré—®é¢˜"""
    
    def __init__(self, texts: list, labels: list, max_length: int = 128, vocab_size: int = 5000):
        self.texts = texts
        self.labels = labels
        self.max_length = max_length
        self.vocab_size = vocab_size
        
        # æ„å»ºè¯æ±‡è¡¨
        self.vocab = self._build_vocab()
        logger.info(f" è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}")
    
    def _build_vocab(self):
        """æ„å»ºè¯æ±‡è¡¨"""
        word_counts = Counter()
        
        # ç»Ÿè®¡æ‰€æœ‰è¯æ±‡
        for text in self.texts:
            words = text.lower().split()
            word_counts.update(words)
        
        # é€‰æ‹©æœ€å¸¸è§çš„è¯æ±‡
        vocab = {'<PAD>': 0, '<UNK>': 1}
        for word, count in word_counts.most_common(self.vocab_size - 2):
            if count >= 2:  # è‡³å°‘å‡ºç°2æ¬¡
                vocab[word] = len(vocab)
        
        return vocab
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx]).lower()
        label = self.labels[idx]
        
        # æ”¹è¿›çš„åˆ†è¯å¤„ç†
        words = text.split()[:self.max_length]
        token_ids = []
        
        for word in words:
            if word in self.vocab:
                token_ids.append(self.vocab[word])
            else:
                token_ids.append(self.vocab['<UNK>'])
        
        # è¡¥é½åˆ°å›ºå®šé•¿åº¦
        if len(token_ids) < self.max_length:
            token_ids += [self.vocab['<PAD>']] * (self.max_length - len(token_ids))
        else:
            token_ids = token_ids[:self.max_length]
        
        return {
            'input_ids': torch.tensor(token_ids, dtype=torch.long),
            'labels': torch.tensor(label, dtype=torch.long)
        }


class ImprovedArcTrainer:
    """æ”¹è¿›çš„Intel Arc GPU è®­ç»ƒå™¨"""

    def __init__(self, model_type: str = "textcnn"):
        self.model_type = model_type
        # ä¼˜å…ˆä½¿ç”¨XPUï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨CPU
        if torch.xpu.is_available():
            self.device = torch.device("xpu:0")
            logger.info("âœ… ä½¿ç”¨Intel XPU GPUåŠ é€Ÿ")
        else:
            self.device = torch.device("cpu")
            logger.warning("âš ï¸ ä½¿ç”¨CPUè®­ç»ƒ")
        
        self.model = None
        self.label_encoder = None
        self.vocab = None

        logger.info(f" åˆå§‹åŒ–æ”¹è¿›è®­ç»ƒå™¨ - æ¨¡å‹ç±»å‹: {model_type}")
        logger.info(f"ğŸ–¥ï¸  è®¡ç®—è®¾å¤‡: {self.device}")

    def load_and_clean_data(self, data_path: str):
        """åŠ è½½å’Œæ¸…æ´—æ•°æ®"""
        logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")

        # åŠ è½½æ•°æ®
        df = pd.read_csv(data_path)
        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        logger.info("ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥:")
        logger.info(f"   ç©ºå€¼æ•°é‡: {df.isnull().sum().sum()}")
        logger.info(f"   é‡å¤æ ·æœ¬: {df.duplicated().sum()}")
        
        # æ£€æŸ¥åˆ—å
        logger.info(f"   åˆ—å: {list(df.columns)}")
        
        # æ‰¾åˆ°æ­£ç¡®çš„æ–‡æœ¬åˆ—å’Œæ ‡ç­¾åˆ—
        text_column = None
        label_column = None
        
        # å°è¯•æ‰¾åˆ°æ–‡æœ¬åˆ—
        possible_text_cols = ['original_log', 'message', 'content', 'text']
        for col in possible_text_cols:
            if col in df.columns:
                text_column = col
                break
        
        if not text_column:
            logger.error("âŒ æœªæ‰¾åˆ°åˆé€‚çš„æ–‡æœ¬åˆ—")
            return None, None, None
        
        # æ ‡ç­¾åˆ—
        label_column = 'category'
        
        logger.info(f"ğŸ” ä½¿ç”¨æ–‡æœ¬åˆ—: {text_column}")
        logger.info(f"ğŸ” ä½¿ç”¨æ ‡ç­¾åˆ—: {label_column}")
        
        # æ•°æ®æ¸…æ´—
        df_cleaned = df.dropna(subset=[text_column, label_column])
        df_cleaned = df_cleaned[df_cleaned[label_column] != 'other']
        
        logger.info(f" æ¸…æ´—åæ•°æ®: {len(df_cleaned)} æ¡è®°å½•")
        
        # åˆ†ææ•°æ®åˆ†å¸ƒ
        texts = df_cleaned[text_column].fillna('').tolist()
        labels = df_cleaned[label_column].tolist()
        
        self._analyze_data_distribution(labels)
        
        # æ ‡ç­¾ç¼–ç 
        unique_labels = sorted(list(set(labels)))
        label_to_id = {label: idx for idx, label in enumerate(unique_labels)}
        encoded_labels = [label_to_id[label] for label in labels]
        
        self.label_encoder = {idx: label for label, idx in label_to_id.items()}
        
        logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ - ç±»åˆ«æ•°: {len(self.label_encoder)}")
        
        return texts, encoded_labels, self.label_encoder

    def _analyze_data_distribution(self, labels):
        """åˆ†ææ•°æ®åˆ†å¸ƒ"""
        label_counts = Counter(labels)
        total = len(labels)
        
        logger.info("ğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ:")
        for label, count in label_counts.items():
            percentage = (count / total) * 100
            logger.info(f"   {label}: {count} æ¡ ({percentage:.1f}%)")
        
        # æ£€æŸ¥æ•°æ®ä¸å¹³è¡¡
        max_count = max(label_counts.values())
        min_count = min(label_counts.values())
        imbalance_ratio = max_count / min_count
        
        if imbalance_ratio > 2:
            logger.warning(f"âš ï¸ æ•°æ®ä¸¥é‡ä¸å¹³è¡¡ (æ¯”ä¾‹: {imbalance_ratio:.1f}:1)")
        else:
            logger.info("âœ… æ•°æ®åˆ†å¸ƒç›¸å¯¹å‡è¡¡")

    def create_improved_model(self, num_classes: int):
        """åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹ - å‡å°‘å¤æ‚åº¦"""
        logger.info(f"ğŸ—ï¸ åˆ›å»ºæ”¹è¿›æ¨¡å‹: {self.model_type}")

        # ä½¿ç”¨æ›´å°çš„æ¨¡å‹é…ç½®
        model_config = {
            'vocab_size': 5000,  # å‡å°‘è¯æ±‡è¡¨å¤§å°
            'embed_dim': 64,      # å‡å°‘åµŒå…¥ç»´åº¦
            'num_classes': num_classes,
            'filter_sizes': [3, 4, 5],
            'num_filters': 64,    # å‡å°‘å·ç§¯æ ¸æ•°é‡
            'dropout': 0.7        # å¢åŠ Dropout
        }

        self.model = ModelFactory.create_model(self.model_type, **model_config)
        self.model.to(self.device)

        param_count = self.model.count_parameters()
        logger.info(f"âœ… æ”¹è¿›æ¨¡å‹åˆ›å»ºæˆåŠŸ - å‚æ•°æ•°é‡: {param_count:,}")
        logger.info(f"ğŸ“Š å‚æ•°/æ ·æœ¬æ¯”: {param_count / 3288:.1f}:1")

    def train_epoch(self, train_loader, criterion, optimizer, scheduler=None):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0

        for batch in train_loader:
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)

            optimizer.zero_grad()
            outputs = self.model(input_ids)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            if scheduler:
                scheduler.step()

            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        return total_loss / len(train_loader), 100 * correct / total

    def validate_epoch(self, val_loader, criterion):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        all_predictions = []
        all_labels = []

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(input_ids)
                loss = criterion(outputs, labels)

                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        # è®¡ç®—F1åˆ†æ•°
        f1 = f1_score(all_labels, all_predictions, average='weighted')
        
        return total_loss / len(val_loader), 100 * correct / total, f1

    def train_with_early_stopping(self, data_path: str, epochs: int = 20, 
                                 patience: int = 5, save_dir: str = "results/models_improved"):
        """å¸¦æ—©åœçš„è®­ç»ƒæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æ”¹è¿›è®­ç»ƒ")

        # æ£€æŸ¥GPUçŠ¶æ€
        if torch.xpu.is_available():
            logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨Intel XPU GPU: {torch.xpu.get_device_name(0)}")
            logger.info(f"ğŸ’¾ GPUå†…å­˜: {torch.xpu.get_device_properties(0).total_memory / (1024**3):.1f} GB")
        else:
            logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°Intel XPU GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")

        # åŠ è½½å’Œæ¸…æ´—æ•°æ®
        texts, labels, label_encoder = self.load_and_clean_data(data_path)
        if texts is None:
            logger.error("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return

        # ä½¿ç”¨Stratified Split
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, labels, test_size=0.2, stratify=labels, random_state=42
        )
        
        # è¿›ä¸€æ­¥åˆ†å‰²éªŒè¯é›†
        train_texts, test_texts, train_labels, test_labels = train_test_split(
            train_texts, train_labels, test_size=0.2, stratify=train_labels, random_state=42
        )

        logger.info(f"ğŸ“Š æ•°æ®åˆ†å‰²å®Œæˆ:")
        logger.info(f"   è®­ç»ƒé›†: {len(train_texts)} æ¡")
        logger.info(f"   éªŒè¯é›†: {len(val_texts)} æ¡")
        logger.info(f"   æµ‹è¯•é›†: {len(test_texts)} æ¡")

        # åˆ›å»ºæ”¹è¿›çš„æ•°æ®é›†
        train_dataset = ImprovedLogDataset(train_texts, train_labels)
        val_dataset = ImprovedLogDataset(val_texts, val_labels)
        test_dataset = ImprovedLogDataset(test_texts, test_labels)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)  # å‡å°æ‰¹æ¬¡å¤§å°
        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)
        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)

        # åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹
        num_classes = len(self.label_encoder)
        self.create_improved_model(num_classes)

        # è®¾ç½®è®­ç»ƒç»„ä»¶ - æ·»åŠ L2æ­£åˆ™åŒ–
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)  # æ·»åŠ L2æ­£åˆ™åŒ–
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)

        best_acc = 0
        best_f1 = 0
        patience_counter = 0

        for epoch in range(epochs):
            logger.info(f"ğŸ“ˆ Epoch {epoch+1}/{epochs}")

            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader, criterion, optimizer)

            # éªŒè¯
            val_loss, val_acc, val_f1 = self.validate_epoch(val_loader, criterion)

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_f1)

            logger.info(f"  è®­ç»ƒ: æŸå¤±={train_loss:.4f}, å‡†ç¡®ç‡={train_acc:.2f}%")
            logger.info(f"  éªŒè¯: æŸå¤±={val_loss:.4f}, å‡†ç¡®ç‡={val_acc:.2f}%, F1={val_f1:.4f}")
            logger.info(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")

            # æ—©åœæ£€æŸ¥
            if val_f1 > best_f1:
                best_f1 = val_f1
                best_acc = val_acc
                patience_counter = 0
                self.save_model(save_dir, "best")
                logger.info(f" ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {val_f1:.4f}, å‡†ç¡®ç‡: {val_acc:.2f}%)")
            else:
                patience_counter += 1
                logger.info(f"â³ æ—©åœè®¡æ•°: {patience_counter}/{patience}")

            # æ—©åœ
            if patience_counter >= patience:
                logger.info(f" æ—©åœè§¦å‘ - {patience} è½®æœªæ”¹å–„")
                break

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model(save_dir, "final")

        # æµ‹è¯•é›†è¯„ä¼°
        test_loss, test_acc, test_f1 = self.validate_epoch(test_loader, criterion)
        logger.info(f" æµ‹è¯•é›†: æŸå¤±={test_loss:.4f}, å‡†ç¡®ç‡={test_acc:.2f}%, F1={test_f1:.4f}")

        logger.info(f"âœ… æ”¹è¿›è®­ç»ƒå®Œæˆ - æœ€ä½³F1: {best_f1:.4f}, æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")

    def save_model(self, save_dir: str, suffix: str = ""):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(save_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        model_path = os.path.join(save_dir, f"improved_arc_gpu_model_{self.model_type}_{suffix}_{timestamp}.pth")
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': self.model.get_config(),
            'label_encoder': self.label_encoder,
            'vocab': getattr(self.model, 'vocab', None)
        }, model_path)

        logger.info(f" æ¨¡å‹å·²ä¿å­˜: {model_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ”¹è¿›çš„Intel Arc GPU è®­ç»ƒå™¨")
    parser.add_argument("--model", type=str, default="textcnn",
                       choices=["textcnn", "fasttext"], help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--data", type=str, required=True, help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--epochs", type=int, default=20, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--patience", type=int, default=5, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--save_dir", type=str, default="results/models_improved", help="æ¨¡å‹ä¿å­˜ç›®å½•")

    args = parser.parse_args()

    logger.info("ğŸ¯ æ”¹è¿›çš„Intel Arc GPU è®­ç»ƒå™¨")

    trainer = ImprovedArcTrainer(model_type=args.model)
    trainer.train_with_early_stopping(args.data, args.epochs, args.patience, args.save_dir)


if __name__ == "__main__":
    main()