#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„æ•°æ®æ¸…ç†è„šæœ¬
"""

import pandas as pd
import re
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def improved_clean_log_content(text):
    """æ”¹è¿›çš„æ—¥å¿—å†…å®¹æ¸…ç†"""
    if pd.isna(text) or text == '':
        return ''
    
    text = str(text)
    
    # ç§»é™¤GitHubå…ƒæ•°æ®
    patterns = [
        r'github\.com/[^,\s]+',
        r'https://github\.com/[^,\s]+',
        r'github_issue',
        r'unknown,github_issue',
        r'[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]+',
        r'[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+,\d+',
        r'https://github\.com/[^,\s]+/issues/\d+',
        r'unknown,,,',  # ç§»é™¤unknown,,,
        r'https://',     # ç§»é™¤https://
        r'[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+,',  # ç§»é™¤ä»“åº“å,
        r',unknown,,,',  # ç§»é™¤,unknown,,,
        r',unknown,',    # ç§»é™¤,unknown,
        r'unknown,',     # ç§»é™¤unknown,
    ]
    
    for pattern in patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'&[a-zA-Z]+;', '', text)
    text = re.sub(r'\[.*?\]', '', text)
    
    # æ¸…ç†å¤šä½™å­—ç¬¦
    text = re.sub(r'^,+|,+$', '', text)
    text = re.sub(r'^"+|"+$', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    # ç§»é™¤ä»¥é€—å·å¼€å¤´çš„éƒ¨åˆ†
    if text.startswith(','):
        text = text.lstrip(',')
    
    # ç§»é™¤ä»¥unknownå¼€å¤´çš„éƒ¨åˆ†
    if text.lower().startswith('unknown'):
        text = re.sub(r'^unknown[,\s]*', '', text, flags=re.IGNORECASE)
    
    return text.strip()

def is_valid_log_improved(text):
    """æ”¹è¿›çš„æœ‰æ•ˆæ—¥å¿—åˆ¤æ–­"""
    if pd.isna(text) or text == '':
        return False
    
    text = str(text).lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥å¿—å…³é”®è¯
    log_keywords = [
        'error', 'warn', 'info', 'debug', 'exception', 'java', 'spring', 
        'at ', 'caused by', 'connection', 'database', 'timeout', 'memory',
        'performance', 'authentication', 'authorization', 'configuration',
        'environment', 'business', 'logic', 'operation', 'monitoring',
        'heartbeat', 'startup', 'boot', 'application'
    ]
    
    has_keyword = any(keyword in text for keyword in log_keywords)
    has_length = len(text) > 30  # å¢åŠ æœ€å°é•¿åº¦è¦æ±‚
    not_mostly_metadata = not text.count(',') > 3  # é¿å…ä¸»è¦æ˜¯å…ƒæ•°æ®çš„å†…å®¹
    
    return has_keyword and has_length and not_mostly_metadata

def clean_dataset(input_path, output_path):
    """æ¸…ç†æ•°æ®é›†"""
    logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {input_path}")
    df = pd.read_csv(input_path)
    logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
    
    # æ¸…ç†æ•°æ®
    cleaned_data = []
    for idx, row in df.iterrows():
        original_log = row.get('original_log', '')
        cleaned_log = improved_clean_log_content(original_log)
        
        if is_valid_log_improved(cleaned_log):
            cleaned_data.append({
                'original_log': cleaned_log,
                'category': row['category'],
                'log_level': row.get('log_level', 'UNKNOWN'),
                'content_type': row.get('content_type', ''),
                'priority': row.get('priority', ''),
                'source_file': row.get('source_file', '')
            })
    
    # åˆ›å»ºæ¸…ç†åçš„æ•°æ®æ¡†
    cleaned_df = pd.DataFrame(cleaned_data)
    logger.info(f"ğŸ“Š æ¸…ç†åæ•°æ®: {len(cleaned_df)} æ¡è®°å½•")
    
    # ä¿å­˜æ¸…ç†åçš„æ•°æ®
    cleaned_df.to_csv(output_path, index=False, encoding='utf-8')
    logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    category_counts = cleaned_df['category'].value_counts()
    logger.info("ğŸ“Š æ¸…ç†åç±»åˆ«åˆ†å¸ƒ:")
    for category, count in category_counts.items():
        logger.info(f"  {category}: {count} æ¡")
    
    # æ˜¾ç¤ºæ ·æœ¬
    logger.info("ğŸ“ æ¸…ç†åçš„æ ·æœ¬:")
    for i, row in cleaned_df.head(5).iterrows():
        logger.info(f"  {i+1}. {row['category']}: {row['original_log'][:100]}...")
    
    return cleaned_df

def main():
    """ä¸»å‡½æ•°"""
    input_path = "data/processed_logs_cleaned.csv"
    output_path = "data/processed_logs_final_cleaned.csv"
    
    logger.info("ğŸ¯ æ”¹è¿›çš„æ•°æ®æ¸…ç†")
    cleaned_df = clean_dataset(input_path, output_path)
    
    logger.info("âœ… æ•°æ®æ¸…ç†å®Œæˆ!")

if __name__ == "__main__":
    main() 