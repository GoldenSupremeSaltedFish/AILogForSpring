#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†é˜¶æ®µæ•°æ®å‡†å¤‡è„šæœ¬
æ”¯æŒå°æ•°æ®é›†å¿«é€ŸéªŒè¯å’Œå®Œæ•´æ•°æ®é›†è®­ç»ƒ
"""

import pandas as pd
import os
import logging
from pathlib import Path
from typing import List, Dict, Any

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def load_category_data(data_dir: str) -> Dict[str, pd.DataFrame]:
    """
    åŠ è½½æ‰€æœ‰åˆ†ç±»æ•°æ®
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
    Returns:
        åˆ†ç±»æ•°æ®å­—å…¸
    """
    category_data = {}
    
    # å®šä¹‰åˆ†ç±»æ˜ å°„
    category_mapping = {
        '01_å †æ ˆå¼‚å¸¸_stack_exception': 'stack_exception',
        '02_æ•°æ®åº“å¼‚å¸¸_database_exception': 'database_exception', 
        '03_è¿æ¥é—®é¢˜_connection_issue': 'connection_issue'
    }
    
    for folder_name, category in category_mapping.items():
        folder_path = os.path.join(data_dir, folder_name)
        if os.path.exists(folder_path):
            logger.info(f"ğŸ“‚ åŠ è½½åˆ†ç±»: {category}")
            
            # è¯»å–è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶
            csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
            dfs = []
            
            for csv_file in csv_files:
                file_path = os.path.join(folder_path, csv_file)
                try:
                    df = pd.read_csv(file_path)
                    # æ·»åŠ åˆ†ç±»æ ‡ç­¾
                    df['category'] = category
                    dfs.append(df)
                    logger.info(f"   âœ… åŠ è½½æ–‡ä»¶: {csv_file} ({len(df)} æ¡è®°å½•)")
                except Exception as e:
                    logger.warning(f"   âš ï¸ è·³è¿‡æ–‡ä»¶: {csv_file} - {e}")
            
            if dfs:
                # åˆå¹¶è¯¥åˆ†ç±»çš„æ‰€æœ‰æ•°æ®
                category_df = pd.concat(dfs, ignore_index=True)
                category_data[category] = category_df
                logger.info(f"   ğŸ“Š åˆ†ç±» {category} æ€»è®¡: {len(category_df)} æ¡è®°å½•")
    
    return category_data


def create_staged_datasets(category_data: Dict[str, pd.DataFrame], 
                          small_samples_per_category: int = 100,
                          large_samples_per_category: int = 1500) -> Dict[str, pd.DataFrame]:
    """
    åˆ›å»ºåˆ†é˜¶æ®µæ•°æ®é›†
    Args:
        category_data: åˆ†ç±»æ•°æ®å­—å…¸
        small_samples_per_category: å°æ•°æ®é›†æ¯ç±»æ ·æœ¬æ•°
        large_samples_per_category: å¤§æ•°æ®é›†æ¯ç±»æ ·æœ¬æ•°
    Returns:
        åŒ…å«å°æ•°æ®é›†å’Œå®Œæ•´æ•°æ®é›†çš„å­—å…¸
    """
    datasets = {}
    
    # åˆ›å»ºå°æ•°æ®é›†ï¼ˆå¿«é€ŸéªŒè¯ç”¨ï¼‰
    logger.info("ğŸ”¬ åˆ›å»ºå°æ•°æ®é›†ï¼ˆå¿«é€ŸéªŒè¯ï¼‰...")
    small_dfs = []
    
    for category, df in category_data.items():
        # å°æ•°æ®é›†ï¼šæ¯ç±»100æ¡
        if len(df) > small_samples_per_category:
            df_small = df.sample(n=small_samples_per_category, random_state=42)
            logger.info(f"ğŸ“Š {category}: å°æ•°æ®é›† {len(df_small)} æ¡ (åŸå§‹ {len(df)} æ¡)")
        else:
            df_small = df
            logger.info(f"ğŸ“Š {category}: å°æ•°æ®é›†ä½¿ç”¨å…¨éƒ¨ {len(df_small)} æ¡")
        
        small_dfs.append(df_small)
    
    small_dataset = pd.concat(small_dfs, ignore_index=True)
    datasets['small'] = small_dataset
    
    # åˆ›å»ºå®Œæ•´æ•°æ®é›†ï¼ˆæ­£å¼è®­ç»ƒç”¨ï¼‰
    logger.info("ğŸš€ åˆ›å»ºå®Œæ•´æ•°æ®é›†ï¼ˆæ­£å¼è®­ç»ƒï¼‰...")
    large_dfs = []
    
    for category, df in category_data.items():
        # å®Œæ•´æ•°æ®é›†ï¼šæ¯ç±»æœ€å¤š1500æ¡
        if len(df) > large_samples_per_category:
            df_large = df.sample(n=large_samples_per_category, random_state=42)
            logger.info(f"ğŸ“Š {category}: å®Œæ•´æ•°æ®é›† {len(df_large)} æ¡ (åŸå§‹ {len(df)} æ¡)")
        else:
            df_large = df
            logger.info(f"ğŸ“Š {category}: å®Œæ•´æ•°æ®é›†ä½¿ç”¨å…¨éƒ¨ {len(df_large)} æ¡")
        
        large_dfs.append(df_large)
    
    large_dataset = pd.concat(large_dfs, ignore_index=True)
    datasets['large'] = large_dataset
    
    return datasets


def process_dataset(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:
    """
    å¤„ç†æ•°æ®é›†
    Args:
        df: åŸå§‹æ•°æ®æ¡†
        dataset_name: æ•°æ®é›†åç§°
    Returns:
        å¤„ç†åçš„æ•°æ®æ¡†
    """
    # ç¡®ä¿æœ‰messageåˆ—
    if 'message' not in df.columns:
        # å°è¯•æ‰¾åˆ°åŒ…å«æ—¥å¿—å†…å®¹çš„åˆ—
        possible_columns = ['log', 'content', 'text', 'message', 'æ—¥å¿—', 'å†…å®¹']
        for col in possible_columns:
            if col in df.columns:
                df['message'] = df[col]
                break
        
        if 'message' not in df.columns:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„åˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºmessage
            first_col = df.columns[0]
            df['message'] = df[first_col].astype(str)
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°messageåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—: {first_col}")
    
    # æ•°æ®æ¸…æ´—
    df['message'] = df['message'].fillna('').astype(str)
    
    # ç§»é™¤ç©ºæ¶ˆæ¯
    df = df[df['message'].str.strip() != '']
    
    logger.info(f"ğŸ“Š {dataset_name} æ•°æ®é›†å¤„ç†å®Œæˆ:")
    logger.info(f"   æ€»æ ·æœ¬æ•°: {len(df)}")
    logger.info(f"   åˆ†ç±»æ•°: {df['category'].nunique()}")
    
    category_counts = df['category'].value_counts()
    for category, count in category_counts.items():
        logger.info(f"   {category}: {count} æ¡")
    
    return df


def save_datasets(datasets: Dict[str, pd.DataFrame], output_dir: str = "data"):
    """
    ä¿å­˜æ•°æ®é›†
    Args:
        datasets: æ•°æ®é›†å­—å…¸
        output_dir: è¾“å‡ºç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    
    for dataset_name, df in datasets.items():
        output_path = os.path.join(output_dir, f"processed_logs_{dataset_name}.csv")
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"ğŸ’¾ {dataset_name} æ•°æ®é›†å·²ä¿å­˜: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆ†é˜¶æ®µæ•°æ®å‡†å¤‡å·¥å…·")
    parser.add_argument("--small_samples", type=int, default=100, 
                       help="å°æ•°æ®é›†æ¯ç±»æ ·æœ¬æ•°")
    parser.add_argument("--large_samples", type=int, default=1500, 
                       help="å®Œæ•´æ•°æ®é›†æ¯ç±»æ ·æœ¬æ•°")
    parser.add_argument("--data_dir", type=str, default="../DATA_OUTPUT", 
                       help="æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="data", 
                       help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ å¼€å§‹åˆ†é˜¶æ®µæ•°æ®å‡†å¤‡")
    logger.info(f"ğŸ“‚ æ•°æ®ç›®å½•: {args.data_dir}")
    logger.info(f"ğŸ“¦ å°æ•°æ®é›†æ¯ç±»æ ·æœ¬æ•°: {args.small_samples}")
    logger.info(f"ğŸ“¦ å®Œæ•´æ•°æ®é›†æ¯ç±»æ ·æœ¬æ•°: {args.large_samples}")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(args.data_dir):
        logger.error(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
        return
    
    # åŠ è½½æ‰€æœ‰åˆ†ç±»æ•°æ®
    category_data = load_category_data(args.data_dir)
    
    if not category_data:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•åˆ†ç±»æ•°æ®")
        return
    
    # åˆ›å»ºåˆ†é˜¶æ®µæ•°æ®é›†
    datasets = create_staged_datasets(
        category_data, 
        small_samples_per_category=args.small_samples,
        large_samples_per_category=args.large_samples
    )
    
    # å¤„ç†å¹¶ä¿å­˜æ•°æ®é›†
    processed_datasets = {}
    for dataset_name, df in datasets.items():
        processed_df = process_dataset(df, dataset_name)
        processed_datasets[dataset_name] = processed_df
    
    # ä¿å­˜æ•°æ®é›†
    save_datasets(processed_datasets, args.output_dir)
    
    logger.info("âœ… åˆ†é˜¶æ®µæ•°æ®å‡†å¤‡å®Œæˆï¼")
    logger.info("ğŸ¯ è®­ç»ƒå‘½ä»¤:")
    logger.info("   å°æ•°æ®é›†éªŒè¯: python scripts/train.py --model textcnn --data data/processed_logs_small.csv --epochs 3")
    logger.info("   å®Œæ•´æ•°æ®é›†è®­ç»ƒ: python scripts/train.py --model textcnn --data data/processed_logs_large.csv --epochs 10")


if __name__ == "__main__":
    main() 