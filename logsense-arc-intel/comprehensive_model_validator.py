#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢æ¨¡å‹éªŒè¯å™¨
å¯¹è®­ç»ƒå¥½çš„ç‰¹å¾å¢å¼ºæ¨¡å‹è¿›è¡Œå¤šæ¬¡éªŒè¯ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®é›†ï¼Œå¹¶åˆ†ç±»åˆ«è¾“å‡ºå‡†ç¡®æ€§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
import logging
import json
import os
from datetime import datetime
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ComprehensiveModelValidator:
    """å…¨é¢æ¨¡å‹éªŒè¯å™¨"""
    
    def __init__(self, model_path, data_path, results_dir="validation_results"):
        self.model_path = model_path
        self.data_path = data_path
        self.results_dir = results_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs(self.results_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.label_encoder = None
        self.struct_extractor = None
        self.tfidf_vectorizer = None
        self.scaler = None
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–éªŒè¯å™¨ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_model_and_components(self):
        """åŠ è½½æ¨¡å‹å’Œç›¸å…³ç»„ä»¶"""
        try:
            # åŠ è½½æ¨¡å‹
            logger.info("ğŸ“¥ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
            self.model = torch.load(self.model_path, map_location=self.device)
            self.model.eval()
            
            # åŠ è½½æ ‡ç­¾ç¼–ç å™¨
            label_encoder_path = self.model_path.replace('.pth', '_label_encoder.pkl')
            if os.path.exists(label_encoder_path):
                import pickle
                with open(label_encoder_path, 'rb') as f:
                    self.label_encoder = pickle.load(f)
                logger.info(f"âœ… æ ‡ç­¾ç¼–ç å™¨å·²åŠ è½½ï¼Œç±»åˆ«æ•°: {len(self.label_encoder.classes_)}")
            
            # åŠ è½½ç»“æ„åŒ–ç‰¹å¾æå–å™¨
            struct_path = self.model_path.replace('.pth', '_struct_extractor.pkl')
            if os.path.exists(struct_path):
                import pickle
                with open(struct_path, 'rb') as f:
                    self.struct_extractor = pickle.load(f)
                logger.info("âœ… ç»“æ„åŒ–ç‰¹å¾æå–å™¨å·²åŠ è½½")
            
            # åŠ è½½TF-IDFå‘é‡å™¨
            tfidf_path = self.model_path.replace('.pth', '_tfidf_vectorizer.pkl')
            if os.path.exists(tfidf_path):
                import pickle
                with open(tfidf_path, 'rb') as f:
                    self.tfidf_vectorizer = pickle.load(f)
                logger.info("âœ… TF-IDFå‘é‡å™¨å·²åŠ è½½")
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨
            scaler_path = self.model_path.replace('.pth', '_scaler.pkl')
            if os.path.exists(scaler_path):
                import pickle
                with open(scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
                logger.info("âœ… æ ‡å‡†åŒ–å™¨å·²åŠ è½½")
            
            logger.info("ğŸ¯ æ‰€æœ‰ç»„ä»¶åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def load_and_prepare_data(self):
        """åŠ è½½å’Œå‡†å¤‡éªŒè¯æ•°æ®"""
        try:
            logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {self.data_path}")
            
            # åŠ è½½æ•°æ®
            if self.data_path.endswith('.csv'):
                df = pd.read_csv(self.data_path)
            elif self.data_path.endswith('.json'):
                df = pd.read_json(self.data_path)
            else:
                raise ValueError("ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼ï¼Œè¯·ä½¿ç”¨CSVæˆ–JSONæ–‡ä»¶")
            
            logger.info(f"ğŸ“ˆ æ•°æ®åŠ è½½å®Œæˆï¼Œæ€»è®°å½•æ•°: {len(df)}")
            logger.info(f"ğŸ“‹ æ•°æ®åˆ—: {list(df.columns)}")
            
            # æ£€æŸ¥å¿…è¦åˆ—
            required_columns = ['cleaned_log', 'category']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            
            # æ•°æ®åŸºæœ¬ä¿¡æ¯
            logger.info(f"ğŸ¯ ç±»åˆ«åˆ†å¸ƒ:")
            category_counts = df['category'].value_counts()
            for category, count in category_counts.items():
                logger.info(f"   {category}: {count}")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def extract_features(self, df):
        """æå–ç‰¹å¾"""
        try:
            logger.info("ğŸ”§ å¼€å§‹ç‰¹å¾æå–...")
            
            # æ–‡æœ¬ç‰¹å¾
            texts = df['cleaned_log'].fillna('').astype(str).tolist()
            
            # ç»“æ„åŒ–ç‰¹å¾
            if self.struct_extractor:
                struct_features = self.struct_extractor.extract_structured_features(df)
                logger.info(f"ğŸ“Š ç»“æ„åŒ–ç‰¹å¾ç»´åº¦: {struct_features.shape}")
            else:
                # å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒçš„ç»“æ„åŒ–ç‰¹å¾æå–å™¨ï¼Œåˆ›å»ºåŸºæœ¬ç‰¹å¾
                struct_features = self.create_basic_features(df)
                logger.info(f"ğŸ“Š åŸºæœ¬ç‰¹å¾ç»´åº¦: {struct_features.shape}")
            
            # TF-IDFç‰¹å¾
            if self.tfidf_vectorizer:
                tfidf_features = self.tfidf_vectorizer.transform(texts).toarray()
                logger.info(f"ğŸ“ TF-IDFç‰¹å¾ç»´åº¦: {tfidf_features.shape}")
            else:
                # åˆ›å»ºåŸºæœ¬çš„TF-IDFç‰¹å¾
                tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
                tfidf_features = tfidf_vectorizer.fit_transform(texts).toarray()
                logger.info(f"ğŸ“ åŸºæœ¬TF-IDFç‰¹å¾ç»´åº¦: {tfidf_features.shape}")
            
            # åˆå¹¶ç‰¹å¾
            if struct_features is not None:
                combined_features = np.hstack([struct_features, tfidf_features])
            else:
                combined_features = tfidf_features
            
            logger.info(f"ğŸ”— åˆå¹¶åç‰¹å¾ç»´åº¦: {combined_features.shape}")
            
            return texts, combined_features
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            raise
    
    def create_basic_features(self, df):
        """åˆ›å»ºåŸºæœ¬ç‰¹å¾"""
        features = {}
        
        # æ—¥å¿—é•¿åº¦ç‰¹å¾
        features['log_length'] = df['cleaned_log'].str.len().fillna(0)
        features['word_count'] = df['cleaned_log'].str.split().str.len().fillna(0)
        
        # ç‰¹æ®Šå­—ç¬¦è®¡æ•°
        features['special_char_count'] = df['cleaned_log'].str.count(r'[!@#$%^&*()_+\-=\[\]{};\':"\\|,.<>/?]').fillna(0)
        features['uppercase_count'] = df['cleaned_log'].str.count(r'[A-Z]').fillna(0)
        features['digit_count'] = df['cleaned_log'].str.count(r'\d').fillna(0)
        
        # è½¬æ¢ä¸ºDataFrame
        feature_df = pd.DataFrame(features)
        
        # æ ‡å‡†åŒ–
        if self.scaler:
            feature_array = self.scaler.transform(feature_df)
        else:
            feature_array = feature_df.values
        
        return feature_array
    
    def prepare_labels(self, df):
        """å‡†å¤‡æ ‡ç­¾"""
        try:
            if self.label_encoder:
                labels = self.label_encoder.transform(df['category'])
                logger.info(f"ğŸ·ï¸ æ ‡ç­¾ç¼–ç å®Œæˆï¼Œç±»åˆ«æ•°: {len(self.label_encoder.classes_)}")
            else:
                # å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒçš„æ ‡ç­¾ç¼–ç å™¨ï¼Œåˆ›å»ºæ–°çš„
                self.label_encoder = LabelEncoder()
                labels = self.label_encoder.fit_transform(df['category'])
                logger.info(f"ğŸ·ï¸ æ–°æ ‡ç­¾ç¼–ç å™¨åˆ›å»ºå®Œæˆï¼Œç±»åˆ«æ•°: {len(self.label_encoder.classes_)}")
            
            return labels
            
        except Exception as e:
            logger.error(f"âŒ æ ‡ç­¾å‡†å¤‡å¤±è´¥: {e}")
            raise
    
    def validate_model(self, texts, features, labels, validation_name="full_dataset"):
        """éªŒè¯æ¨¡å‹"""
        try:
            logger.info(f"ğŸ” å¼€å§‹æ¨¡å‹éªŒè¯: {validation_name}")
            
            # å‡†å¤‡æ•°æ®
            text_tensor = torch.tensor([self.text_to_sequence(text) for text in texts], dtype=torch.long)
            feature_tensor = torch.tensor(features, dtype=torch.float32)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            text_tensor = text_tensor.to(self.device)
            feature_tensor = feature_tensor.to(self.device)
            
            # é¢„æµ‹
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(text_tensor, feature_tensor)
                predictions = torch.argmax(outputs, dim=1).cpu().numpy()
                probabilities = torch.softmax(outputs, dim=1).cpu().numpy()
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(labels, predictions)
            f1 = f1_score(labels, predictions, average='weighted')
            
            # åˆ†ç±»æŠ¥å‘Š
            class_report = classification_report(
                labels, predictions, 
                target_names=self.label_encoder.classes_,
                output_dict=True
            )
            
            # æ··æ·†çŸ©é˜µ
            conf_matrix = confusion_matrix(labels, predictions)
            
            logger.info(f"ğŸ“Š éªŒè¯ç»“æœ - {validation_name}:")
            logger.info(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
            logger.info(f"   F1åˆ†æ•°: {f1:.4f}")
            
            return {
                'validation_name': validation_name,
                'accuracy': accuracy,
                'f1_score': f1,
                'predictions': predictions,
                'probabilities': probabilities,
                'true_labels': labels,
                'classification_report': class_report,
                'confusion_matrix': conf_matrix,
                'class_names': self.label_encoder.classes_
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            raise
    
    def text_to_sequence(self, text, max_length=100):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ—"""
        # ç®€å•çš„åˆ†è¯å’Œç´¢å¼•åŒ–
        words = text.split()[:max_length]
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„å“ˆå¸Œæ–¹æ³•ï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨é¢„è®­ç»ƒçš„è¯æ±‡è¡¨
        sequence = [hash(word) % 10000 for word in words]
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        if len(sequence) < max_length:
            sequence.extend([0] * (max_length - len(sequence)))
        return sequence[:max_length]
    
    def perform_multiple_validations(self, df, num_validations=5):
        """æ‰§è¡Œå¤šæ¬¡éªŒè¯"""
        logger.info(f"ğŸ”„ å¼€å§‹æ‰§è¡Œ {num_validations} æ¬¡éªŒè¯...")
        
        validation_results = []
        
        for i in range(num_validations):
            logger.info(f"ğŸ”„ ç¬¬ {i+1} æ¬¡éªŒè¯...")
            
            # éšæœºæ‰“ä¹±æ•°æ®
            df_shuffled = df.sample(frac=1.0, random_state=i).reset_index(drop=True)
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            texts, features = self.extract_features(df_shuffled)
            labels = self.prepare_labels(df_shuffled)
            
            # éªŒè¯æ¨¡å‹
            result = self.validate_model(
                texts, features, labels, 
                validation_name=f"validation_{i+1}"
            )
            
            validation_results.append(result)
            
            logger.info(f"âœ… ç¬¬ {i+1} æ¬¡éªŒè¯å®Œæˆ")
        
        return validation_results
    
    def analyze_validation_results(self, validation_results):
        """åˆ†æéªŒè¯ç»“æœ"""
        logger.info("ğŸ“ˆ åˆ†æéªŒè¯ç»“æœ...")
        
        # ç»Ÿè®¡æŒ‡æ ‡
        accuracies = [result['accuracy'] for result in validation_results]
        f1_scores = [result['f1_score'] for result in validation_results]
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'accuracy': {
                'mean': np.mean(accuracies),
                'std': np.std(accuracies),
                'min': np.min(accuracies),
                'max': np.max(accuracies)
            },
            'f1_score': {
                'mean': np.mean(f1_scores),
                'std': np.std(f1_scores),
                'min': np.min(f1_scores),
                'max': np.max(f1_scores)
            }
        }
        
        logger.info("ğŸ“Š éªŒè¯ç»“æœç»Ÿè®¡:")
        logger.info(f"   å‡†ç¡®ç‡ - å‡å€¼: {stats['accuracy']['mean']:.4f}, æ ‡å‡†å·®: {stats['accuracy']['std']:.4f}")
        logger.info(f"   å‡†ç¡®ç‡ - èŒƒå›´: [{stats['accuracy']['min']:.4f}, {stats['accuracy']['max']:.4f}]")
        logger.info(f"   F1åˆ†æ•° - å‡å€¼: {stats['f1_score']['mean']:.4f}, æ ‡å‡†å·®: {stats['f1_score']['std']:.4f}")
        logger.info(f"   F1åˆ†æ•° - èŒƒå›´: [{stats['f1_score']['min']:.4f}, {stats['f1_score']['max']:.4f}]")
        
        return stats
    
    def generate_per_category_analysis(self, validation_results):
        """ç”Ÿæˆæ¯ä¸ªç±»åˆ«çš„è¯¦ç»†åˆ†æ"""
        logger.info("ğŸ¯ ç”Ÿæˆæ¯ä¸ªç±»åˆ«çš„è¯¦ç»†åˆ†æ...")
        
        # ä½¿ç”¨æœ€åä¸€æ¬¡éªŒè¯çš„ç»“æœè¿›è¡Œè¯¦ç»†åˆ†æ
        final_result = validation_results[-1]
        
        # æŒ‰ç±»åˆ«åˆ†æ
        category_analysis = {}
        
        for i, class_name in enumerate(final_result['class_names']):
            # æ‰¾åˆ°è¯¥ç±»åˆ«çš„æ ·æœ¬
            class_mask = final_result['true_labels'] == i
            class_predictions = final_result['predictions'][class_mask]
            class_true = final_result['true_labels'][class_mask]
            
            if len(class_true) > 0:
                # è®¡ç®—è¯¥ç±»åˆ«çš„æŒ‡æ ‡
                class_accuracy = accuracy_score(class_true, class_predictions)
                class_f1 = f1_score(class_true, class_predictions, average='binary')
                
                # è®¡ç®—è¯¥ç±»åˆ«çš„é¢„æµ‹åˆ†å¸ƒ
                prediction_counts = Counter(class_predictions)
                
                category_analysis[class_name] = {
                    'sample_count': len(class_true),
                    'accuracy': class_accuracy,
                    'f1_score': class_f1,
                    'prediction_distribution': {
                        self.label_encoder.classes_[j]: count 
                        for j, count in prediction_counts.items()
                    }
                }
        
        return category_analysis
    
    def save_results(self, validation_results, stats, category_analysis):
        """ä¿å­˜ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜éªŒè¯ç»“æœ
            results_file = os.path.join(self.results_dir, f"validation_results_{timestamp}.json")
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            save_data = {
                'timestamp': timestamp,
                'model_path': self.model_path,
                'data_path': self.data_path,
                'statistics': stats,
                'category_analysis': category_analysis,
                'validation_summary': []
            }
            
            # æ·»åŠ æ¯æ¬¡éªŒè¯çš„æ‘˜è¦
            for result in validation_results:
                save_data['validation_summary'].append({
                    'validation_name': result['validation_name'],
                    'accuracy': result['accuracy'],
                    'f1_score': result['f1_score']
                })
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
            
            # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
            self.save_detailed_report(validation_results, stats, category_analysis, timestamp)
            
            return results_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            raise
    
    def save_detailed_report(self, validation_results, stats, category_analysis, timestamp):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Š"""
        try:
            # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
            report_file = os.path.join(self.results_dir, f"detailed_report_{timestamp}.txt")
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("ç‰¹å¾å¢å¼ºæ¨¡å‹å…¨é¢éªŒè¯æŠ¥å‘Š\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"éªŒè¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ¨¡å‹è·¯å¾„: {self.model_path}\n")
                f.write(f"æ•°æ®è·¯å¾„: {self.data_path}\n\n")
                
                # ç»Ÿè®¡ä¿¡æ¯
                f.write("ğŸ“Š éªŒè¯ç»“æœç»Ÿè®¡\n")
                f.write("-" * 40 + "\n")
                f.write(f"å‡†ç¡®ç‡ - å‡å€¼: {stats['accuracy']['mean']:.4f}\n")
                f.write(f"å‡†ç¡®ç‡ - æ ‡å‡†å·®: {stats['accuracy']['std']:.4f}\n")
                f.write(f"å‡†ç¡®ç‡ - èŒƒå›´: [{stats['accuracy']['min']:.4f}, {stats['accuracy']['max']:.4f}]\n")
                f.write(f"F1åˆ†æ•° - å‡å€¼: {stats['f1_score']['mean']:.4f}\n")
                f.write(f"F1åˆ†æ•° - æ ‡å‡†å·®: {stats['f1_score']['std']:.4f}\n")
                f.write(f"F1åˆ†æ•° - èŒƒå›´: [{stats['f1_score']['min']:.4f}, {stats['f1_score']['max']:.4f}]\n\n")
                
                # æ¯æ¬¡éªŒè¯ç»“æœ
                f.write("ğŸ”„ å„æ¬¡éªŒè¯ç»“æœ\n")
                f.write("-" * 40 + "\n")
                for result in validation_results:
                    f.write(f"{result['validation_name']}: å‡†ç¡®ç‡={result['accuracy']:.4f}, F1={result['f1_score']:.4f}\n")
                f.write("\n")
                
                # ç±»åˆ«åˆ†æ
                f.write("ğŸ¯ å„ç±»åˆ«è¯¦ç»†åˆ†æ\n")
                f.write("-" * 40 + "\n")
                for category, analysis in category_analysis.items():
                    f.write(f"\nç±»åˆ«: {category}\n")
                    f.write(f"  æ ·æœ¬æ•°é‡: {analysis['sample_count']}\n")
                    f.write(f"  å‡†ç¡®ç‡: {analysis['accuracy']:.4f}\n")
                    f.write(f"  F1åˆ†æ•°: {analysis['f1_score']:.4f}\n")
                    f.write(f"  é¢„æµ‹åˆ†å¸ƒ:\n")
                    for pred_cat, count in analysis['prediction_distribution'].items():
                        f.write(f"    {pred_cat}: {count}\n")
                
                f.write("\n" + "=" * 80 + "\n")
                f.write("æŠ¥å‘Šç»“æŸ\n")
                f.write("=" * 80 + "\n")
            
            logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è¯¦ç»†æŠ¥å‘Šå¤±è´¥: {e}")
    
    def create_visualizations(self, validation_results, stats, category_analysis):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        try:
            logger.info("ğŸ“Š åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 1. éªŒè¯ç»“æœè¶‹åŠ¿å›¾
            plt.figure(figsize=(12, 8))
            
            # å‡†ç¡®ç‡è¶‹åŠ¿
            plt.subplot(2, 2, 1)
            accuracies = [result['accuracy'] for result in validation_results]
            plt.plot(range(1, len(accuracies) + 1), accuracies, 'bo-', linewidth=2, markersize=8)
            plt.axhline(y=stats['accuracy']['mean'], color='r', linestyle='--', label=f'å‡å€¼: {stats["accuracy"]["mean"]:.4f}')
            plt.fill_between(range(1, len(accuracies) + 1), 
                           [stats['accuracy']['mean'] - stats['accuracy']['std']] * len(accuracies),
                           [stats['accuracy']['mean'] + stats['accuracy']['std']] * len(accuracies),
                           alpha=0.3, color='r', label=f'Â±1æ ‡å‡†å·®')
            plt.xlabel('éªŒè¯æ¬¡æ•°')
            plt.ylabel('å‡†ç¡®ç‡')
            plt.title('éªŒè¯å‡†ç¡®ç‡è¶‹åŠ¿')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # F1åˆ†æ•°è¶‹åŠ¿
            plt.subplot(2, 2, 2)
            f1_scores = [result['f1_score'] for result in validation_results]
            plt.plot(range(1, len(f1_scores) + 1), f1_scores, 'go-', linewidth=2, markersize=8)
            plt.axhline(y=stats['f1_score']['mean'], color='r', linestyle='--', label=f'å‡å€¼: {stats["f1_score"]["mean"]:.4f}')
            plt.fill_between(range(1, len(f1_scores) + 1), 
                           [stats['f1_score']['mean'] - stats['f1_score']['std']] * len(f1_scores),
                           [stats['f1_score']['mean'] + stats['f1_score']['std']] * len(f1_scores),
                           alpha=0.3, color='r', label=f'Â±1æ ‡å‡†å·®')
            plt.xlabel('éªŒè¯æ¬¡æ•°')
            plt.ylabel('F1åˆ†æ•°')
            plt.title('éªŒè¯F1åˆ†æ•°è¶‹åŠ¿')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ç±»åˆ«å‡†ç¡®ç‡åˆ†å¸ƒ
            plt.subplot(2, 2, 3)
            categories = list(category_analysis.keys())
            category_accuracies = [category_analysis[cat]['accuracy'] for cat in categories]
            plt.bar(categories, category_accuracies, color='skyblue', alpha=0.7)
            plt.xlabel('ç±»åˆ«')
            plt.ylabel('å‡†ç¡®ç‡')
            plt.title('å„ç±»åˆ«å‡†ç¡®ç‡')
            plt.xticks(rotation=45, ha='right')
            plt.grid(True, alpha=0.3)
            
            # ç±»åˆ«æ ·æœ¬æ•°é‡åˆ†å¸ƒ
            plt.subplot(2, 2, 4)
            category_counts = [category_analysis[cat]['sample_count'] for cat in categories]
            plt.bar(categories, category_counts, color='lightgreen', alpha=0.7)
            plt.xlabel('ç±»åˆ«')
            plt.ylabel('æ ·æœ¬æ•°é‡')
            plt.title('å„ç±»åˆ«æ ·æœ¬æ•°é‡')
            plt.xticks(rotation=45, ha='right')
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plot_file = os.path.join(self.results_dir, f"validation_plots_{timestamp}.png")
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_file}")
            
            # 2. æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
            final_result = validation_results[-1]
            plt.figure(figsize=(10, 8))
            sns.heatmap(final_result['confusion_matrix'], 
                       annot=True, fmt='d', cmap='Blues',
                       xticklabels=final_result['class_names'],
                       yticklabels=final_result['class_names'])
            plt.title('æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾')
            plt.xlabel('é¢„æµ‹ç±»åˆ«')
            plt.ylabel('çœŸå®ç±»åˆ«')
            
            # ä¿å­˜æ··æ·†çŸ©é˜µ
            cm_file = os.path.join(self.results_dir, f"confusion_matrix_{timestamp}.png")
            plt.savefig(cm_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ğŸ”¥ æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {cm_file}")
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")
    
    def run_comprehensive_validation(self, num_validations=5):
        """è¿è¡Œå…¨é¢éªŒè¯"""
        try:
            logger.info("ğŸš€ å¼€å§‹å…¨é¢æ¨¡å‹éªŒè¯...")
            
            # 1. åŠ è½½æ¨¡å‹å’Œç»„ä»¶
            self.load_model_and_components()
            
            # 2. åŠ è½½æ•°æ®
            df = self.load_and_prepare_data()
            
            # 3. æ‰§è¡Œå¤šæ¬¡éªŒè¯
            validation_results = self.perform_multiple_validations(df, num_validations)
            
            # 4. åˆ†æç»“æœ
            stats = self.analyze_validation_results(validation_results)
            
            # 5. ç”Ÿæˆç±»åˆ«åˆ†æ
            category_analysis = self.generate_per_category_analysis(validation_results)
            
            # 6. ä¿å­˜ç»“æœ
            results_file = self.save_results(validation_results, stats, category_analysis)
            
            # 7. åˆ›å»ºå¯è§†åŒ–
            self.create_visualizations(validation_results, stats, category_analysis)
            
            logger.info("ğŸ‰ å…¨é¢éªŒè¯å®Œæˆï¼")
            
            return {
                'validation_results': validation_results,
                'statistics': stats,
                'category_analysis': category_analysis,
                'results_file': results_file
            }
            
        except Exception as e:
            logger.error(f"âŒ å…¨é¢éªŒè¯å¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å…¨é¢æ¨¡å‹éªŒè¯å™¨")
    parser.add_argument("--model_path", type=str, required=True, help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("--data_path", type=str, required=True, help="éªŒè¯æ•°æ®è·¯å¾„")
    parser.add_argument("--num_validations", type=int, default=5, help="éªŒè¯æ¬¡æ•°")
    parser.add_argument("--results_dir", type=str, default="validation_results", help="ç»“æœä¿å­˜ç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = ComprehensiveModelValidator(
        model_path=args.model_path,
        data_path=args.data_path,
        results_dir=args.results_dir
    )
    
    # è¿è¡ŒéªŒè¯
    results = validator.run_comprehensive_validation(args.num_validations)
    
    # è¾“å‡ºæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ¯ éªŒè¯å®Œæˆæ‘˜è¦")
    print("="*60)
    print(f"ğŸ“Š å¹³å‡å‡†ç¡®ç‡: {results['statistics']['accuracy']['mean']:.4f} Â± {results['statistics']['accuracy']['std']:.4f}")
    print(f"ğŸ“Š å¹³å‡F1åˆ†æ•°: {results['statistics']['f1_score']['mean']:.4f} Â± {results['statistics']['f1_score']['std']:.4f}")
    print(f"ğŸ“ ç»“æœä¿å­˜ä½ç½®: {results['results_dir']}")
    print("="*60)

if __name__ == "__main__":
    main() 