#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆæ¨¡å‹éªŒè¯è„šæœ¬
ä½¿ç”¨feature_enhancedæ¨¡å‹è¿›è¡Œå®Œæ•´éªŒè¯
"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import os
import sys
import re
from collections import Counter
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# å®šä¹‰ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„ç±»
class StructuredFeatureExtractor:
    """ç»“æ„åŒ–ç‰¹å¾æå–å™¨"""
    
    def __init__(self, max_tfidf_features=1000):
        self.max_tfidf_features = max_tfidf_features
        self.label_encoders = {}
        self.tfidf_vectorizer = None
        self.scaler = StandardScaler()
        self.feature_names = []
    
    def extract_structured_features(self, df):
        """æå–ç»“æ„åŒ–ç‰¹å¾"""
        features = {}
        
        # 1. æ—¥å¿—çº§åˆ«ç‰¹å¾
        if 'log_level' in df.columns:
            features['log_level'] = self._encode_categorical(df['log_level'], 'log_level')
        
        # 2. ä»original_logä¸­æå–ç‰¹å¾
        if 'original_log' in df.columns:
            # é”™è¯¯ç ç‰¹å¾
            features['has_error_code'] = df['original_log'].str.contains(r'\b\d{3,5}\b', regex=True).astype(int)
            features['error_code_count'] = df['original_log'].str.count(r'\b\d{3,5}\b').fillna(0)
            
            # è·¯å¾„ç‰¹å¾
            features['has_path'] = df['original_log'].str.contains(r'[/\\][\w\./\\]+').astype(int)
            features['path_count'] = df['original_log'].str.count(r'[/\\][\w\./\\]+').fillna(0)
            features['path_depth'] = df['original_log'].str.count(r'[/\\]').fillna(0)
            
            # æ•°å­—ç‰¹å¾
            features['has_numbers'] = df['original_log'].str.contains(r'\d').astype(int)
            features['number_count'] = df['original_log'].str.count(r'\d').fillna(0)
            
            # ç±»åç‰¹å¾
            features['has_classes'] = df['original_log'].str.contains(r'\b[A-Z][a-zA-Z]*\.[A-Z][a-zA-Z]*\b').astype(int)
            features['class_count'] = df['original_log'].str.count(r'\b[A-Z][a-zA-Z]*\.[A-Z][a-zA-Z]*\b').fillna(0)
            
            # æ–¹æ³•åç‰¹å¾
            features['has_methods'] = df['original_log'].str.contains(r'\b[a-z][a-zA-Z]*\([^)]*\)').astype(int)
            features['method_count'] = df['original_log'].str.count(r'\b[a-z][a-zA-Z]*\([^)]*\)').fillna(0)
            
            # æ—¶é—´æˆ³ç‰¹å¾
            features['has_timestamps'] = df['original_log'].str.contains(r'\d{4}-\d{2}-\d{2}|\d{2}:\d{2}:\d{2}').astype(int)
            
            # æ—¥å¿—é•¿åº¦ç‰¹å¾
            features['log_length'] = df['original_log'].str.len().fillna(0)
            features['word_count'] = df['original_log'].str.split().str.len().fillna(0)
            
            # ç‰¹æ®Šå­—ç¬¦ç‰¹å¾
            features['special_char_count'] = df['original_log'].str.count(r'[^a-zA-Z0-9\s]').fillna(0)
            features['uppercase_count'] = df['original_log'].str.count(r'[A-Z]').fillna(0)
            features['digit_count'] = df['original_log'].str.count(r'\d').fillna(0)
            
            # å¼‚å¸¸ç‰¹å¾
            features['has_exception'] = df['original_log'].str.contains(r'Exception|Error', case=False).astype(int)
            features['exception_count'] = df['original_log'].str.count(r'(?i)Exception|Error').fillna(0)
            
            # å †æ ˆè·Ÿè¸ªç‰¹å¾
            features['has_stack_trace'] = df['original_log'].str.contains(r'at\s+\w+\.\w+\(', regex=True).astype(int)
            features['stack_trace_count'] = df['original_log'].str.count(r'at\s+\w+\.\w+\(').fillna(0)
        
        # 3. å…¶ä»–ç‰¹å¾
        if 'line_number' in df.columns:
            features['line_number'] = df['line_number'].fillna(0)
        
        if 'priority' in df.columns:
            features['priority'] = self._encode_categorical(df['priority'], 'priority')
        
        if 'content_type' in df.columns:
            features['content_type'] = self._encode_categorical(df['content_type'], 'content_type')
        
        return pd.DataFrame(features)
    
    def _encode_categorical(self, series, feature_name):
        """ç¼–ç åˆ†ç±»ç‰¹å¾"""
        if feature_name not in self.label_encoders:
            self.label_encoders[feature_name] = LabelEncoder()
            return self.label_encoders[feature_name].fit_transform(series.fillna('unknown'))
        else:
            # å¤„ç†æœªè§è¿‡çš„ç±»åˆ«
            unique_values = series.unique()
            known_values = self.label_encoders[feature_name].classes_
            unknown_mask = ~series.isin(known_values)
            
            if unknown_mask.any():
                # å°†æœªçŸ¥ç±»åˆ«æ›¿æ¢ä¸ºæœ€å¸¸è§çš„ç±»åˆ«
                most_common = series[~unknown_mask].mode()[0] if len(series[~unknown_mask]) > 0 else 'unknown'
                series = series.copy()
                series[unknown_mask] = most_common
            
            return self.label_encoders[feature_name].transform(series.fillna('unknown'))

class TextCNN(nn.Module):
    """TextCNNæ¨¡å‹"""
    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, num_classes, dropout=0.5):
        super(TextCNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filters, (size, embed_dim)) for size in filter_sizes
        ])
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)
    
    def get_output_dim(self):
        return len(self.convs) * self.convs[0].out_channels
    
    def forward(self, x):
        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]
        x = x.unsqueeze(1)  # [batch_size, 1, seq_len, embed_dim]
        
        conv_outputs = []
        for conv in self.convs:
            conv_out = torch.relu(conv(x))  # [batch_size, num_filters, seq_len-k+1, 1]
            conv_out = conv_out.squeeze(3)  # [batch_size, num_filters, seq_len-k+1]
            pooled = torch.max_pool1d(conv_out, conv_out.size(2))  # [batch_size, num_filters, 1]
            conv_outputs.append(pooled.squeeze(2))  # [batch_size, num_filters]
        
        x = torch.cat(conv_outputs, dim=1)  # [batch_size, len(filter_sizes) * num_filters]
        x = self.dropout(x)
        x = self.fc(x)
        return x

class StructuredFeatureMLP(nn.Module):
    """ç»“æ„åŒ–ç‰¹å¾MLP"""
    
    def __init__(self, input_dim, hidden_dims=[256, 128], dropout=0.3):
        super(StructuredFeatureMLP, self).__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        
        self.mlp = nn.Sequential(*layers)
        self.output_dim = hidden_dims[-1]
    
    def forward(self, x):
        return self.mlp(x)

class DualChannelLogClassifier(nn.Module):
    """åŒé€šé“æ—¥å¿—åˆ†ç±»å™¨"""
    
    def __init__(self, text_encoder, struct_input_dim, num_classes, fusion_dim=256):
        super(DualChannelLogClassifier, self).__init__()
        
        self.text_encoder = text_encoder
        self.struct_mlp = StructuredFeatureMLP(struct_input_dim)
        
        # èåˆå±‚
        total_features = text_encoder.get_output_dim() + self.struct_mlp.output_dim
        self.fusion_layer = nn.Sequential(
            nn.Linear(total_features, fusion_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(fusion_dim, num_classes)
        )
    
    def forward(self, text_inputs, struct_inputs):
        # æ–‡æœ¬ç‰¹å¾æå– - ä¿®æ”¹ä¸ºåªè¿”å›ç‰¹å¾ï¼Œä¸è¿›è¡Œåˆ†ç±»
        embedded = self.text_encoder.embedding(text_inputs)  # [batch_size, seq_len, embed_dim]
        embedded = embedded.unsqueeze(1)  # [batch_size, 1, seq_len, embed_dim]
        
        # å·ç§¯
        conv_outputs = []
        for conv in self.text_encoder.convs:
            conv_out = torch.relu(conv(embedded))  # [batch_size, num_filters, seq_len-k+1, 1]
            conv_out = conv_out.squeeze(3)  # [batch_size, num_filters, seq_len-k+1]
            pooled = torch.max_pool1d(conv_out, conv_out.size(2))  # [batch_size, num_filters, 1]
            conv_outputs.append(pooled.squeeze(2))  # [batch_size, num_filters]
        
        # æ‹¼æ¥
        text_features = torch.cat(conv_outputs, dim=1)  # [batch_size, len(filter_sizes) * num_filters]
        text_features = self.text_encoder.dropout(text_features)
        
        # ç»“æ„åŒ–ç‰¹å¾æå–
        struct_features = self.struct_mlp(struct_inputs)  # [batch, struct_dim]
        
        # ç‰¹å¾èåˆ
        combined_features = torch.cat([text_features, struct_features], dim=1)
        
        # åˆ†ç±»
        output = self.fusion_layer(combined_features)
        
        return output

def text_to_sequence(text, vocab, max_len=100):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ—"""
    words = text.lower().split()
    sequence = []
    for word in words[:max_len]:
        sequence.append(vocab.get(word, vocab.get('<UNK>', 1)))
    
    # å¡«å……åˆ°æœ€å¤§é•¿åº¦
    while len(sequence) < max_len:
        sequence.append(vocab.get('<PAD>', 0))
    
    return sequence

def build_vocab_from_data(texts, vocab_size=5000):
    """ä»æ•°æ®æ„å»ºè¯æ±‡è¡¨"""
    word_counts = Counter()
    for text in texts:
        words = text.lower().split()
        word_counts.update(words)
    
    vocab = {'<PAD>': 0, '<UNK>': 1}
    for word, count in word_counts.most_common(vocab_size - 2):
        if count >= 2:
            vocab[word] = len(vocab)
    
    return vocab

class FinalModelValidator:
    """æœ€ç»ˆæ¨¡å‹éªŒè¯å™¨"""
    
    def __init__(self, model_path):
        self.model_path = model_path
        self.device = torch.device("xpu" if torch.xpu.is_available() else "cpu")
        self.model = None
        self.vocab = None
        self.label_encoder = None
        self.feature_extractor = None
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ” åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # æ·»åŠ å®‰å…¨çš„å…¨å±€å˜é‡
        torch.serialization.add_safe_globals([
            'sklearn.preprocessing._label.LabelEncoder',
            'sklearn.preprocessing._data.StandardScaler', 
            'sklearn.feature_extraction.text.TfidfVectorizer'
        ])
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(self.model_path, map_location='cpu', weights_only=False)
        
        # æå–ç»„ä»¶
        self.vocab = checkpoint['vocab']
        self.label_encoder = checkpoint['label_encoder']
        self.feature_extractor = checkpoint['feature_extractor']
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}")
        print(f"   ç±»åˆ«æ•°: {len(self.label_encoder.classes_)}")
        print(f"   ç±»åˆ«: {list(self.label_encoder.classes_)}")
        
        # åˆ›å»ºæ¨¡å‹
        vocab_size = len(self.vocab)
        num_classes = len(self.label_encoder.classes_)
        
        # åˆ›å»ºæ–‡æœ¬ç¼–ç å™¨
        text_encoder = TextCNN(
            vocab_size=vocab_size,
            embed_dim=128,
            num_filters=128,
            filter_sizes=[3, 4, 5],
            num_classes=num_classes,  # è¿™é‡Œä½¿ç”¨å®é™…çš„ç±»åˆ«æ•°
            dropout=0.5
        )
        
        # ç»“æ„åŒ–ç‰¹å¾ç»´åº¦ï¼ˆä»æƒé‡ä¸­è·å–ï¼‰
        struct_input_dim = 1018  # ä»æƒé‡åˆ†æä¸­å¾—åˆ°çš„å®é™…ç»´åº¦
        
        self.model = DualChannelLogClassifier(
            text_encoder=text_encoder,
            struct_input_dim=struct_input_dim,
            num_classes=num_classes,
            fusion_dim=256
        )
        
        # åŠ è½½æƒé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ!")
        
    def load_validation_data(self, data_path):
        """åŠ è½½éªŒè¯æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½éªŒè¯æ•°æ®: {data_path}")
        
        if not os.path.exists(data_path):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            return None
        
        df = pd.read_csv(data_path)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ! æ ·æœ¬æ•°: {len(df)}")
        print(f"   åŸå§‹ç±»åˆ«åˆ†å¸ƒ:\n{df['category'].value_counts()}")
        
        # è¿‡æ»¤æ‰è®­ç»ƒæ—¶æ²¡æœ‰çš„ç±»åˆ«
        valid_categories = set(self.label_encoder.classes_)
        df_filtered = df[df['category'].isin(valid_categories)].copy()
        
        print(f"âœ… è¿‡æ»¤åæ•°æ®: æ ·æœ¬æ•°: {len(df_filtered)}")
        print(f"   è¿‡æ»¤åç±»åˆ«åˆ†å¸ƒ:\n{df_filtered['category'].value_counts()}")
        
        return df_filtered
    
    def prepare_features(self, df):
        """å‡†å¤‡ç‰¹å¾"""
        print("ğŸ”§ å‡†å¤‡ç‰¹å¾...")
        
        # æ–‡æœ¬åºåˆ—åŒ– - ä½¿ç”¨original_log
        text_sequences = []
        for text in df['original_log']:
            sequence = text_to_sequence(text, self.vocab)
            text_sequences.append(sequence)
        
        text_tensor = torch.tensor(text_sequences, dtype=torch.long)
        
        # ç»“æ„åŒ–ç‰¹å¾ - ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ–¹æ³•
        struct_features = self.feature_extractor.extract_structured_features(df)
        
        # è½¬æ¢ä¸ºæ•°å€¼
        struct_tensor = torch.tensor(struct_features.values, dtype=torch.float32)
        
        # æ ‡ç­¾
        labels = self.label_encoder.transform(df['category'])
        label_tensor = torch.tensor(labels, dtype=torch.long)
        
        print(f"âœ… ç‰¹å¾å‡†å¤‡å®Œæˆ!")
        print(f"   æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {text_tensor.shape}")
        print(f"   ç»“æ„åŒ–ç‰¹å¾å½¢çŠ¶: {struct_tensor.shape}")
        print(f"   æ ‡ç­¾å½¢çŠ¶: {label_tensor.shape}")
        
        return text_tensor, struct_tensor, label_tensor
    
    def validate_model(self, text_tensor, struct_tensor, label_tensor):
        """éªŒè¯æ¨¡å‹"""
        print("ğŸ” å¼€å§‹æ¨¡å‹éªŒè¯...")
        
        self.model.eval()
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            # åˆ†æ‰¹å¤„ç†
            batch_size = 32
            num_samples = len(text_tensor)
            
            for i in range(0, num_samples, batch_size):
                end_idx = min(i + batch_size, num_samples)
                
                batch_text = text_tensor[i:end_idx].to(self.device)
                batch_struct = struct_tensor[i:end_idx].to(self.device)
                batch_labels = label_tensor[i:end_idx]
                
                outputs = self.model(batch_text, batch_struct)
                predictions = torch.argmax(outputs, dim=1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(batch_labels.numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(all_labels, all_predictions)
        f1 = f1_score(all_labels, all_predictions, average='weighted')
        
        print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
        print(f"   æ€»ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   æ€»ä½“F1åˆ†æ•°: {f1:.4f}")
        
        # åˆ†ç±»æŠ¥å‘Š
        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        report = classification_report(all_labels, all_predictions, 
                                     target_names=self.label_encoder.classes_,
                                     digits=4)
        print(report)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, all_predictions)
        self.plot_confusion_matrix(cm, self.label_encoder.classes_)
        
        return accuracy, f1, all_predictions, all_labels
    
    def plot_confusion_matrix(self, cm, class_names):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=class_names, yticklabels=class_names)
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_path = f"results/plots/final_validation_confusion_matrix_{timestamp}.png"
        os.makedirs(os.path.dirname(plot_path), exist_ok=True)
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜: {plot_path}")
    
    def run_validation(self, data_path):
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        print("=" * 60)
        print("ğŸš€ æœ€ç»ˆæ¨¡å‹éªŒè¯")
        print("=" * 60)
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
        # åŠ è½½æ•°æ®
        df = self.load_validation_data(data_path)
        if df is None:
            return
        
        # å‡†å¤‡ç‰¹å¾
        text_tensor, struct_tensor, label_tensor = self.prepare_features(df)
        
        # éªŒè¯æ¨¡å‹
        accuracy, f1, predictions, labels = self.validate_model(text_tensor, struct_tensor, label_tensor)
        
        # ä¿å­˜ç»“æœ
        self.save_results(accuracy, f1, predictions, labels, df)
        
        print("\n" + "=" * 60)
        print("âœ… éªŒè¯å®Œæˆ!")
        print("=" * 60)
    
    def save_results(self, accuracy, f1, predictions, labels, df):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = f"results/validation_results_{timestamp}"
        os.makedirs(results_dir, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_df = df.copy()
        results_df['true_label'] = labels
        results_df['predicted_label'] = predictions
        results_df['correct'] = (labels == predictions)
        
        # è½¬æ¢æ ‡ç­¾åç§°
        results_df['true_category'] = self.label_encoder.inverse_transform(labels)
        results_df['predicted_category'] = self.label_encoder.inverse_transform(predictions)
        
        # ä¿å­˜åˆ°CSV
        results_path = os.path.join(results_dir, "validation_results.csv")
        results_df.to_csv(results_path, index=False)
        
        # ä¿å­˜æ‘˜è¦
        summary = {
            'timestamp': timestamp,
            'model_path': self.model_path,
            'total_samples': len(df),
            'accuracy': accuracy,
            'f1_score': f1,
            'class_distribution': df['category'].value_counts().to_dict(),
            'per_class_accuracy': {}
        }
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        for i, class_name in enumerate(self.label_encoder.classes_):
            class_mask = (labels == i)
            if class_mask.sum() > 0:
                class_accuracy = (predictions[class_mask] == labels[class_mask]).mean()
                summary['per_class_accuracy'][class_name] = class_accuracy
        
        # ä¿å­˜æ‘˜è¦
        import json
        summary_path = os.path.join(results_dir, "validation_summary.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
        print(f"   - è¯¦ç»†ç»“æœ: validation_results.csv")
        print(f"   - æ‘˜è¦æŠ¥å‘Š: validation_summary.json")

def main():
    """ä¸»å‡½æ•°"""
    # æ¨¡å‹æ–‡ä»¶è·¯å¾„
    model_path = "results/models/feature_enhanced_model_20250812_004934.pth"
    
    # éªŒè¯æ•°æ®è·¯å¾„
    data_path = "data/processed_logs_full.csv"
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = FinalModelValidator(model_path)
    
    # è¿è¡ŒéªŒè¯
    validator.run_validation(data_path)

if __name__ == "__main__":
    main()
