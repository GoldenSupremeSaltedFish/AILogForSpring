#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒ‡æ ‡è®¡ç®—æ¨¡å—
"""

import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import logging
from typing import Dict, Any, Tuple

logger = logging.getLogger(__name__)


class MetricsCalculator:
    """æŒ‡æ ‡è®¡ç®—å™¨"""
    
    @staticmethod
    def calculate_metrics(y_true, y_pred, labels=None):
        """è®¡ç®—å„ç§æŒ‡æ ‡"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if torch.is_tensor(y_true):
            y_true = y_true.cpu().numpy()
        if torch.is_tensor(y_pred):
            y_pred = y_pred.cpu().numpy()
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(y_true, y_pred)
        
        # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average='weighted', zero_division=0
        )
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        
        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'confusion_matrix': cm
        }
        
        return metrics
    
    @staticmethod
    def print_metrics(metrics: Dict[str, Any]):
        """æ‰“å°æŒ‡æ ‡"""
        logger.info("ğŸ“Š æ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
        logger.info(f"   å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
        logger.info(f"   ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
        logger.info(f"   å¬å›ç‡: {metrics['recall']:.4f}")
        logger.info(f"   F1åˆ†æ•°: {metrics['f1']:.4f}")
        
        # æ‰“å°æ··æ·†çŸ©é˜µ
        logger.info("   æ··æ·†çŸ©é˜µ:")
        cm = metrics['confusion_matrix']
        for i, row in enumerate(cm):
            logger.info(f"     {row}")
    
    @staticmethod
    def calculate_class_metrics(y_true, y_pred, class_names=None):
        """è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡"""
        if torch.is_tensor(y_true):
            y_true = y_true.cpu().numpy()
        if torch.is_tensor(y_pred):
            y_pred = y_pred.cpu().numpy()
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average=None, zero_division=0
        )
        
        class_metrics = {}
        for i in range(len(precision)):
            class_name = class_names[i] if class_names else f"Class_{i}"
            class_metrics[class_name] = {
                'precision': precision[i],
                'recall': recall[i],
                'f1': f1[i],
                'support': support[i]
            }
        
        return class_metrics 