#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è´¨é‡å¢å¼ºå™¨ - ä¸“æ³¨äºæ—¥å¿—æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹
"""

import pandas as pd
import numpy as np
import logging
import re
import json
from collections import Counter
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class LogFeatureExtractor:
    """æ—¥å¿—ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        # é”™è¯¯ç æ¨¡å¼
        self.error_patterns = [
            r'error\s*[:\s]*([A-Z0-9_]+)',  # ERROR: CODE123
            r'err\s*[:\s]*([A-Z0-9_]+)',    # ERR: CODE123
            r'([A-Z]{2,10}\d{3,6})',        # é€šç”¨é”™è¯¯ç 
            r'([A-Z]{2,10}Exception)',      # Javaå¼‚å¸¸
            r'([A-Z][a-z]+Exception)',      # å¼‚å¸¸ç±»å
        ]
        
        # è·¯å¾„æ¨¡å¼
        self.path_patterns = [
            r'([A-Za-z]:\\[^\s]+)',         # Windowsè·¯å¾„
            r'(/[^\s]+)',                    # Unixè·¯å¾„
            r'([A-Za-z0-9_/.-]+\.(java|py|js|ts|go|cpp|c|h))',  # æ–‡ä»¶è·¯å¾„
        ]
        
        # æ—¶é—´æˆ³æ¨¡å¼
        self.timestamp_patterns = [
            r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})',
            r'(\d{2}:\d{2}:\d{2})',
            r'(\d{4}/\d{2}/\d{2})',
        ]
        
        # æ•°å­—æ¨¡å¼
        self.number_patterns = [
            r'(\d+\.\d+)',                   # æµ®ç‚¹æ•°
            r'(\d+)',                        # æ•´æ•°
        ]
        
        # ç±»åæ¨¡å¼
        self.class_patterns = [
            r'([A-Z][a-zA-Z0-9_]*\.java)',  # Javaç±»æ–‡ä»¶
            r'([A-Z][a-zA-Z0-9_]*\.py)',    # Pythonæ–‡ä»¶
            r'([A-Z][a-zA-Z0-9_]*\.js)',    # JavaScriptæ–‡ä»¶
            r'([A-Z][a-zA-Z0-9_]*\.ts)',    # TypeScriptæ–‡ä»¶
        ]
        
        # æ–¹æ³•åæ¨¡å¼
        self.method_patterns = [
            r'([a-z][a-zA-Z0-9_]*\()',      # æ–¹æ³•è°ƒç”¨
            r'([a-z][a-zA-Z0-9_]*\s*:)',    # æ–¹æ³•å®šä¹‰
        ]
    
    def extract_structural_features(self, text: str) -> Dict[str, str]:
        """æå–ç»“æ„åŒ–ç‰¹å¾"""
        features = {}
        
        # æå–é”™è¯¯ç 
        error_codes = []
        for pattern in self.error_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            error_codes.extend(matches)
        features['error_codes'] = ' '.join(set(error_codes)) if error_codes else ''
        
        # æå–è·¯å¾„
        paths = []
        for pattern in self.path_patterns:
            matches = re.findall(pattern, text)
            paths.extend(matches)
        features['paths'] = ' '.join(set(paths)) if paths else ''
        
        # æå–æ—¶é—´æˆ³
        timestamps = []
        for pattern in self.timestamp_patterns:
            matches = re.findall(pattern, text)
            timestamps.extend(matches)
        features['timestamps'] = ' '.join(set(timestamps)) if timestamps else ''
        
        # æå–æ•°å­—
        numbers = []
        for pattern in self.number_patterns:
            matches = re.findall(pattern, text)
            numbers.extend(matches)
        features['numbers'] = ' '.join(set(numbers)) if numbers else ''
        
        # æå–ç±»å
        classes = []
        for pattern in self.class_patterns:
            matches = re.findall(pattern, text)
            classes.extend(matches)
        features['classes'] = ' '.join(set(classes)) if classes else ''
        
        # æå–æ–¹æ³•å
        methods = []
        for pattern in self.method_patterns:
            matches = re.findall(pattern, text)
            methods.extend(matches)
        features['methods'] = ' '.join(set(methods)) if methods else ''
        
        return features


class AdvancedLogCleaner:
    """é«˜çº§æ—¥å¿—æ¸…æ´—å™¨"""
    
    def __init__(self):
        # éœ€è¦ç§»é™¤çš„å…ƒæ•°æ®æ¨¡å¼
        self.metadata_patterns = [
            # GitHubç›¸å…³
            r'github\.com/[^\s,]+',
            r'https://github\.com/[^\s,]+',
            r'github_issue',
            r'unknown,github_issue',
            r'[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+,\d+',
            r'https://github\.com/[^\s,]+/issues/\d+',
            
            # æ—¶é—´æˆ³
            r'[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]+',
            r'[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}',
            
            # HTMLæ ‡ç­¾
            r'<[^>]+>',
            r'&[a-zA-Z]+;',
            r'\[.*?\]',
            
            # å¤šä½™å­—ç¬¦
            r'^,+|,+$',
            r'^"+|"+$',
            r'unknown,,,',
            r',unknown,,,',
            r',unknown,',
            r'unknown,',
            r'https://',
        ]
        
        # éœ€è¦æ ‡å‡†åŒ–çš„æ¨¡å¼
        self.normalization_patterns = [
            (r'\s+', ' '),  # å¤šä¸ªç©ºæ ¼å˜å•ä¸ª
            (r'^\s+|\s+$', ''),  # é¦–å°¾ç©ºæ ¼
        ]
        
        # æ—¥å¿—çº§åˆ«æ¨¡å¼
        self.log_levels = ['DEBUG', 'INFO', 'WARN', 'WARNING', 'ERROR', 'FATAL', 'TRACE']
    
    def clean_log_content(self, text: str) -> str:
        """æ¸…æ´—æ—¥å¿—å†…å®¹"""
        if pd.isna(text) or text == '':
            return ''
        
        text = str(text)
        
        # ç§»é™¤å…ƒæ•°æ®
        for pattern in self.metadata_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # æ ‡å‡†åŒ–
        for pattern, replacement in self.normalization_patterns:
            text = re.sub(pattern, replacement, text)
        
        # ç§»é™¤ä»¥é€—å·å¼€å¤´çš„éƒ¨åˆ†
        if text.startswith(','):
            text = text.lstrip(',')
        
        # ç§»é™¤ä»¥unknownå¼€å¤´çš„éƒ¨åˆ†
        if text.lower().startswith('unknown'):
            text = re.sub(r'^unknown[,\s]*', '', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def extract_log_level(self, text: str) -> str:
        """æå–æ—¥å¿—çº§åˆ«"""
        text_upper = text.upper()
        for level in self.log_levels:
            if level in text_upper:
                return level
        return 'UNKNOWN'
    
    def is_valid_log(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆæ—¥å¿—"""
        if not text or len(text.strip()) < 10:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥å¿—ç‰¹å¾
        log_indicators = ['error', 'warn', 'info', 'debug', 'exception', 'failed', 'success']
        text_lower = text.lower()
        
        return any(indicator in text_lower for indicator in log_indicators)


class LogStructureAnalyzer:
    """æ—¥å¿—ç»“æ„åˆ†æå™¨"""
    
    def __init__(self):
        self.structure_patterns = {
            'java_stack': r'at\s+([a-zA-Z0-9_.]+)\.([a-zA-Z0-9_]+)\([^)]*\)',
            'python_traceback': r'File\s+"([^"]+)"\s*,\s*line\s+(\d+)',
            'json_log': r'\{[^{}]*"[^"]*"\s*:\s*[^,{}]*\}',
            'key_value': r'([a-zA-Z0-9_]+)\s*[=:]\s*([^\s,]+)',
        }
    
    def analyze_log_structure(self, text: str) -> Dict[str, any]:
        """åˆ†ææ—¥å¿—ç»“æ„"""
        structure_info = {
            'has_java_stack': False,
            'has_python_traceback': False,
            'has_json': False,
            'has_key_value': False,
            'structure_type': 'unknown'
        }
        
        # æ£€æŸ¥Javaå †æ ˆ
        if re.search(self.structure_patterns['java_stack'], text):
            structure_info['has_java_stack'] = True
            structure_info['structure_type'] = 'java_stack'
        
        # æ£€æŸ¥Pythonå›æº¯
        elif re.search(self.structure_patterns['python_traceback'], text):
            structure_info['has_python_traceback'] = True
            structure_info['structure_type'] = 'python_traceback'
        
        # æ£€æŸ¥JSONæ ¼å¼
        elif re.search(self.structure_patterns['json_log'], text):
            structure_info['has_json'] = True
            structure_info['structure_type'] = 'json'
        
        # æ£€æŸ¥é”®å€¼å¯¹
        elif re.search(self.structure_patterns['key_value'], text):
            structure_info['has_key_value'] = True
            structure_info['structure_type'] = 'key_value'
        
        return structure_info


class DataQualityEnhancer:
    """æ•°æ®è´¨é‡å¢å¼ºå™¨"""
    
    def __init__(self):
        self.cleaner = AdvancedLogCleaner()
        self.extractor = LogFeatureExtractor()
        self.analyzer = LogStructureAnalyzer()
    
    def enhance_data_quality(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¢å¼ºæ•°æ®è´¨é‡"""
        logger.info("ğŸš€ å¼€å§‹æ•°æ®è´¨é‡å¢å¼º")
        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
        
        # 1. æ•°æ®æ¸…æ´—
        logger.info("ğŸ§¹ æ­¥éª¤1: æ•°æ®æ¸…æ´—")
        df_cleaned = self._clean_data(df)
        
        # 2. ç‰¹å¾å·¥ç¨‹
        logger.info("ğŸ”§ æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹")
        df_enhanced = self._extract_features(df_cleaned)
        
        # 3. ç»“æ„åˆ†æ
        logger.info("ğŸ“‹ æ­¥éª¤3: ç»“æ„åˆ†æ")
        df_final = self._analyze_structure(df_enhanced)
        
        # 4. è´¨é‡è¯„ä¼°
        logger.info("ğŸ“ˆ æ­¥éª¤4: è´¨é‡è¯„ä¼°")
        self._assess_quality(df_final)
        
        return df_final
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æ•°æ®"""
        # ç§»é™¤ç©ºå€¼
        df_cleaned = df.dropna(subset=['original_log', 'category'])
        
        # æ¸…æ´—æ—¥å¿—å†…å®¹
        df_cleaned['cleaned_log'] = df_cleaned['original_log'].apply(self.cleaner.clean_log_content)
        
        # æå–æ—¥å¿—çº§åˆ«
        df_cleaned['log_level'] = df_cleaned['cleaned_log'].apply(self.cleaner.extract_log_level)
        
        # è¿‡æ»¤æ— æ•ˆæ—¥å¿—
        df_cleaned = df_cleaned[df_cleaned['cleaned_log'].apply(self.cleaner.is_valid_log)]
        
        logger.info(f"âœ… æ¸…æ´—åæ•°æ®: {len(df_cleaned)} æ¡è®°å½•")
        
        # åˆ†ææ—¥å¿—çº§åˆ«åˆ†å¸ƒ
        level_counts = df_cleaned['log_level'].value_counts()
        logger.info("ğŸ“Š æ—¥å¿—çº§åˆ«åˆ†å¸ƒ:")
        for level, count in level_counts.items():
            percentage = (count / len(df_cleaned)) * 100
            logger.info(f"  {level}: {count} æ¡ ({percentage:.1f}%)")
        
        return df_cleaned
    
    def _extract_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æå–ç‰¹å¾"""
        logger.info("ğŸ” æå–ç»“æ„åŒ–ç‰¹å¾...")
        
        # æå–ç‰¹å¾
        features_list = []
        for idx, row in df.iterrows():
            features = self.extractor.extract_structural_features(row['cleaned_log'])
            features_list.append(features)
        
        # è½¬æ¢ä¸ºDataFrame
        features_df = pd.DataFrame(features_list)
        
        # åˆå¹¶ç‰¹å¾
        df_enhanced = pd.concat([df.reset_index(drop=True), features_df], axis=1)
        
        # ç»Ÿè®¡ç‰¹å¾æå–ç»“æœ
        logger.info("ğŸ“Š ç‰¹å¾æå–ç»Ÿè®¡:")
        for col in features_df.columns:
            non_empty = (features_df[col] != '').sum()
            percentage = (non_empty / len(features_df)) * 100
            logger.info(f"  {col}: {non_empty} æ¡ ({percentage:.1f}%)")
        
        return df_enhanced
    
    def _analyze_structure(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ†ææ—¥å¿—ç»“æ„"""
        logger.info("ğŸ“‹ åˆ†ææ—¥å¿—ç»“æ„...")
        
        # åˆ†æç»“æ„
        structure_list = []
        for idx, row in df.iterrows():
            structure = self.analyzer.analyze_log_structure(row['cleaned_log'])
            structure_list.append(structure)
        
        # è½¬æ¢ä¸ºDataFrame
        structure_df = pd.DataFrame(structure_list)
        
        # åˆå¹¶ç»“æ„ä¿¡æ¯
        df_final = pd.concat([df.reset_index(drop=True), structure_df], axis=1)
        
        # ç»Ÿè®¡ç»“æ„ç±»å‹
        structure_counts = df_final['structure_type'].value_counts()
        logger.info("ğŸ“Š æ—¥å¿—ç»“æ„ç±»å‹åˆ†å¸ƒ:")
        for struct_type, count in structure_counts.items():
            percentage = (count / len(df_final)) * 100
            logger.info(f"  {struct_type}: {count} æ¡ ({percentage:.1f}%)")
        
        return df_final
    
    def _assess_quality(self, df: pd.DataFrame) -> None:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        logger.info("ğŸ“ˆ æ•°æ®è´¨é‡è¯„ä¼°:")
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        total_logs = len(df)
        valid_logs = len(df[df['cleaned_log'].str.len() > 10])
        logs_with_features = len(df[df['error_codes'] != ''])
        logs_with_paths = len(df[df['paths'] != ''])
        
        logger.info(f"  ğŸ“Š æ€»æ—¥å¿—æ•°: {total_logs}")
        logger.info(f"  âœ… æœ‰æ•ˆæ—¥å¿—æ•°: {valid_logs} ({valid_logs/total_logs*100:.1f}%)")
        logger.info(f"  ğŸ” åŒ…å«é”™è¯¯ç : {logs_with_features} ({logs_with_features/total_logs*100:.1f}%)")
        logger.info(f"  ğŸ“ åŒ…å«è·¯å¾„: {logs_with_paths} ({logs_with_paths/total_logs*100:.1f}%)")
        
        # ç±»åˆ«åˆ†å¸ƒ
        category_counts = df['category'].value_counts()
        logger.info("ğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in category_counts.items():
            percentage = (count / total_logs) * 100
            logger.info(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
    
    def create_enhanced_dataset(self, input_path: str, output_path: str) -> pd.DataFrame:
        """åˆ›å»ºå¢å¼ºæ•°æ®é›†"""
        logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {input_path}")
        df = pd.read_csv(input_path)
        
        # å¢å¼ºæ•°æ®è´¨é‡
        df_enhanced = self.enhance_data_quality(df)
        
        # ä¿å­˜ç»“æœ
        df_enhanced.to_csv(output_path, index=False)
        logger.info(f"ğŸ’¾ ä¿å­˜å¢å¼ºæ•°æ®: {output_path}")
        
        return df_enhanced


def main():
    """ä¸»å‡½æ•°"""
    enhancer = DataQualityEnhancer()
    
    input_path = "data/processed_logs_final_cleaned.csv"
    output_path = "data/processed_logs_quality_enhanced.csv"
    
    # åˆ›å»ºå¢å¼ºæ•°æ®é›†
    df_enhanced = enhancer.create_enhanced_dataset(input_path, output_path)
    
    logger.info("âœ… æ•°æ®è´¨é‡å¢å¼ºå®Œæˆ!")
    
    # æ˜¾ç¤ºå¢å¼ºåçš„æ•°æ®æ ·ä¾‹
    logger.info("ğŸ“‹ å¢å¼ºæ•°æ®æ ·ä¾‹:")
    sample_cols = ['category', 'cleaned_log', 'log_level', 'error_codes', 'paths', 'structure_type']
    print(df_enhanced[sample_cols].head(3).to_string())


if __name__ == "__main__":
    main() 