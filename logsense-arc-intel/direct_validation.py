#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›´æ¥éªŒè¯è®­ç»ƒå¥½çš„ç‰¹å¾å¢å¼ºæ¨¡å‹
"""

import sys
import os
import torch
import pandas as pd
import numpy as np
import logging
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info("ğŸš€ å¼€å§‹ç›´æ¥éªŒè¯è®­ç»ƒå¥½çš„æ¨¡å‹...")
        
        # å¯¼å…¥è®­ç»ƒè„šæœ¬ä¸­çš„ç±»
        from feature_enhanced_model import DualChannelLogClassifier, StructuredFeatureExtractor
        
        # æ¨¡å‹è·¯å¾„
        model_path = "results/models/feature_enhanced_model_20250809_103658.pth"
        data_path = "data/processed_logs_advanced_enhanced.csv"
        
        logger.info(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½æ¨¡å‹
        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
        
        # æå–ç»„ä»¶
        label_encoder = checkpoint['label_encoder']
        feature_extractor = checkpoint['feature_extractor']
        model_state_dict = checkpoint['model_state_dict']
        
        logger.info(f"âœ… æ¨¡å‹ç»„ä»¶åŠ è½½å®Œæˆï¼Œç±»åˆ«æ•°: {len(label_encoder.classes_)}")
        
        # é‡æ–°åˆ›å»ºæ¨¡å‹
        model = DualChannelLogClassifier(
            vocab_size=10000,
            embedding_dim=128,
            num_filters=128,
            filter_sizes=[3, 4, 5],
            num_classes=len(label_encoder.classes_),
            struct_input_dim=1018
        )
        
        # åŠ è½½æƒé‡
        model.load_state_dict(model_state_dict)
        model.eval()
        
        logger.info("ğŸ”§ æ¨¡å‹é‡å»ºå®Œæˆ")
        
        # åŠ è½½æ•°æ®
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {data_path}")
        df = pd.read_csv(data_path)
        logger.info(f"ğŸ“ˆ æ•°æ®åŠ è½½å®Œæˆï¼Œæ€»è®°å½•æ•°: {len(df)}")
        
        # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
        logger.info("ğŸ¯ ç±»åˆ«åˆ†å¸ƒ:")
        category_counts = df['category'].value_counts()
        for category, count in category_counts.items():
            logger.info(f"   {category}: {count}")
        
        # æ‰§è¡ŒéªŒè¯
        logger.info("ğŸ” å¼€å§‹æ¨¡å‹éªŒè¯...")
        
        # æå–ç‰¹å¾
        texts = df['cleaned_log'].fillna('').astype(str).tolist()
        features = feature_extractor.extract_features(df)
        labels = label_encoder.transform(df['category'])
        
        # å‡†å¤‡è¾“å…¥
        text_tensor = torch.tensor([feature_extractor.text_to_sequence(text) for text in texts], dtype=torch.long)
        feature_tensor = torch.tensor(features, dtype=torch.float32)
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = model(text_tensor, feature_tensor)
            predictions = torch.argmax(outputs, dim=1).numpy()
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(labels, predictions)
        f1 = f1_score(labels, predictions, average='weighted')
        
        # åˆ†ç±»æŠ¥å‘Š
        class_report = classification_report(
            labels, predictions,
            target_names=label_encoder.classes_,
            output_dict=True
        )
        
        # æ··æ·†çŸ©é˜µ
        conf_matrix = confusion_matrix(labels, predictions)
        
        # è¾“å‡ºç»“æœ
        logger.info("ğŸ“Š éªŒè¯ç»“æœ:")
        logger.info(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        logger.info(f"   F1åˆ†æ•°: {f1:.4f}")
        
        # æŒ‰ç±»åˆ«åˆ†æ
        logger.info("\nğŸ¯ å„ç±»åˆ«è¯¦ç»†åˆ†æ:")
        for i, class_name in enumerate(label_encoder.classes_):
            class_mask = labels == i
            if np.sum(class_mask) > 0:
                class_accuracy = accuracy_score(labels[class_mask], predictions[class_mask])
                class_f1 = f1_score(labels[class_mask], predictions[class_mask], average='binary')
                logger.info(f"   {class_name}: å‡†ç¡®ç‡={class_accuracy:.4f}, F1={class_f1:.4f}, æ ·æœ¬æ•°={np.sum(class_mask)}")
        
        # ä¿å­˜ç»“æœ
        results_dir = "direct_validation_results"
        os.makedirs(results_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = os.path.join(results_dir, f"direct_validation_{timestamp}.json")
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = {
            'timestamp': timestamp,
            'model_path': model_path,
            'data_path': data_path,
            'overall_accuracy': accuracy,
            'overall_f1_score': f1,
            'category_analysis': {}
        }
        
        # æ·»åŠ æ¯ä¸ªç±»åˆ«çš„åˆ†æ
        for i, class_name in enumerate(label_encoder.classes_):
            class_mask = labels == i
            if np.sum(class_mask) > 0:
                class_accuracy = accuracy_score(labels[class_mask], predictions[class_mask])
                class_f1 = f1_score(labels[class_mask], predictions[class_mask], average='binary')
                
                save_data['category_analysis'][class_name] = {
                    'sample_count': int(np.sum(class_mask)),
                    'accuracy': float(class_accuracy),
                    'f1_score': float(class_f1)
                }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # è¾“å‡ºæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ¯ ç›´æ¥éªŒè¯å®Œæˆæ‘˜è¦")
        print("="*60)
        print(f"ğŸ“Š æ•´ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"ğŸ“Š æ•´ä½“F1åˆ†æ•°: {f1:.4f}")
        print(f"ğŸ“ ç»“æœä¿å­˜ä½ç½®: {results_dir}/")
        print("="*60)
        
    except Exception as e:
        logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main() 