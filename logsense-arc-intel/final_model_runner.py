#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆæ¨¡å‹è¿è¡Œå™¨ - ä½¿ç”¨è®­ç»ƒæ—¶çš„åŸå§‹æ•°æ®é‡å»ºè¯æ±‡è¡¨
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
import logging
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from datetime import datetime
import os
import json
import warnings
from collections import Counter
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å®šä¹‰è®­ç»ƒæ—¶ä½¿ç”¨çš„ç±»
class StructuredFeatureExtractor:
    """ç»“æ„åŒ–ç‰¹å¾æå–å™¨"""
    
    def __init__(self, max_tfidf_features=1000):
        self.max_tfidf_features = max_tfidf_features
        self.label_encoders = {}
        self.tfidf_vectorizer = None
        self.scaler = StandardScaler()
        self.feature_names = []

class FinalModelRunner:
    """æœ€ç»ˆæ¨¡å‹è¿è¡Œå™¨"""

    def __init__(self, model_path, data_path):
        self.model_path = model_path
        self.data_path = data_path
        
        # æ£€æµ‹Intel Arc GPU
        if torch.xpu.is_available():
            self.device = torch.device("xpu")
            logger.info("ğŸ® æ£€æµ‹åˆ°Intel Arc GPUï¼Œä½¿ç”¨XPUè®¾å¤‡")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            logger.info("ğŸ® æ£€æµ‹åˆ°NVIDIA GPUï¼Œä½¿ç”¨CUDAè®¾å¤‡")
        else:
            self.device = torch.device("cpu")
            logger.info("ğŸ’» ä½¿ç”¨CPUè®¾å¤‡")
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = "final_validation_results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        # æ¨¡å‹ç»„ä»¶
        self.model = None
        self.label_encoder = None
        self.vocab = None
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–æœ€ç»ˆæ¨¡å‹è¿è¡Œå™¨ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")

    def build_vocab_from_data(self, texts, vocab_size=8000):
        """ä»æ•°æ®æ„å»ºè¯æ±‡è¡¨ - ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ–¹æ³•"""
        logger.info("ğŸ”¤ æ„å»ºè¯æ±‡è¡¨...")
        word_counts = Counter()
        for text in texts:
            words = text.lower().split()
            word_counts.update(words)
        
        vocab = {'<PAD>': 0, '<UNK>': 1}
        for word, count in word_counts.most_common(vocab_size - 2):
            if count >= 2:
                vocab[word] = len(vocab)
        
        logger.info(f"ğŸ“š è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œå¤§å°: {len(vocab)}")
        return vocab

    def text_to_sequence(self, text, vocab, max_length=128):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ— - ä½¿ç”¨è¯æ±‡è¡¨"""
        words = text.lower().split()[:max_length]
        sequence = []
        for word in words:
            if word in vocab:
                sequence.append(vocab[word])
            else:
                sequence.append(vocab['<UNK>'])
        
        if len(sequence) < max_length:
            sequence.extend([vocab['<PAD>']] * (max_length - len(sequence)))
        return sequence[:max_length]

    def create_model(self):
        """åˆ›å»ºæ¨¡å‹ç»“æ„"""
        try:
            logger.info("ğŸ”§ åˆ›å»ºæ¨¡å‹ç»“æ„...")
            
            # åˆ›å»ºåŒé€šé“æ¨¡å‹ - åŒ¹é…è®­ç»ƒæ—¶çš„ç»“æ„
            class DualChannelModel(nn.Module):
                def __init__(self, vocab_size, embedding_dim=128, num_filters=128, 
                            filter_sizes=[3, 4, 5], num_classes=9, struct_input_dim=1018):
                    super().__init__()
                    
                    # æ–‡æœ¬ç¼–ç å™¨ (TextCNN)
                    self.text_encoder = nn.ModuleDict({
                        'embedding': nn.Embedding(vocab_size, embedding_dim),
                        'convs': nn.ModuleList([
                            nn.Conv2d(1, num_filters, (k, embedding_dim)) for k in filter_sizes
                        ]),
                        'dropout': nn.Dropout(0.5),
                        'fc': nn.Linear(len(filter_sizes) * num_filters, num_classes)
                    })
                    
                    # ç»“æ„åŒ–ç‰¹å¾MLP
                    self.struct_mlp = nn.ModuleDict({
                        'mlp': nn.Sequential(
                            nn.Linear(struct_input_dim, 256),
                            nn.ReLU(),
                            nn.Dropout(0.3),
                            nn.Linear(256, 128),
                            nn.ReLU(),
                            nn.Dropout(0.3)
                        )
                    })
                    
                    # èåˆå±‚
                    self.fusion_layer = nn.Sequential(
                        nn.Linear(len(filter_sizes) * num_filters + 128, 256),
                        nn.ReLU(),
                        nn.Dropout(0.3),
                        nn.Linear(256, num_classes)
                    )
                
                def forward(self, text_inputs, struct_inputs):
                    # æ–‡æœ¬ç‰¹å¾æå–
                    embedded = self.text_encoder['embedding'](text_inputs)
                    embedded = embedded.unsqueeze(1)
                    
                    # å·ç§¯ç‰¹å¾æå–
                    conv_outputs = []
                    for conv in self.text_encoder['convs']:
                        conv_out = F.relu(conv(embedded))
                        conv_out = conv_out.squeeze(3)
                        pooled = F.max_pool1d(conv_out, conv_out.size(2))
                        conv_outputs.append(pooled.squeeze(2))
                    
                    # æ‹¼æ¥å·ç§¯è¾“å‡º
                    text_features = torch.cat(conv_outputs, dim=1)
                    text_features = self.text_encoder['dropout'](text_features)
                    
                    # ç»“æ„åŒ–ç‰¹å¾å¤„ç†
                    struct_features = self.struct_mlp['mlp'](struct_inputs)
                    
                    # ç‰¹å¾èåˆ
                    combined_features = torch.cat([text_features, struct_features], dim=1)
                    output = self.fusion_layer(combined_features)
                    
                    return output
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            self.model = DualChannelModel(
                vocab_size=len(self.vocab),
                num_classes=len(self.label_encoder.classes_),
                struct_input_dim=1018
            )
            self.model.to(self.device)
            
            logger.info("âœ… æ¨¡å‹ç»“æ„åˆ›å»ºå®Œæˆ")
            
            # åŠ è½½æ¨¡å‹æƒé‡
            if hasattr(self, 'checkpoint') and 'model_state_dict' in self.checkpoint:
                self.model.load_state_dict(self.checkpoint['model_state_dict'])
                logger.info("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
            
            self.model.eval()
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            raise

    def load_model_weights(self):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        try:
            logger.info("ğŸ“¥ åŠ è½½æ¨¡å‹æƒé‡...")
            
            # å°è¯•åŠ è½½checkpoint
            checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
            
            # æå–ç»„ä»¶
            self.label_encoder = checkpoint['label_encoder']
            
            # åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„è¯æ±‡è¡¨
            if 'vocab' in checkpoint:
                self.vocab = checkpoint['vocab']
                logger.info(f"ğŸ“š åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„è¯æ±‡è¡¨ï¼Œå¤§å°: {len(self.vocab)}")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒæ—¶ä¿å­˜çš„è¯æ±‡è¡¨ï¼Œå°†é‡æ–°æ„å»º")
                self.vocab = None
            
            # ä¿å­˜checkpointä¾›åç»­ä½¿ç”¨
            self.checkpoint = checkpoint
            
            logger.info("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}")
            raise

    def extract_features(self, df):
        """æå–ç‰¹å¾"""
        try:
            logger.info("ğŸ”§ å¼€å§‹ç‰¹å¾æå–...")
            
            # æ–‡æœ¬ç‰¹å¾
            texts = df['cleaned_log'].fillna('').astype(str).tolist()
            
            # åŸºæœ¬ç»“æ„åŒ–ç‰¹å¾
            features = {}
            features['log_length'] = df['cleaned_log'].str.len().fillna(0)
            features['word_count'] = df['cleaned_log'].str.split().str.len().fillna(0)
            features['special_char_count'] = df['cleaned_log'].str.count(r'[!@#$%^&*()_+\-=\[\]{};\':"\\|,.<>/?]').fillna(0)
            features['uppercase_count'] = df['cleaned_log'].str.count(r'[A-Z]').fillna(0)
            features['digit_count'] = df['cleaned_log'].str.count(r'\d').fillna(0)
            
            # æ·»åŠ æ›´å¤šç»“æ„åŒ–ç‰¹å¾
            features['contains_error'] = df['cleaned_log'].str.contains(r'error|Error|ERROR', case=False).astype(int)
            features['contains_warning'] = df['cleaned_log'].str.contains(r'warn|Warn|WARNING', case=False).astype(int)
            features['contains_exception'] = df['cleaned_log'].str.contains(r'exception|Exception|EXCEPTION', case=False).astype(int)
            features['contains_failed'] = df['cleaned_log'].str.contains(r'fail|Fail|FAILED', case=False).astype(int)
            
            # TF-IDFç‰¹å¾ - è°ƒæ•´åˆ°åˆé€‚çš„å¤§å°ä»¥åŒ¹é…1018ç»´
            tfidf_vectorizer = TfidfVectorizer(max_features=1008, ngram_range=(1, 2))
            tfidf_features = tfidf_vectorizer.fit_transform(texts).toarray()
            
            # åˆå¹¶ç‰¹å¾
            struct_features = pd.DataFrame(features).values
            combined_features = np.hstack([struct_features, tfidf_features])
            
            # ç¡®ä¿è¾“å‡º1018ç»´
            if combined_features.shape[1] != 1018:
                if combined_features.shape[1] < 1018:
                    # å¦‚æœç»´åº¦ä¸å¤Ÿï¼Œç”¨é›¶å¡«å……
                    padding = np.zeros((combined_features.shape[0], 1018 - combined_features.shape[1]))
                    combined_features = np.hstack([combined_features, padding])
                else:
                    # å¦‚æœç»´åº¦è¿‡å¤šï¼Œæˆªå–å‰1018ç»´
                    combined_features = combined_features[:, :1018]
            
            logger.info(f"ğŸ”— ç‰¹å¾æå–å®Œæˆï¼Œæ€»ç»´åº¦: {combined_features.shape}")
            
            return texts, combined_features
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            raise

    def load_and_prepare_data(self):
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
        try:
            logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {self.data_path}")
            
            if self.data_path.endswith('.csv'):
                df = pd.read_csv(self.data_path)
            else:
                raise ValueError("è¯·ä½¿ç”¨CSVæ ¼å¼çš„æ•°æ®æ–‡ä»¶")
            
            # æ•°æ®æ¸…æ´—
            df_cleaned = df.dropna(subset=['cleaned_log', 'category'])
            df_cleaned = df_cleaned[df_cleaned['cleaned_log'].str.strip() != '']
            
            logger.info(f"ğŸ“ˆ æ•°æ®åŠ è½½å®Œæˆï¼Œæ€»è®°å½•æ•°: {len(df_cleaned)}")
            
            # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
            logger.info("ğŸ¯ ç±»åˆ«åˆ†å¸ƒ:")
            category_counts = df_cleaned['category'].value_counts()
            for category, count in category_counts.items():
                logger.info(f"   {category}: {count}")
            
            # ä½¿ç”¨å·²åŠ è½½çš„è¯æ±‡è¡¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™é‡æ–°æ„å»º
            texts = df_cleaned['cleaned_log'].tolist()
            if self.vocab is None:
                logger.info("ğŸ”¤ é‡æ–°æ„å»ºè¯æ±‡è¡¨...")
                self.vocab = self.build_vocab_from_data(texts)
            else:
                logger.info(f"ğŸ“š ä½¿ç”¨å·²åŠ è½½çš„è¯æ±‡è¡¨ï¼Œå¤§å°: {len(self.vocab)}")
            
            # æå–ç»“æ„åŒ–ç‰¹å¾
            texts, struct_features = self.extract_features(df_cleaned)
            
            # å‡†å¤‡æ ‡ç­¾
            labels = self.label_encoder.transform(df_cleaned['category'])
            
            return df_cleaned, texts, struct_features, labels
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise

    def validate_model(self, texts, features, labels, validation_name="validation"):
        """éªŒè¯æ¨¡å‹"""
        try:
            logger.info(f"ğŸ” å¼€å§‹æ¨¡å‹éªŒè¯: {validation_name}")
            
            # å‡†å¤‡æ•°æ®
            text_tensor = torch.tensor([self.text_to_sequence(text, self.vocab) for text in texts], dtype=torch.long)
            feature_tensor = torch.tensor(features, dtype=torch.float32)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            text_tensor = text_tensor.to(self.device)
            feature_tensor = feature_tensor.to(self.device)
            
            # é¢„æµ‹
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(text_tensor, feature_tensor)
                predictions = torch.argmax(outputs, dim=1).cpu().numpy()
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(labels, predictions)
            f1 = f1_score(labels, predictions, average='weighted')
            
            # åˆ†ç±»æŠ¥å‘Š
            class_report = classification_report(
                labels, predictions,
                target_names=self.label_encoder.classes_,
                output_dict=True
            )
            
            # æ··æ·†çŸ©é˜µ
            conf_matrix = confusion_matrix(labels, predictions)
            
            logger.info(f"ğŸ“Š éªŒè¯ç»“æœ - {validation_name}:")
            logger.info(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
            logger.info(f"   F1åˆ†æ•°: {f1:.4f}")
            
            return {
                'validation_name': validation_name,
                'accuracy': accuracy,
                'f1_score': f1,
                'predictions': predictions,
                'true_labels': labels,
                'classification_report': class_report,
                'confusion_matrix': conf_matrix,
                'class_names': self.label_encoder.classes_
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            raise

    def run_validation(self):
        """è¿è¡ŒéªŒè¯"""
        try:
            logger.info("ğŸš€ å¼€å§‹æ¨¡å‹éªŒè¯...")
            
            # 1. åŠ è½½æ¨¡å‹æƒé‡ï¼ˆè·å–æ ‡ç­¾ç¼–ç å™¨ï¼‰
            self.load_model_weights()
            
            # 2. åŠ è½½å¹¶å‡†å¤‡æ•°æ®ï¼ˆæ„å»ºè¯æ±‡è¡¨ï¼‰
            df, texts, features, labels = self.load_and_prepare_data()
            
            # 3. åˆ›å»ºæ¨¡å‹ç»“æ„
            self.create_model()
            
            # 4. éªŒè¯æ¨¡å‹
            result = self.validate_model(texts, features, labels, "full_validation")
            
            # 5. ä¿å­˜ç»“æœ
            self.save_results(result)
            
            logger.info("ğŸ‰ éªŒè¯å®Œæˆï¼")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
            raise

    def save_results(self, result):
        """ä¿å­˜ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜éªŒè¯ç»“æœ
            results_file = os.path.join(self.results_dir, f"final_validation_results_{timestamp}.json")
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            save_data = {
                'timestamp': timestamp,
                'model_path': self.model_path,
                'data_path': self.data_path,
                'vocab_size': len(self.vocab),
                'num_classes': len(self.label_encoder.classes_),
                'accuracy': result['accuracy'],
                'f1_score': result['f1_score'],
                'classification_report': result['classification_report'],
                'confusion_matrix': result['confusion_matrix'].tolist(),
                'class_names': result['class_names'].tolist()
            }
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
            
            # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
            self.save_detailed_report(result, timestamp)
            
            return results_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            raise

    def save_detailed_report(self, result, timestamp):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Š"""
        try:
            report_file = os.path.join(self.results_dir, f"final_detailed_report_{timestamp}.txt")
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write("æœ€ç»ˆæ¨¡å‹éªŒè¯æŠ¥å‘Š\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"éªŒè¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ¨¡å‹è·¯å¾„: {self.model_path}\n")
                f.write(f"æ•°æ®è·¯å¾„: {self.data_path}\n")
                f.write(f"è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}\n")
                f.write(f"ç±»åˆ«æ•°é‡: {len(self.label_encoder.classes_)}\n\n")
                
                # éªŒè¯ç»“æœ
                f.write("ğŸ“Š éªŒè¯ç»“æœ\n")
                f.write("-" * 40 + "\n")
                f.write(f"å‡†ç¡®ç‡: {result['accuracy']:.4f}\n")
                f.write(f"F1åˆ†æ•°: {result['f1_score']:.4f}\n\n")
                
                # åˆ†ç±»æŠ¥å‘Š
                f.write("ğŸ¯ åˆ†ç±»æŠ¥å‘Š\n")
                f.write("-" * 40 + "\n")
                for class_name in result['class_names']:
                    if class_name in result['classification_report']:
                        class_metrics = result['classification_report'][class_name]
                        f.write(f"\nç±»åˆ«: {class_name}\n")
                        f.write(f"  ç²¾ç¡®ç‡: {class_metrics['precision']:.4f}\n")
                        f.write(f"  å¬å›ç‡: {class_metrics['recall']:.4f}\n")
                        f.write(f"  F1åˆ†æ•°: {class_metrics['f1-score']:.4f}\n")
                        f.write(f"  æ”¯æŒæ•°: {class_metrics['support']}\n")
                
                f.write("\n" + "=" * 80 + "\n")
                f.write("æŠ¥å‘Šç»“æŸ\n")
                f.write("=" * 80 + "\n")
            
            logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è¯¦ç»†æŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æœ€ç»ˆæ¨¡å‹è¿è¡Œå™¨")
    parser.add_argument("--model_path", type=str, required=True, help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("--data_path", type=str, required=True, help="éªŒè¯æ•°æ®è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨¡å‹è¿è¡Œå™¨
    runner = FinalModelRunner(
        model_path=args.model_path,
        data_path=args.data_path
    )
    
    # è¿è¡ŒéªŒè¯
    result = runner.run_validation()
    
    # è¾“å‡ºæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ¯ éªŒè¯å®Œæˆæ‘˜è¦")
    print("="*60)
    print(f"ğŸ“Š å‡†ç¡®ç‡: {result['accuracy']:.4f}")
    print(f"ğŸ“Š F1åˆ†æ•°: {result['f1_score']:.4f}")
    print(f"ğŸ“ ç»“æœä¿å­˜ä½ç½®: {runner.results_dir}/")
    print("="*60)

if __name__ == "__main__":
    main()
