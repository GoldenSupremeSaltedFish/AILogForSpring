# -*- coding: utf-8 -*-
"""
Issueæ—¥å¿—æ•°æ®å‡†å¤‡è„šæœ¬
å°†æ¸…æ´—åçš„issueæ—¥å¿—æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼ï¼ŒåŒ…æ‹¬æ•°æ®æ··æ·†
"""

import pandas as pd
import numpy as np
import re
import random
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class IssueDataPreparer:
    """Issueæ—¥å¿—æ•°æ®å‡†å¤‡å™¨"""
    
    def __init__(self):
        # å®šä¹‰æ•°æ®æ··æ·†è§„åˆ™
        self.augmentation_patterns = {
            'stack_exception': [
                # æ›¿æ¢å¸¸è§çš„å¼‚å¸¸ç±»å
                (r'NullPointerException', ['NullPointerException', 'IllegalArgumentException', 'RuntimeException']),
                (r'SQLException', ['SQLException', 'DataAccessException', 'DatabaseException']),
                (r'ConnectionException', ['ConnectionException', 'ConnectException', 'SocketException']),
                # æ›¿æ¢å¸¸è§çš„åŒ…å
                (r'org\.springframework\.', ['org.springframework.', 'org.hibernate.', 'org.apache.']),
                (r'java\.lang\.', ['java.lang.', 'java.util.', 'java.io.']),
                # æ›¿æ¢å¸¸è§çš„é”™è¯¯ä¿¡æ¯
                (r'Connection refused', ['Connection refused', 'Connection timeout', 'Connection failed']),
                (r'Table.*not found', ['Table not found', 'Column not found', 'Schema not found']),
            ],
            'startup_failure': [
                (r'Failed to start', ['Failed to start', 'Unable to start', 'Cannot start']),
                (r'Port.*in use', ['Port already in use', 'Address already in use', 'Port is busy']),
                (r'BeanCreationException', ['BeanCreationException', 'ContextLoadException', 'ApplicationContextException']),
            ],
            'auth_error': [
                (r'Authentication failed', ['Authentication failed', 'Login failed', 'Auth failed']),
                (r'Access denied', ['Access denied', 'Permission denied', 'Forbidden']),
                (r'Invalid token', ['Invalid token', 'Token expired', 'Token invalid']),
            ],
            'db_error': [
                (r'SQLException', ['SQLException', 'DatabaseException', 'DataAccessException']),
                (r'Connection.*failed', ['Connection failed', 'Connection refused', 'Connection timeout']),
                (r'Duplicate entry', ['Duplicate entry', 'Constraint violation', 'Unique constraint']),
            ],
            'connection_issue': [
                (r'Connection.*timeout', ['Connection timeout', 'Connection refused', 'Connection failed']),
                (r'Network.*unreachable', ['Network unreachable', 'Host unreachable', 'No route to host']),
            ],
            'timeout': [
                (r'Request.*timeout', ['Request timeout', 'Response timeout', 'Operation timeout']),
                (r'Read.*timeout', ['Read timeout', 'Write timeout', 'Socket timeout']),
            ],
            'performance': [
                (r'OutOfMemoryError', ['OutOfMemoryError', 'MemoryError', 'Heap space']),
                (r'GC.*overhead', ['GC overhead', 'Garbage collection', 'Memory pressure']),
            ],
            'config': [
                (r'Configuration.*error', ['Configuration error', 'Config error', 'Property error']),
                (r'Property.*not found', ['Property not found', 'Environment variable not found', 'Config not found']),
            ],
            'business': [
                (r'Business.*error', ['Business error', 'Logic error', 'Service error']),
                (r'Validation.*failed', ['Validation failed', 'Invalid input', 'Data validation failed']),
            ],
            'normal': [
                (r'INFO.*Started', ['INFO: Started', 'INFO: Running', 'INFO: Application started']),
                (r'DEBUG.*', ['DEBUG: ', 'TRACE: ', 'INFO: ']),
            ]
        }
    
    def load_cleaned_data(self, data_path: str) -> pd.DataFrame:
        """åŠ è½½æ¸…æ´—åçš„æ•°æ®"""
        logger.info(f"ğŸ“‚ åŠ è½½æ¸…æ´—æ•°æ®: {data_path}")
        
        try:
            df = pd.read_csv(data_path)
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•")
            return df
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def augment_data(self, df: pd.DataFrame, augmentation_factor: float = 0.3) -> pd.DataFrame:
        """æ•°æ®å¢å¼º/æ··æ·†"""
        logger.info("ğŸ”„ å¼€å§‹æ•°æ®å¢å¼º...")
        
        # æ£€æŸ¥åˆ—åå¹¶ç»Ÿä¸€
        if 'label' in df.columns:
            label_col = 'label'
        elif 'auto_label' in df.columns:
            label_col = 'auto_label'
        else:
            logger.error("âŒ æœªæ‰¾åˆ°æ ‡ç­¾åˆ—")
            return df
            
        if 'text' in df.columns:
            text_col = 'text'
        elif 'cleaned_message' in df.columns:
            text_col = 'cleaned_message'
        else:
            logger.error("âŒ æœªæ‰¾åˆ°æ–‡æœ¬åˆ—")
            return df
        
        augmented_data = []
        
        for _, row in df.iterrows():
            # æ·»åŠ åŸå§‹æ•°æ®
            augmented_data.append(row.to_dict())
            
            # æ ¹æ®ç±»åˆ«è¿›è¡Œæ•°æ®å¢å¼º
            label = row.get(label_col, 'unknown')
            message = row.get(text_col, '')
            
            if label in self.augmentation_patterns and random.random() < augmentation_factor:
                # å¯¹éƒ¨åˆ†æ•°æ®è¿›è¡Œå¢å¼º
                augmented_message = self._augment_message(message, label)
                if augmented_message != message:
                    augmented_row = row.copy()
                    augmented_row[text_col] = augmented_message
                    augmented_row['is_augmented'] = True
                    augmented_data.append(augmented_row)
        
        augmented_df = pd.DataFrame(augmented_data)
        logger.info(f"ğŸ“Š æ•°æ®å¢å¼ºå®Œæˆ: {len(df)} -> {len(augmented_df)} æ¡è®°å½•")
        
        return augmented_df
    
    def _augment_message(self, message: str, label: str) -> str:
        """å¯¹å•æ¡æ¶ˆæ¯è¿›è¡Œå¢å¼º"""
        if label not in self.augmentation_patterns:
            return message
        
        augmented_message = message
        
        for pattern, replacements in self.augmentation_patterns[label]:
            if re.search(pattern, augmented_message, re.IGNORECASE):
                # éšæœºé€‰æ‹©ä¸€ä¸ªæ›¿æ¢é¡¹
                replacement = random.choice(replacements)
                augmented_message = re.sub(pattern, replacement, augmented_message, flags=re.IGNORECASE)
                break  # åªæ›¿æ¢ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ¨¡å¼
        
        return augmented_message
    
    def balance_data(self, df: pd.DataFrame, max_per_class: int = 1000) -> pd.DataFrame:
        """å¹³è¡¡å„ç±»åˆ«æ•°æ®é‡"""
        logger.info(f"âš–ï¸ å¼€å§‹æ•°æ®å¹³è¡¡ (æ¯ç±»æœ€å¤š {max_per_class} æ¡)...")
        
        # æ£€æŸ¥åˆ—åå¹¶ç»Ÿä¸€
        if 'label' in df.columns:
            label_col = 'label'
        elif 'auto_label' in df.columns:
            label_col = 'auto_label'
        else:
            logger.error("âŒ æœªæ‰¾åˆ°æ ‡ç­¾åˆ—")
            return df
        
        balanced_dfs = []
        for category in df[label_col].unique():
            category_df = df[df[label_col] == category]
            if len(category_df) > max_per_class:
                # éšæœºé‡‡æ ·
                category_df = category_df.sample(n=max_per_class, random_state=42)
                logger.info(f"  {category}: é‡‡æ · {len(category_df)} æ¡ (åŸå§‹ {len(df[df[label_col] == category])} æ¡)")
            else:
                logger.info(f"  {category}: ä½¿ç”¨å…¨éƒ¨ {len(category_df)} æ¡")
            
            balanced_dfs.append(category_df)
        
        balanced_df = pd.concat(balanced_dfs, ignore_index=True)
        logger.info(f"ğŸ“Š æ•°æ®å¹³è¡¡å®Œæˆ: {len(balanced_df)} æ¡è®°å½•")
        
        return balanced_df
    
    def prepare_training_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å‡†å¤‡è®­ç»ƒæ•°æ®æ ¼å¼"""
        logger.info("ğŸ“‹ å‡†å¤‡è®­ç»ƒæ•°æ®æ ¼å¼...")
        
        # æ£€æŸ¥åˆ—åå¹¶ç»Ÿä¸€
        if 'text' in df.columns and 'label' in df.columns:
            # å·²ç»æ˜¯æ ‡å‡†æ ¼å¼
            training_df = df[['text', 'label']].copy()
        elif 'cleaned_message' in df.columns and 'auto_label' in df.columns:
            # éœ€è¦è½¬æ¢æ ¼å¼
            training_df = df[['cleaned_message', 'auto_label']].copy()
            training_df.columns = ['text', 'label']
        else:
            logger.error("âŒ æœªæ‰¾åˆ°å¿…è¦çš„æ–‡æœ¬å’Œæ ‡ç­¾åˆ—")
            return df
        
        # ç§»é™¤ç©ºæ–‡æœ¬
        training_df = training_df[training_df['text'].str.len() > 0]
        
        # æ·»åŠ æ•°æ®æ¥æºæ ‡è¯†
        training_df['source'] = 'issue_logs'
        training_df['timestamp'] = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logger.info(f"âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ: {len(training_df)} æ¡è®°å½•")
        
        return training_df
    
    def extract_structured_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æå–ç»“æ„åŒ–ç‰¹å¾"""
        logger.info("ğŸ” æå–ç»“æ„åŒ–ç‰¹å¾...")
        
        # æ£€æŸ¥æ–‡æœ¬åˆ—å
        if 'text' in df.columns:
            text_col = 'text'
        elif 'cleaned_message' in df.columns:
            text_col = 'cleaned_message'
        else:
            logger.error("âŒ æœªæ‰¾åˆ°æ–‡æœ¬åˆ—")
            return df
        
        def extract_features(text):
            features = {}
            
            # æ–‡æœ¬é•¿åº¦ç‰¹å¾
            features['text_length'] = len(text)
            features['word_count'] = len(text.split())
            
            # é”™è¯¯ç±»å‹ç‰¹å¾
            features['has_exception'] = 1 if re.search(r'Exception|Error', text, re.IGNORECASE) else 0
            features['has_stack_trace'] = 1 if re.search(r'at |Caused by:', text, re.IGNORECASE) else 0
            features['has_sql'] = 1 if re.search(r'SQL|Database|Table|Column', text, re.IGNORECASE) else 0
            features['has_connection'] = 1 if re.search(r'Connection|Socket|Network', text, re.IGNORECASE) else 0
            features['has_auth'] = 1 if re.search(r'Authentication|Authorization|Token|Login', text, re.IGNORECASE) else 0
            features['has_timeout'] = 1 if re.search(r'Timeout|timeout', text, re.IGNORECASE) else 0
            features['has_memory'] = 1 if re.search(r'Memory|OutOfMemory|GC', text, re.IGNORECASE) else 0
            features['has_config'] = 1 if re.search(r'Configuration|Property|Config', text, re.IGNORECASE) else 0
            
            # æ—¥å¿—çº§åˆ«ç‰¹å¾
            features['has_error'] = 1 if re.search(r'ERROR|FATAL', text, re.IGNORECASE) else 0
            features['has_warn'] = 1 if re.search(r'WARN|WARNING', text, re.IGNORECASE) else 0
            features['has_info'] = 1 if re.search(r'INFO', text, re.IGNORECASE) else 0
            features['has_debug'] = 1 if re.search(r'DEBUG|TRACE', text, re.IGNORECASE) else 0
            
            # ç‰¹æ®Šå­—ç¬¦ç‰¹å¾
            features['has_colon'] = 1 if ':' in text else 0
            features['has_bracket'] = 1 if re.search(r'[\(\)\[\]\{\}]', text) else 0
            features['has_dot'] = 1 if '.' in text else 0
            features['has_underscore'] = 1 if '_' in text else 0
            
            return features
        
        # æå–ç‰¹å¾
        feature_dfs = []
        for _, row in df.iterrows():
            features = extract_features(row[text_col])
            feature_df = pd.DataFrame([features])
            feature_dfs.append(feature_df)
        
        features_df = pd.concat(feature_dfs, ignore_index=True)
        
        # åˆå¹¶ç‰¹å¾åˆ°åŸå§‹æ•°æ®
        result_df = pd.concat([df, features_df], axis=1)
        
        logger.info(f"âœ… ç»“æ„åŒ–ç‰¹å¾æå–å®Œæˆ: {len(features_df.columns)} ä¸ªç‰¹å¾")
        
        return result_df
    
    def save_processed_data(self, df: pd.DataFrame, output_path: str):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        logger.info(f"ğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°: {output_path}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸ: {len(df)} æ¡è®°å½•")
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
        if 'label' in df.columns:
            label_counts = df['label'].value_counts()
            logger.info("ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
            for label, count in label_counts.items():
                percentage = (count / len(df)) * 100
                logger.info(f"  {label}: {count} æ¡ ({percentage:.1f}%)")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ Issueæ—¥å¿—æ•°æ®å‡†å¤‡å¼€å§‹...")
    
    # åˆå§‹åŒ–æ•°æ®å‡†å¤‡å™¨
    preparer = IssueDataPreparer()
    
    # åŠ è½½æ¸…æ´—åçš„æ•°æ®
    input_file = "../DATA_OUTPUT/issue_logs_training_20250812_001907.csv"
    df = preparer.load_cleaned_data(input_file)
    
    if df.empty:
        logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œé€€å‡º")
        return
    
    # æ•°æ®å¢å¼º
    df_augmented = preparer.augment_data(df, augmentation_factor=0.3)
    
    # æ•°æ®å¹³è¡¡
    df_balanced = preparer.balance_data(df_augmented, max_per_class=1000)
    
    # æå–ç»“æ„åŒ–ç‰¹å¾
    df_with_features = preparer.extract_structured_features(df_balanced)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    training_df = preparer.prepare_training_data(df_with_features)
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"data/issue_logs_processed_{timestamp}.csv"
    preparer.save_processed_data(training_df, output_file)
    
    # åŒæ—¶ä¿å­˜ä¸€ä¸ªæ ‡å‡†å‘½åçš„æ–‡ä»¶
    standard_output = "data/processed_logs_issue_enhanced.csv"
    preparer.save_processed_data(training_df, standard_output)
    
    logger.info("ğŸ‰ Issueæ—¥å¿—æ•°æ®å‡†å¤‡å®Œæˆï¼")
    logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    logger.info(f"  - {output_file}")
    logger.info(f"  - {standard_output}")

if __name__ == "__main__":
    main()
