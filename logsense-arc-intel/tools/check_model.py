#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦æ­£ç¡®ä¿å­˜äº†æƒé‡å’Œè¯æ±‡è¡¨
"""

import torch
import os
import sys
import re
import pandas as pd
import numpy as np
from collections import Counter
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer

# å®šä¹‰ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„StructuredFeatureExtractorç±»
class StructuredFeatureExtractor:
    """ç»“æ„åŒ–ç‰¹å¾æå–å™¨"""
    
    def __init__(self, max_tfidf_features=1000):
        self.max_tfidf_features = max_tfidf_features
        self.label_encoders = {}
        self.tfidf_vectorizer = None
        self.scaler = None
        self.feature_names = []
    
    def extract_features(self, df):
        """æå–ç»“æ„åŒ–ç‰¹å¾"""
        features = {}
        
        # æ—¥å¿—çº§åˆ«
        features['log_level'] = df['log_level'].fillna('unknown')
        
        # æ˜¯å¦åŒ…å«å †æ ˆè·Ÿè¸ª
        features['contains_stack'] = df['original_log'].str.contains(r'at\s+\w+\.\w+\(', regex=True).astype(int)
        
        # å¼‚å¸¸ç±»å‹
        features['exception_type'] = df['original_log'].str.extract(r'(\w+Exception|\w+Error)')[0].fillna('none')
        
        # æ–‡ä»¶è·¯å¾„
        features['file_path'] = df['original_log'].str.extract(r'at\s+([\w\.]+)\.\w+\([^)]*\)')[0].fillna('unknown')
        
        # å‡½æ•°å
        features['function'] = df['original_log'].str.extract(r'at\s+[\w\.]+\.(\w+)\([^)]*\)')[0].fillna('unknown')
        
        # è¡Œå·
        features['line_number'] = df['original_log'].str.extract(r'\(([^)]+\.java:\d+)\)')[0].fillna('unknown')
        
        # æ•°å­—ç‰¹å¾
        features['number_count'] = df['original_log'].str.count(r'\d+')
        features['special_char_count'] = df['original_log'].str.count(r'[^\w\s]')
        features['log_length'] = df['original_log'].str.len()
        features['word_count'] = df['original_log'].str.split().str.len()
        
        # æ—¶é—´æˆ³ç‰¹å¾
        features['has_timestamp'] = df['original_log'].str.contains(r'\d{4}-\d{2}-\d{2}|\d{2}:\d{2}:\d{2}').astype(int)
        
        # è·¯å¾„æ·±åº¦
        paths = df['original_log'].str.extract(r'([/\w\.]+\.java)')[0].fillna('')
        features['path_depth'] = paths.str.count('/').fillna(0) + paths.str.count(r'\\').fillna(0)
        
        return pd.DataFrame(features)

def check_model_file(model_path):
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"""
    print(f"ğŸ” æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: {model_path}")
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return False
    
    try:
        # æ·»åŠ å®‰å…¨çš„å…¨å±€å˜é‡
        torch.serialization.add_safe_globals([
            'sklearn.preprocessing._label.LabelEncoder',
            'sklearn.preprocessing._data.StandardScaler', 
            'sklearn.feature_extraction.text.TfidfVectorizer'
        ])
        
        # åŠ è½½æ¨¡å‹æ–‡ä»¶
        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
        
        print("âœ… æ¨¡å‹æ–‡ä»¶åŠ è½½æˆåŠŸ!")
        print(f"ğŸ“¦ æ£€æŸ¥ç‚¹åŒ…å«çš„é”®: {list(checkpoint.keys())}")
        
        # æ£€æŸ¥å„ä¸ªç»„ä»¶
        checks = {
            'model_state_dict': 'model_state_dict' in checkpoint,
            'label_encoder': 'label_encoder' in checkpoint,
            'feature_extractor': 'feature_extractor' in checkpoint,
            'vocab': 'vocab' in checkpoint,
            'best_acc': 'best_acc' in checkpoint,
            'best_f1': 'best_f1' in checkpoint,
            'timestamp': 'timestamp' in checkpoint
        }
        
        print("\nğŸ“Š ç»„ä»¶æ£€æŸ¥ç»“æœ:")
        for component, exists in checks.items():
            status = "âœ…" if exists else "âŒ"
            print(f"   {status} {component}")
        
        # è¯¦ç»†æ£€æŸ¥è¯æ±‡è¡¨
        if 'vocab' in checkpoint:
            vocab = checkpoint['vocab']
            print(f"\nğŸ“š è¯æ±‡è¡¨ä¿¡æ¯:")
            print(f"   å¤§å°: {len(vocab)}")
            print(f"   åŒ…å«PAD: {'<PAD>' in vocab}")
            print(f"   åŒ…å«UNK: {'<UNK>' in vocab}")
            print(f"   å‰5ä¸ªè¯: {list(vocab.items())[:5]}")
        else:
            print("\nâŒ è¯æ±‡è¡¨æœªæ‰¾åˆ°!")
        
        # æ£€æŸ¥æ¨¡å‹æƒé‡
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
            print(f"\nğŸ¤– æ¨¡å‹æƒé‡ä¿¡æ¯:")
            print(f"   æƒé‡é”®æ•°é‡: {len(state_dict)}")
            print(f"   æƒé‡é”®: {list(state_dict.keys())}")
        else:
            print("\nâŒ æ¨¡å‹æƒé‡æœªæ‰¾åˆ°!")
        
        # æ£€æŸ¥æ ‡ç­¾ç¼–ç å™¨
        if 'label_encoder' in checkpoint:
            label_encoder = checkpoint['label_encoder']
            print(f"\nğŸ·ï¸ æ ‡ç­¾ç¼–ç å™¨ä¿¡æ¯:")
            print(f"   ç±»åˆ«æ•°: {len(label_encoder.classes_)}")
            print(f"   ç±»åˆ«: {list(label_encoder.classes_)}")
        else:
            print("\nâŒ æ ‡ç­¾ç¼–ç å™¨æœªæ‰¾åˆ°!")
        
        # æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
        if 'best_acc' in checkpoint and 'best_f1' in checkpoint:
            print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
            print(f"   æœ€ä½³å‡†ç¡®ç‡: {checkpoint['best_acc']:.4f}")
            print(f"   æœ€ä½³F1åˆ†æ•°: {checkpoint['best_f1']:.4f}")
        
        if 'timestamp' in checkpoint:
            print(f"\nâ° ä¿å­˜æ—¶é—´: {checkpoint['timestamp']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥æœ€æ–°çš„feature_enhancedæ¨¡å‹æ–‡ä»¶
    models_dir = "results/models"
    
    if not os.path.exists(models_dir):
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
        return
    
    # è·å–æ‰€æœ‰feature_enhancedæ¨¡å‹æ–‡ä»¶
    model_files = [f for f in os.listdir(models_dir) if 'feature_enhanced' in f and f.endswith('.pth')]
    
    if not model_files:
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°feature_enhancedæ¨¡å‹æ–‡ä»¶: {models_dir}")
        return
    
    # æŒ‰æ—¶é—´æ’åºï¼Œæ£€æŸ¥æœ€æ–°çš„
    model_files.sort()
    latest_model = os.path.join(models_dir, model_files[-1])
    
    print("=" * 60)
    print("ğŸ” æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å·¥å…·")
    print("=" * 60)
    
    success = check_model_file(latest_model)
    
    if success:
        print("\n" + "=" * 60)
        print("âœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å®Œæˆ!")
        print("=" * 60)
    else:
        print("\n" + "=" * 60)
        print("âŒ æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å¤±è´¥!")
        print("=" * 60)

if __name__ == "__main__":
    main()
