# -*- coding: utf-8 -*-
"""
Issueæ—¥å¿—æ•°æ®é€‚é…å™¨
å°†æˆ‘ä»¬çš„issueæ—¥å¿—æ•°æ®è½¬æ¢ä¸ºéªŒè¯è„šæœ¬æœŸæœ›çš„æ ¼å¼
"""

import pandas as pd
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def adapt_issue_data(input_file: str, output_file: str):
    """é€‚é…issueæ—¥å¿—æ•°æ®æ ¼å¼"""
    logger.info(f"ğŸ”„ å¼€å§‹é€‚é…æ•°æ®æ ¼å¼...")
    logger.info(f"ğŸ“‚ è¾“å…¥æ–‡ä»¶: {input_file}")
    logger.info(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    try:
        # è¯»å–åŸå§‹æ•°æ®
        df = pd.read_csv(input_file)
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•")
        
        # æ˜¾ç¤ºåŸå§‹åˆ—å
        logger.info(f"ğŸ“‹ åŸå§‹åˆ—å: {df.columns.tolist()}")
        
        # åˆ›å»ºé€‚é…åçš„æ•°æ®æ¡†
        adapted_df = pd.DataFrame()
        
        # æ˜ å°„åˆ—å
        if 'text' in df.columns:
            adapted_df['cleaned_log'] = df['text']
        elif 'cleaned_message' in df.columns:
            adapted_df['cleaned_log'] = df['cleaned_message']
        else:
            raise ValueError("æœªæ‰¾åˆ°æ–‡æœ¬åˆ—")
        
        if 'label' in df.columns:
            adapted_df['category'] = df['label']
        elif 'auto_label' in df.columns:
            adapted_df['category'] = df['auto_label']
        else:
            raise ValueError("æœªæ‰¾åˆ°æ ‡ç­¾åˆ—")
        
        # æ·»åŠ å…¶ä»–å¿…è¦çš„åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        for col in ['source', 'timestamp', 'is_augmented']:
            if col in df.columns:
                adapted_df[col] = df[col]
        
        # æ·»åŠ ç»“æ„åŒ–ç‰¹å¾åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        feature_cols = [col for col in df.columns if col not in ['text', 'label', 'cleaned_message', 'auto_label', 'source', 'timestamp', 'is_augmented']]
        for col in feature_cols:
            adapted_df[col] = df[col]
        
        # æ•°æ®æ¸…æ´—
        adapted_df = adapted_df.dropna(subset=['cleaned_log', 'category'])
        adapted_df = adapted_df[adapted_df['cleaned_log'].str.strip() != '']
        
        logger.info(f"âœ… æ•°æ®é€‚é…å®Œæˆ: {len(adapted_df)} æ¡è®°å½•")
        logger.info(f"ğŸ“‹ é€‚é…ååˆ—å: {adapted_df.columns.tolist()}")
        
        # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
        category_counts = adapted_df['category'].value_counts()
        logger.info("ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in category_counts.items():
            percentage = (count / len(adapted_df)) * 100
            logger.info(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
        
        # ä¿å­˜é€‚é…åçš„æ•°æ®
        adapted_df.to_csv(output_file, index=False, encoding='utf-8')
        logger.info(f"ğŸ’¾ é€‚é…æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é€‚é…å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ Issueæ—¥å¿—æ•°æ®é€‚é…å™¨å¯åŠ¨...")
    
    # è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„
    input_file = "data/processed_logs_issue_enhanced.csv"
    output_file = "data/issue_logs_adapted_for_validation.csv"
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(input_file).exists():
        logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    # æ‰§è¡Œæ•°æ®é€‚é…
    if adapt_issue_data(input_file, output_file):
        logger.info("ğŸ‰ æ•°æ®é€‚é…å®Œæˆï¼")
        logger.info(f"ğŸ“ é€‚é…åçš„æ–‡ä»¶: {output_file}")
        logger.info("ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨æ­¤æ–‡ä»¶è¿›è¡Œæ¨¡å‹éªŒè¯")
    else:
        logger.error("âŒ æ•°æ®é€‚é…å¤±è´¥")

if __name__ == "__main__":
    main()
