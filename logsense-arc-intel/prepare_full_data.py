#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‡†å¤‡å®Œæ•´æ•°æ®é›† - åˆå¹¶æ‰€æœ‰11ç§ç±»åˆ«çš„æ—¥å¿—
"""

import os
import pandas as pd
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç±»åˆ«æ˜ å°„
CATEGORIES = {
    '01_å †æ ˆå¼‚å¸¸_stack_exception': 'stack_exception',
    '02_æ•°æ®åº“å¼‚å¸¸_database_exception': 'database_exception',
    '03_è¿æ¥é—®é¢˜_connection_issue': 'connection_issue',
    '04_è®¤è¯æˆæƒ_auth_authorization': 'auth_authorization',
    '05_é…ç½®ç¯å¢ƒ_config_environment': 'config_environment',
    '06_ä¸šåŠ¡é€»è¾‘_business_logic': 'business_logic',
    '07_æ­£å¸¸æ“ä½œ_normal_operation': 'normal_operation',
    '08_ç›‘æ§å¿ƒè·³_monitoring_heartbeat': 'monitoring_heartbeat',
    '09_å†…å­˜æ€§èƒ½_memory_performance': 'memory_performance',
    '10_è¶…æ—¶é”™è¯¯_timeout': 'timeout',
    '11_SpringBootå¯åŠ¨å¤±è´¥_spring_boot_startup_failure': 'spring_boot_startup_failure'
}

def load_all_data(data_path: str):
    """åŠ è½½æ‰€æœ‰ç±»åˆ«æ•°æ®"""
    logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")
    
    all_data = []
    
    for folder_name, category in CATEGORIES.items():
        folder_path = os.path.join(data_path, folder_name)
        
        if not os.path.exists(folder_path):
            logger.warning(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {folder_path}")
            continue
            
        logger.info(f"ğŸ“ å¤„ç†: {category}")
        
        # æŸ¥æ‰¾CSVæ–‡ä»¶
        csv_files = list(Path(folder_path).glob("*.csv"))
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file)
                df['category'] = category
                all_data.append(df)
                logger.info(f"  ğŸ“„ {csv_file.name}: {len(df)} æ¡")
            except Exception as e:
                logger.error(f"âŒ è¯»å–å¤±è´¥: {csv_file.name} - {e}")
    
    if not all_data:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®")
        return None
    
    combined_df = pd.concat(all_data, ignore_index=True)
    logger.info(f"âœ… åˆå¹¶å®Œæˆ: {len(combined_df)} æ¡è®°å½•")
    
    return combined_df

def main():
    """ä¸»å‡½æ•°"""
    data_path = r"C:\Users\30871\Desktop\AILogForSpring\DATA_OUTPUT"
    output_path = "data/processed_logs_full.csv"
    
    # åŠ è½½æ•°æ®
    df = load_all_data(data_path)
    if df is None:
        return
    
    # æ¸…æ´—æ•°æ®
    df_cleaned = df.dropna(subset=['original_log', 'category'])
    df_cleaned = df_cleaned[df_cleaned['original_log'].str.strip() != '']
    df_cleaned = df_cleaned.drop_duplicates(subset=['original_log'])
    
    # åˆ†æåˆ†å¸ƒ
    category_counts = df_cleaned['category'].value_counts()
    logger.info("ğŸ“Š æ•°æ®åˆ†å¸ƒ:")
    for category, count in category_counts.items():
        percentage = (count / len(df_cleaned)) * 100
        logger.info(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
    
    # ä¿å­˜æ•°æ®
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df_cleaned.to_csv(output_path, index=False, encoding='utf-8')
    
    logger.info(f"âœ… å®Œæˆ! ä¿å­˜åˆ°: {output_path}")
    logger.info(f"ğŸ“Š æœ€ç»ˆæ•°æ®: {len(df_cleaned)} æ¡, {len(category_counts)} ä¸ªç±»åˆ«")

if __name__ == "__main__":
    main() 