#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Intel Arc GPU åŠ é€Ÿçš„APIæœåŠ¡å™¨
"""

import torch
import torch.nn as nn
import logging
import json
import os
import sys
from datetime import datetime
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import ModelFactory
from data import LogPreprocessor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class LogPredictor:
    """æ—¥å¿—é¢„æµ‹å™¨"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self.label_encoder = None
        self.preprocessor = LogPreprocessor()
        
        # è®¾ç½®è®¾å¤‡
        if torch.xpu.is_available():
            self.device = torch.device("xpu:0")
            logger.info("âœ… ä½¿ç”¨Intel XPU GPUåŠ é€Ÿæ¨ç†")
        else:
            self.device = torch.device("cpu")
            logger.warning("âš ï¸ ä½¿ç”¨CPUæ¨ç†")
        
        self.load_model()
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        logger.info(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {self.model_path}")
        
        checkpoint = torch.load(self.model_path, map_location=self.device)
        
        # è·å–æ¨¡å‹é…ç½®
        model_config = checkpoint.get('model_config', {})
        self.label_encoder = checkpoint.get('label_encoder', {})
        
        # åˆ›å»ºæ¨¡å‹
        self.model = ModelFactory.create_model("textcnn", **model_config)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ - ç±»åˆ«æ•°: {len(self.label_encoder)}")
        logger.info(f"ğŸ–¥ï¸  æ¨ç†è®¾å¤‡: {self.device}")
    
    def preprocess_text(self, text: str) -> torch.Tensor:
        """é¢„å¤„ç†æ–‡æœ¬"""
        # æ¸…ç†å’Œæ ‡å‡†åŒ–æ–‡æœ¬
        processed_text = self.preprocessor.normalize_text(text)
        
        # ç®€å•åˆ†è¯
        tokens = processed_text.split()[:128]  # æœ€å¤§é•¿åº¦128
        token_ids = [hash(token) % 10000 for token in tokens]  # è¯æ±‡è¡¨å¤§å°10000
        
        # è¡¥é½åˆ°å›ºå®šé•¿åº¦
        if len(token_ids) < 128:
            token_ids += [0] * (128 - len(token_ids))
        
        return torch.tensor(token_ids, dtype=torch.long).unsqueeze(0).to(self.device)
    
    def predict(self, text: str) -> Dict[str, Any]:
        """é¢„æµ‹æ—¥å¿—ç±»åˆ«"""
        try:
            # é¢„å¤„ç†
            input_tensor = self.preprocess_text(text)
            
            # æ¨ç†
            with torch.no_grad():
                outputs = self.model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                predicted_class = torch.argmax(outputs, dim=1).item()
                confidence = probabilities[0][predicted_class].item()
            
            # è·å–ç±»åˆ«åç§°
            class_name = self.label_encoder.get(predicted_class, f"class_{predicted_class}")
            
            # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
            all_probabilities = {}
            for class_id, class_name in self.label_encoder.items():
                all_probabilities[class_name] = probabilities[0][class_id].item()
            
            result = {
                'predicted_class': class_name,
                'confidence': confidence,
                'all_probabilities': all_probabilities,
                'input_text': text,
                'processed_text': self.preprocessor.normalize_text(text)
            }
            
            return result
            
        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
            return {
                'error': str(e),
                'input_text': text
            }
    
    def batch_predict(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡é¢„æµ‹"""
        results = []
        for text in texts:
            result = self.predict(text)
            results.append(result)
        return results


class GPULogAPI:
    """GPUåŠ é€Ÿçš„æ—¥å¿—APIæœåŠ¡å™¨"""
    
    def __init__(self, model_path: str):
        self.predictor = LogPredictor(model_path)
        logger.info("ğŸš€ GPUåŠ é€Ÿæ—¥å¿—APIæœåŠ¡å™¨å·²å¯åŠ¨")
    
    def predict_single(self, text: str) -> Dict[str, Any]:
        """å•æ¡æ—¥å¿—é¢„æµ‹"""
        return self.predictor.predict(text)
    
    def predict_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ—¥å¿—é¢„æµ‹"""
        return self.predictor.batch_predict(texts)
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_path': self.predictor.model_path,
            'device': str(self.predictor.device),
            'num_classes': len(self.predictor.label_encoder),
            'classes': list(self.predictor.label_encoder.values()),
            'model_parameters': sum(p.numel() for p in self.predictor.model.parameters())
        }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="GPUåŠ é€Ÿæ—¥å¿—åˆ†ç±»API")
    parser.add_argument("--model", type=str, 
                       default="results/models_gpu/arc_gpu_model_textcnn_best_20250805_003813.pth",
                       help="æ¨¡å‹æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--test", action="store_true", help="è¿è¡Œæµ‹è¯•")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.model):
        logger.error(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        return
    
    # åˆ›å»ºAPIå®ä¾‹
    api = GPULogAPI(args.model)
    
    if args.test:
        # è¿è¡Œæµ‹è¯•
        logger.info("ğŸ§ª è¿è¡Œæµ‹è¯•...")
        
        test_texts = [
            "Error: Connection timeout to database server",
            "Info: User login successful from IP 192.168.1.100",
            "Warning: Disk space is running low on /var/log",
            "Exception: java.lang.NullPointerException at com.example.Main.main",
            "Database connection failed: timeout after 30 seconds"
        ]
        
        for text in test_texts:
            result = api.predict_single(text)
            logger.info(f"ğŸ“ è¾“å…¥: {text}")
            logger.info(f"ğŸ¯ é¢„æµ‹: {result['predicted_class']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
            logger.info("---")
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    model_info = api.get_model_info()
    logger.info("ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    logger.info(f"   è®¾å¤‡: {model_info['device']}")
    logger.info(f"   ç±»åˆ«æ•°: {model_info['num_classes']}")
    logger.info(f"   ç±»åˆ«: {model_info['classes']}")
    logger.info(f"   å‚æ•°æ•°é‡: {model_info['model_parameters']:,}")


if __name__ == "__main__":
    main() 