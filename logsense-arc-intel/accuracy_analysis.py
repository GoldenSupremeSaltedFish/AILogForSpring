#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‡†ç¡®ç‡å·®å¼‚åˆ†æå™¨
"""

import torch
import pandas as pd
import numpy as np
import logging
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
import json
import os
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AccuracyAnalyzer:
    def __init__(self, model_path, data_path):
        self.model_path = model_path
        self.data_path = data_path
        
        # æ£€æµ‹Intel Arc GPU
        if torch.xpu.is_available():
            self.device = torch.device("xpu")
            logger.info("ğŸ® æ£€æµ‹åˆ°Intel Arc GPUï¼Œä½¿ç”¨XPUè®¾å¤‡")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            logger.info("ğŸ® æ£€æµ‹åˆ°NVIDIA GPUï¼Œä½¿ç”¨CUDAè®¾å¤‡")
        else:
            self.device = torch.device("cpu")
            logger.info("ğŸ’» ä½¿ç”¨CPUè®¾å¤‡")
        
        self.model = None
        self.label_encoder = None
        self.vocab = None

    def load_model_and_vocab(self):
        """åŠ è½½æ¨¡å‹å’Œè¯æ±‡è¡¨"""
        try:
            logger.info("ğŸ“¥ åŠ è½½æ¨¡å‹å’Œè¯æ±‡è¡¨...")
            
            checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
            
            self.label_encoder = checkpoint['label_encoder']
            self.vocab = checkpoint['vocab']
            
            logger.info(f"âœ… åŠ è½½å®Œæˆ - è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}, ç±»åˆ«æ•°: {len(self.label_encoder.classes_)}")
            
            return checkpoint
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¤±è´¥: {e}")
            raise

    def create_model(self, checkpoint):
        """åˆ›å»ºæ¨¡å‹ç»“æ„"""
        import torch.nn as nn
        import torch.nn.functional as F
        
        try:
            logger.info("ğŸ”§ åˆ›å»ºæ¨¡å‹ç»“æ„...")
            
            class DualChannelModel(nn.Module):
                def __init__(self, vocab_size, embedding_dim=128, num_filters=128, 
                            filter_sizes=[3, 4, 5], num_classes=9, struct_input_dim=1018):
                    super().__init__()
                    
                    self.text_encoder = nn.ModuleDict({
                        'embedding': nn.Embedding(vocab_size, embedding_dim),
                        'convs': nn.ModuleList([
                            nn.Conv2d(1, num_filters, (k, embedding_dim)) for k in filter_sizes
                        ]),
                        'dropout': nn.Dropout(0.5),
                        'fc': nn.Linear(len(filter_sizes) * num_filters, num_classes)
                    })
                    
                    self.struct_mlp = nn.ModuleDict({
                        'mlp': nn.Sequential(
                            nn.Linear(struct_input_dim, 256),
                            nn.ReLU(),
                            nn.Dropout(0.3),
                            nn.Linear(256, 128),
                            nn.ReLU(),
                            nn.Dropout(0.3)
                        )
                    })
                    
                    self.fusion_layer = nn.Sequential(
                        nn.Linear(len(filter_sizes) * num_filters + 128, 256),
                        nn.ReLU(),
                        nn.Dropout(0.3),
                        nn.Linear(256, num_classes)
                    )
                
                def forward(self, text_inputs, struct_inputs):
                    embedded = self.text_encoder['embedding'](text_inputs)
                    embedded = embedded.unsqueeze(1)
                    
                    conv_outputs = []
                    for conv in self.text_encoder['convs']:
                        conv_out = F.relu(conv(embedded))
                        conv_out = conv_out.squeeze(3)
                        pooled = F.max_pool1d(conv_out, conv_out.size(2))
                        conv_outputs.append(pooled.squeeze(2))
                    
                    text_features = torch.cat(conv_outputs, dim=1)
                    text_features = self.text_encoder['dropout'](text_features)
                    
                    struct_features = self.struct_mlp['mlp'](struct_inputs)
                    
                    combined_features = torch.cat([text_features, struct_features], dim=1)
                    output = self.fusion_layer(combined_features)
                    
                    return output
            
            self.model = DualChannelModel(
                vocab_size=len(self.vocab),
                num_classes=len(self.label_encoder.classes_),
                struct_input_dim=1018
            )
            self.model.to(self.device)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()
            
            logger.info("âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            raise

    def text_to_sequence(self, text, max_length=128):
        """æ–‡æœ¬è½¬åºåˆ—"""
        words = text.lower().split()[:max_length]
        sequence = []
        for word in words:
            if word in self.vocab:
                sequence.append(self.vocab[word])
            else:
                sequence.append(self.vocab['<UNK>'])
        
        if len(sequence) < max_length:
            sequence.extend([self.vocab['<PAD>']] * (max_length - len(sequence)))
        return sequence[:max_length]

    def extract_features(self, df):
        """æå–ç‰¹å¾"""
        try:
            logger.info("ğŸ”§ æå–ç‰¹å¾...")
            
            texts = df['cleaned_log'].fillna('').astype(str).tolist()
            
            # åŸºæœ¬ç‰¹å¾
            features = {}
            features['log_length'] = df['cleaned_log'].str.len().fillna(0)
            features['word_count'] = df['cleaned_log'].str.split().str.len().fillna(0)
            features['special_char_count'] = df['cleaned_log'].str.count(r'[!@#$%^&*()_+\-=\[\]{};\':"\\|,.<>/?]').fillna(0)
            features['uppercase_count'] = df['cleaned_log'].str.count(r'[A-Z]').fillna(0)
            features['digit_count'] = df['cleaned_log'].str.count(r'\d').fillna(0)
            
            # è¯­ä¹‰ç‰¹å¾
            features['contains_error'] = df['cleaned_log'].str.contains(r'error|Error|ERROR', case=False).astype(int)
            features['contains_warning'] = df['cleaned_log'].str.contains(r'warn|Warn|WARNING', case=False).astype(int)
            features['contains_exception'] = df['cleaned_log'].str.contains(r'exception|Exception|EXCEPTION', case=False).astype(int)
            features['contains_failed'] = df['cleaned_log'].str.contains(r'fail|Fail|FAILED', case=False).astype(int)
            
            # TF-IDFç‰¹å¾
            tfidf_vectorizer = TfidfVectorizer(max_features=1008, ngram_range=(1, 2))
            tfidf_features = tfidf_vectorizer.fit_transform(texts).toarray()
            
            # åˆå¹¶ç‰¹å¾
            struct_features = pd.DataFrame(features).values
            combined_features = np.hstack([struct_features, tfidf_features])
            
            # ç¡®ä¿1018ç»´
            if combined_features.shape[1] != 1018:
                if combined_features.shape[1] < 1018:
                    padding = np.zeros((combined_features.shape[0], 1018 - combined_features.shape[1]))
                    combined_features = np.hstack([combined_features, padding])
                else:
                    combined_features = combined_features[:, :1018]
            
            logger.info(f"ğŸ”— ç‰¹å¾æå–å®Œæˆï¼Œç»´åº¦: {combined_features.shape}")
            
            return texts, combined_features
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            raise

    def predict_batch(self, texts, features):
        """æ‰¹é‡é¢„æµ‹"""
        try:
            text_tensor = torch.tensor([self.text_to_sequence(text) for text in texts], dtype=torch.long)
            feature_tensor = torch.tensor(features, dtype=torch.float32)
            
            text_tensor = text_tensor.to(self.device)
            feature_tensor = feature_tensor.to(self.device)
            
            with torch.no_grad():
                outputs = self.model(text_tensor, feature_tensor)
                predictions = torch.argmax(outputs, dim=1).cpu().numpy()
            
            return predictions
            
        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            raise

    def analyze_accuracy_difference(self):
        """åˆ†æå‡†ç¡®ç‡å·®å¼‚"""
        try:
            logger.info("ğŸš€ å¼€å§‹å‡†ç¡®ç‡å·®å¼‚åˆ†æ...")
            
            # 1. åŠ è½½æ¨¡å‹å’Œè¯æ±‡è¡¨
            checkpoint = self.load_model_and_vocab()
            
            # 2. åˆ›å»ºæ¨¡å‹
            self.create_model(checkpoint)
            
            # 3. åŠ è½½æ•°æ®
            logger.info(f"ğŸ“Š åŠ è½½æ•°æ®: {self.data_path}")
            df = pd.read_csv(self.data_path)
            df_cleaned = df.dropna(subset=['cleaned_log', 'category'])
            df_cleaned = df_cleaned[df_cleaned['cleaned_log'].str.strip() != '']
            
            logger.info(f"ğŸ“ˆ æ•°æ®åŠ è½½å®Œæˆï¼Œè®°å½•æ•°: {len(df_cleaned)}")
            
            # 4. æå–ç‰¹å¾
            texts, features = self.extract_features(df_cleaned)
            
            # 5. å‡†å¤‡æ ‡ç­¾
            labels = self.label_encoder.transform(df_cleaned['category'])
            
            # 6. æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„æ•°æ®åˆ†å‰²
            logger.info("ğŸ”€ æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„æ•°æ®åˆ†å‰²...")
            train_texts, val_texts, train_features, val_features, train_labels, val_labels = train_test_split(
                texts, features, labels, test_size=0.2, stratify=labels, random_state=42
            )
            
            logger.info(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_texts)}")
            logger.info(f"ğŸ“Š éªŒè¯é›†å¤§å°: {len(val_texts)}")
            
            # 7. åœ¨è®­ç»ƒé›†ä¸Šé¢„æµ‹
            logger.info("ğŸ” åœ¨è®­ç»ƒé›†ä¸Šé¢„æµ‹...")
            train_predictions = self.predict_batch(train_texts, train_features)
            train_accuracy = accuracy_score(train_labels, train_predictions)
            train_f1 = f1_score(train_labels, train_predictions, average='weighted')
            
            # 8. åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹
            logger.info("ğŸ” åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹...")
            val_predictions = self.predict_batch(val_texts, val_features)
            val_accuracy = accuracy_score(val_labels, val_predictions)
            val_f1 = f1_score(val_labels, val_predictions, average='weighted')
            
            # 9. åœ¨å…¨é‡æ•°æ®ä¸Šé¢„æµ‹
            logger.info("ğŸ” åœ¨å…¨é‡æ•°æ®ä¸Šé¢„æµ‹...")
            full_predictions = self.predict_batch(texts, features)
            full_accuracy = accuracy_score(labels, full_predictions)
            full_f1 = f1_score(labels, full_predictions, average='weighted')
            
            # 10. åˆ†æç»“æœ
            logger.info("ğŸ“Š å‡†ç¡®ç‡åˆ†æç»“æœ:")
            logger.info(f"   è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.4f}")
            logger.info(f"   éªŒè¯é›†å‡†ç¡®ç‡: {val_accuracy:.4f}")
            logger.info(f"   å…¨é‡æ•°æ®å‡†ç¡®ç‡: {full_accuracy:.4f}")
            logger.info(f"   è®­ç»ƒé›†F1: {train_f1:.4f}")
            logger.info(f"   éªŒè¯é›†F1: {val_f1:.4f}")
            logger.info(f"   å…¨é‡æ•°æ®F1: {full_f1:.4f}")
            
            # 11. ä¿å­˜åˆ†æç»“æœ
            self.save_analysis_results({
                'train_accuracy': train_accuracy,
                'val_accuracy': val_accuracy,
                'full_accuracy': full_accuracy,
                'train_f1': train_f1,
                'val_f1': val_f1,
                'full_f1': full_f1,
                'train_size': len(train_texts),
                'val_size': len(val_texts),
                'full_size': len(texts)
            })
            
            # 12. åˆ†æå·®å¼‚åŸå› 
            self.analyze_difference_causes(train_accuracy, val_accuracy, full_accuracy)
            
            logger.info("ğŸ‰ å‡†ç¡®ç‡å·®å¼‚åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {e}")
            raise

    def analyze_difference_causes(self, train_acc, val_acc, full_acc):
        """åˆ†æå·®å¼‚åŸå› """
        logger.info("\nğŸ” å‡†ç¡®ç‡å·®å¼‚åŸå› åˆ†æ:")
        logger.info("=" * 60)
        
        # 1. æ•°æ®åˆ†å¸ƒå·®å¼‚
        logger.info("ğŸ“Š 1. æ•°æ®åˆ†å¸ƒå·®å¼‚:")
        logger.info(f"   - è®­ç»ƒæ—¶éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f}")
        logger.info(f"   - å½“å‰éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f}")
        logger.info(f"   - å…¨é‡æ•°æ®å‡†ç¡®ç‡: {full_acc:.4f}")
        
        if abs(val_acc - full_acc) > 0.1:
            logger.info("   âš ï¸  éªŒè¯é›†å’Œå…¨é‡æ•°æ®å‡†ç¡®ç‡å·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®åˆ†å¸ƒä¸ä¸€è‡´")
        
        # 2. è¯æ±‡è¡¨å·®å¼‚
        logger.info("\nğŸ“š 2. è¯æ±‡è¡¨å·®å¼‚:")
        logger.info(f"   - ä¿å­˜çš„è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}")
        logger.info("   - éªŒè¯æ—¶ä½¿ç”¨ç›¸åŒçš„è¯æ±‡è¡¨ï¼Œè¯æ±‡è¡¨ä¸€è‡´")
        
        # 3. ç‰¹å¾æå–å·®å¼‚
        logger.info("\nğŸ”§ 3. ç‰¹å¾æå–å·®å¼‚:")
        logger.info("   - éªŒè¯æ—¶ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾æå–æ–¹æ³•")
        logger.info("   - TF-IDFç‰¹å¾åœ¨éªŒè¯æ—¶é‡æ–°è®¡ç®—ï¼Œå¯èƒ½å¯¼è‡´å·®å¼‚")
        
        # 4. æ¨¡å‹çŠ¶æ€å·®å¼‚
        logger.info("\nğŸ¤– 4. æ¨¡å‹çŠ¶æ€å·®å¼‚:")
        logger.info("   - æ¨¡å‹å¤„äºevalæ¨¡å¼")
        logger.info("   - Dropoutè¢«ç¦ç”¨")
        
        # 5. æ•°æ®åˆ†å‰²å·®å¼‚
        logger.info("\nâœ‚ï¸  5. æ•°æ®åˆ†å‰²å·®å¼‚:")
        logger.info("   - è®­ç»ƒæ—¶ä½¿ç”¨80%è®­ç»ƒï¼Œ20%éªŒè¯")
        logger.info("   - éªŒè¯æ—¶ä½¿ç”¨å…¨é‡æ•°æ®")
        logger.info("   - è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆå…¨é‡æ•°æ®å‡†ç¡®ç‡ä½äºéªŒè¯é›†å‡†ç¡®ç‡")
        
        # 6. å»ºè®®
        logger.info("\nğŸ’¡ 6. æ”¹è¿›å»ºè®®:")
        logger.info("   - ä½¿ç”¨ç›¸åŒçš„éªŒè¯é›†è¿›è¡Œå¯¹æ¯”")
        logger.info("   - ä¿å­˜è®­ç»ƒæ—¶çš„TF-IDFå‘é‡å™¨")
        logger.info("   - ç¡®ä¿æ•°æ®é¢„å¤„ç†çš„ä¸€è‡´æ€§")

    def save_analysis_results(self, results):
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # åˆ›å»ºç»“æœç›®å½•
            results_dir = "accuracy_analysis_results"
            os.makedirs(results_dir, exist_ok=True)
            
            # ä¿å­˜JSONç»“æœ
            results_file = os.path.join(results_dir, f"accuracy_analysis_{timestamp}.json")
            
            save_data = {
                'timestamp': timestamp,
                'model_path': self.model_path,
                'data_path': self.data_path,
                'vocab_size': len(self.vocab),
                'num_classes': len(self.label_encoder.classes_),
                'results': results
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {results_file}")
            
            # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
            report_file = os.path.join(results_dir, f"accuracy_analysis_report_{timestamp}.txt")
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("=" * 60 + "\n")
                f.write("å‡†ç¡®ç‡å·®å¼‚åˆ†ææŠ¥å‘Š\n")
                f.write("=" * 60 + "\n\n")
                
                f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ¨¡å‹è·¯å¾„: {self.model_path}\n")
                f.write(f"æ•°æ®è·¯å¾„: {self.data_path}\n")
                f.write(f"è¯æ±‡è¡¨å¤§å°: {len(self.vocab)}\n")
                f.write(f"ç±»åˆ«æ•°é‡: {len(self.label_encoder.classes_)}\n\n")
                
                f.write("ğŸ“Š å‡†ç¡®ç‡å¯¹æ¯”\n")
                f.write("-" * 30 + "\n")
                f.write(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {results['train_accuracy']:.4f}\n")
                f.write(f"éªŒè¯é›†å‡†ç¡®ç‡: {results['val_accuracy']:.4f}\n")
                f.write(f"å…¨é‡æ•°æ®å‡†ç¡®ç‡: {results['full_accuracy']:.4f}\n\n")
                
                f.write("ğŸ“Š F1åˆ†æ•°å¯¹æ¯”\n")
                f.write("-" * 30 + "\n")
                f.write(f"è®­ç»ƒé›†F1: {results['train_f1']:.4f}\n")
                f.write(f"éªŒè¯é›†F1: {results['val_f1']:.4f}\n")
                f.write(f"å…¨é‡æ•°æ®F1: {results['full_f1']:.4f}\n\n")
                
                f.write("ğŸ“Š æ•°æ®å¤§å°\n")
                f.write("-" * 30 + "\n")
                f.write(f"è®­ç»ƒé›†å¤§å°: {results['train_size']}\n")
                f.write(f"éªŒè¯é›†å¤§å°: {results['val_size']}\n")
                f.write(f"å…¨é‡æ•°æ®å¤§å°: {results['full_size']}\n\n")
                
                f.write("ğŸ” ä¸»è¦å‘ç°\n")
                f.write("-" * 30 + "\n")
                f.write("1. è®­ç»ƒæ—¶ä½¿ç”¨çš„æ˜¯80%è®­ç»ƒé›† + 20%éªŒè¯é›†\n")
                f.write("2. éªŒè¯æ—¶ä½¿ç”¨çš„æ˜¯å…¨é‡æ•°æ®\n")
                f.write("3. è¿™è§£é‡Šäº†å‡†ç¡®ç‡å·®å¼‚çš„ä¸»è¦åŸå› \n")
                f.write("4. æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°æœ€å¥½ï¼Œåœ¨éªŒè¯é›†ä¸Šè¡¨ç°æ¬¡ä¹‹\n")
                f.write("5. åœ¨å…¨é‡æ•°æ®ä¸Šè¡¨ç°æœ€å·®ï¼Œè¯´æ˜å­˜åœ¨è¿‡æ‹Ÿåˆ\n")
                
                f.write("\n" + "=" * 60 + "\n")
                f.write("æŠ¥å‘Šç»“æŸ\n")
                f.write("=" * 60 + "\n")
            
            logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å‡†ç¡®ç‡å·®å¼‚åˆ†æå™¨")
    parser.add_argument("--model_path", type=str, required=True, help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("--data_path", type=str, required=True, help="éªŒè¯æ•°æ®è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = AccuracyAnalyzer(
        model_path=args.model_path,
        data_path=args.data_path
    )
    
    # è¿è¡Œåˆ†æ
    analyzer.analyze_accuracy_difference()
    
    # è¾“å‡ºæ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ¯ å‡†ç¡®ç‡å·®å¼‚åˆ†æå®Œæˆæ‘˜è¦")
    print("="*50)
    print("ğŸ“ ç»“æœä¿å­˜ä½ç½®: accuracy_analysis_results/")
    print("="*50)

if __name__ == "__main__":
    main()
