#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ•°æ®è´¨é‡å¢å¼ºå™¨ - åŒ…å«æ™ºèƒ½æ•°æ®å¹³è¡¡å’Œç²¾ç»†ç‰¹å¾å·¥ç¨‹
"""

import pandas as pd
import numpy as np
import logging
import re
from collections import Counter
from sklearn.utils import resample
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class AdvancedFeatureExtractor:
    """é«˜çº§ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        # é”™è¯¯ç æ¨¡å¼ - æ›´ç²¾ç¡®çš„åŒ¹é…
        self.error_patterns = [
            r'error\s*[:\s]*([A-Z0-9_]{3,10})',  # ERROR: CODE123
            r'err\s*[:\s]*([A-Z0-9_]{3,10})',    # ERR: CODE123
            r'([A-Z]{2,10}\d{3,6})',             # é€šç”¨é”™è¯¯ç 
            r'([A-Z]{2,10}Exception)',           # Javaå¼‚å¸¸
            r'([A-Z][a-z]+Exception)',           # å¼‚å¸¸ç±»å
            r'([A-Z][a-z]+Error)',               # é”™è¯¯ç±»å
        ]
        
        # è·¯å¾„æ¨¡å¼ - æ›´ç²¾ç¡®çš„åŒ¹é…
        self.path_patterns = [
            r'([A-Za-z]:\\[^\s,]+)',             # Windowsè·¯å¾„
            r'(/[^\s,]+)',                        # Unixè·¯å¾„
            r'([A-Za-z0-9_/.-]+\.(java|py|js|ts|go|cpp|c|h))',  # æ–‡ä»¶è·¯å¾„
            r'([a-zA-Z0-9_.]+\.(java|py|js|ts))', # ç±»æ–‡ä»¶
        ]
        
        # æ•°å­—æ¨¡å¼ - æ›´ç²¾ç¡®çš„åŒ¹é…
        self.number_patterns = [
            r'(\d+\.\d+)',                        # æµ®ç‚¹æ•°
            r'(\d{3,6})',                         # é”™è¯¯ä»£ç 
            r'(\d+)',                             # æ•´æ•°
        ]
        
        # ç±»åæ¨¡å¼ - æ›´ç²¾ç¡®çš„åŒ¹é…
        self.class_patterns = [
            r'([A-Z][a-zA-Z0-9_]*\.java)',       # Javaç±»æ–‡ä»¶
            r'([A-Z][a-zA-Z0-9_]*\.py)',         # Pythonæ–‡ä»¶
            r'([A-Z][a-zA-Z0-9_]*\.js)',         # JavaScriptæ–‡ä»¶
            r'([A-Z][a-zA-Z0-9_]*\.ts)',         # TypeScriptæ–‡ä»¶
        ]
        
        # æ–¹æ³•åæ¨¡å¼
        self.method_patterns = [
            r'([a-z][a-zA-Z0-9_]*\()',           # æ–¹æ³•è°ƒç”¨
            r'([a-z][a-zA-Z0-9_]*\s*:)',         # æ–¹æ³•å®šä¹‰
        ]
        
        # æ—¶é—´æˆ³æ¨¡å¼
        self.timestamp_patterns = [
            r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})',
            r'(\d{2}:\d{2}:\d{2})',
            r'(\d{4}/\d{2}/\d{2})',
        ]
    
    def extract_advanced_features(self, text: str) -> dict:
        """æå–é«˜çº§ç‰¹å¾"""
        features = {}
        
        # æå–é”™è¯¯ç 
        error_codes = []
        for pattern in self.error_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                for match in matches:
                    if isinstance(match, tuple):
                        error_codes.extend([m for m in match if m and len(m) > 2])
                    else:
                        if match and len(match) > 2:
                            error_codes.append(match)
        features['error_codes'] = ' '.join(set(error_codes)) if error_codes else ''
        
        # æå–è·¯å¾„
        paths = []
        for pattern in self.path_patterns:
            matches = re.findall(pattern, text)
            if matches:
                for match in matches:
                    if isinstance(match, tuple):
                        paths.extend([m for m in match if m and len(m) > 3])
                    else:
                        if match and len(match) > 3:
                            paths.append(match)
        features['paths'] = ' '.join(set(paths)) if paths else ''
        
        # æå–æ•°å­—
        numbers = []
        for pattern in self.number_patterns:
            matches = re.findall(pattern, text)
            if matches:
                for match in matches:
                    if isinstance(match, tuple):
                        numbers.extend([m for m in match if m])
                    else:
                        numbers.append(match)
        features['numbers'] = ' '.join(set(numbers)) if numbers else ''
        
        # æå–ç±»å
        classes = []
        for pattern in self.class_patterns:
            matches = re.findall(pattern, text)
            if matches:
                for match in matches:
                    if isinstance(match, tuple):
                        classes.extend([m for m in match if m])
                    else:
                        classes.append(match)
        features['classes'] = ' '.join(set(classes)) if classes else ''
        
        # æå–æ–¹æ³•å
        methods = []
        for pattern in self.method_patterns:
            matches = re.findall(pattern, text)
            if matches:
                for match in matches:
                    if isinstance(match, tuple):
                        methods.extend([m for m in match if m])
                    else:
                        methods.append(match)
        features['methods'] = ' '.join(set(methods)) if methods else ''
        
        # æå–æ—¶é—´æˆ³
        timestamps = []
        for pattern in self.timestamp_patterns:
            matches = re.findall(pattern, text)
            if matches:
                for match in matches:
                    if isinstance(match, tuple):
                        timestamps.extend([m for m in match if m])
                    else:
                        timestamps.append(match)
        features['timestamps'] = ' '.join(set(timestamps)) if timestamps else ''
        
        return features


class SmartDataBalancer:
    """æ™ºèƒ½æ•°æ®å¹³è¡¡å™¨"""
    
    def __init__(self, target_samples_per_class=500, min_samples_per_class=10):
        self.target_samples_per_class = target_samples_per_class
        self.min_samples_per_class = min_samples_per_class
    
    def balance_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ™ºèƒ½å¹³è¡¡æ•°æ®"""
        logger.info(f"ğŸ¯ å¼€å§‹æ™ºèƒ½æ•°æ®å¹³è¡¡ï¼Œç›®æ ‡æ¯ç±»æ ·æœ¬æ•°: {self.target_samples_per_class}")
        
        balanced_dfs = []
        category_counts = df['category'].value_counts()
        
        for category in category_counts.index:
            category_df = df[df['category'] == category].copy()
            current_count = len(category_df)
            
            logger.info(f"ğŸ“Š å¤„ç†ç±»åˆ«: {category} (å½“å‰: {current_count} æ¡)")
            
            if current_count < self.min_samples_per_class:
                logger.warning(f"  âš ï¸ ç±»åˆ« {category} æ ·æœ¬æ•°è¿‡å°‘ ({current_count} < {self.min_samples_per_class})ï¼Œè·³è¿‡")
                continue
            
            if current_count < self.target_samples_per_class:
                # æ™ºèƒ½ä¸Šé‡‡æ ·
                balanced_df = self._smart_oversample(category_df, self.target_samples_per_class)
                logger.info(f"  âœ… æ™ºèƒ½ä¸Šé‡‡æ ·åˆ° {len(balanced_df)} æ¡")
            elif current_count > self.target_samples_per_class:
                # æ™ºèƒ½ä¸‹é‡‡æ ·
                balanced_df = self._smart_undersample(category_df, self.target_samples_per_class)
                logger.info(f"  âœ… æ™ºèƒ½ä¸‹é‡‡æ ·åˆ° {len(balanced_df)} æ¡")
            else:
                # ä¿æŒåŸæ ·
                balanced_df = category_df
                logger.info(f"  âœ… ä¿æŒ {current_count} æ¡")
            
            balanced_dfs.append(balanced_df)
        
        # åˆå¹¶æ‰€æœ‰ç±»åˆ«
        balanced_df = pd.concat(balanced_dfs, ignore_index=True)
        
        # æ‰“ä¹±æ•°æ®
        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        logger.info(f"ğŸ“Š å¹³è¡¡åæ•°æ®: {len(balanced_df)} æ¡è®°å½•")
        
        # éªŒè¯å¹³è¡¡ç»“æœ
        final_counts = balanced_df['category'].value_counts()
        logger.info("ğŸ“Š å¹³è¡¡ååˆ†å¸ƒ:")
        for category, count in final_counts.items():
            percentage = (count / len(balanced_df)) * 100
            logger.info(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
        
        return balanced_df
    
    def _smart_oversample(self, df: pd.DataFrame, target_count: int) -> pd.DataFrame:
        """æ™ºèƒ½ä¸Šé‡‡æ ·"""
        if len(df) == 0:
            return df
        
        # è®¡ç®—éœ€è¦é‡å¤çš„æ¬¡æ•°
        repeat_times = target_count // len(df)
        remainder = target_count % len(df)
        
        # é‡å¤é‡‡æ ·
        repeated_samples = []
        for _ in range(repeat_times):
            repeated_samples.append(df)
        
        # æ·»åŠ å‰©ä½™æ ·æœ¬
        if remainder > 0:
            remainder_samples = df.sample(n=remainder, random_state=42)
            repeated_samples.append(remainder_samples)
        
        # åˆå¹¶å¹¶æ‰“ä¹±
        oversampled = pd.concat(repeated_samples, ignore_index=True)
        oversampled = oversampled.sample(frac=1, random_state=42).reset_index(drop=True)
        
        return oversampled
    
    def _smart_undersample(self, df: pd.DataFrame, target_count: int) -> pd.DataFrame:
        """æ™ºèƒ½ä¸‹é‡‡æ ·"""
        if len(df) <= target_count:
            return df
        
        # åˆ†å±‚é‡‡æ ·
        undersampled = df.sample(n=target_count, random_state=42)
        return undersampled


class AdvancedDataQualityEnhancer:
    """é«˜çº§æ•°æ®è´¨é‡å¢å¼ºå™¨"""
    
    def __init__(self):
        self.extractor = AdvancedFeatureExtractor()
        self.balancer = SmartDataBalancer()
    
    def enhance_data_quality(self, df: pd.DataFrame, balance_data: bool = True) -> pd.DataFrame:
        """å¢å¼ºæ•°æ®è´¨é‡"""
        logger.info("ğŸš€ å¼€å§‹é«˜çº§æ•°æ®è´¨é‡å¢å¼º")
        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
        
        # 1. æ•°æ®æ¸…æ´—
        logger.info("ğŸ§¹ æ­¥éª¤1: æ•°æ®æ¸…æ´—")
        df_cleaned = self._clean_data(df)
        
        # 2. é«˜çº§ç‰¹å¾å·¥ç¨‹
        logger.info("ğŸ”§ æ­¥éª¤2: é«˜çº§ç‰¹å¾å·¥ç¨‹")
        df_enhanced = self._extract_advanced_features(df_cleaned)
        
        # 3. æ•°æ®å¹³è¡¡
        if balance_data:
            logger.info("âš–ï¸ æ­¥éª¤3: æ™ºèƒ½æ•°æ®å¹³è¡¡")
            df_balanced = self.balancer.balance_data(df_enhanced)
        else:
            df_balanced = df_enhanced
        
        # 4. è´¨é‡è¯„ä¼°
        logger.info("ğŸ“ˆ æ­¥éª¤4: è´¨é‡è¯„ä¼°")
        self._assess_quality(df_balanced)
        
        return df_balanced
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æ•°æ®"""
        # ç§»é™¤ç©ºå€¼
        df_cleaned = df.dropna(subset=['original_log', 'category'])
        
        # æ¸…æ´—æ—¥å¿—å†…å®¹
        df_cleaned['cleaned_log'] = df_cleaned['original_log'].apply(self._clean_log_content)
        
        # æå–æ—¥å¿—çº§åˆ«
        df_cleaned['log_level'] = df_cleaned['cleaned_log'].apply(self._extract_log_level)
        
        # è¿‡æ»¤æ— æ•ˆæ—¥å¿—
        df_cleaned = df_cleaned[df_cleaned['cleaned_log'].apply(self._is_valid_log)]
        
        logger.info(f"âœ… æ¸…æ´—åæ•°æ®: {len(df_cleaned)} æ¡è®°å½•")
        
        return df_cleaned
    
    def _clean_log_content(self, text: str) -> str:
        """æ¸…æ´—æ—¥å¿—å†…å®¹"""
        if pd.isna(text) or text == '':
            return ''
        
        text = str(text)
        
        # ç§»é™¤å…ƒæ•°æ®
        metadata_patterns = [
            r'github\.com/[^\s,]+',
            r'https://github\.com/[^\s,]+',
            r'github_issue',
            r'unknown,github_issue',
            r'[a-zA-Z0-9_-]+/[a-zA-Z0-9_-]+,\d+',
            r'https://github\.com/[^\s,]+/issues/\d+',
            r'[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]+',
            r'[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}',
            r'<[^>]+>',
            r'&[a-zA-Z]+;',
            r'\[.*?\]',
            r'^,+|,+$',
            r'^"+|"+$',
            r'unknown,,,',
            r',unknown,,,',
            r',unknown,',
            r'unknown,',
            r'https://',
        ]
        
        for pattern in metadata_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # æ ‡å‡†åŒ–
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # ç§»é™¤ä»¥é€—å·å¼€å¤´çš„éƒ¨åˆ†
        if text.startswith(','):
            text = text.lstrip(',')
        
        # ç§»é™¤ä»¥unknownå¼€å¤´çš„éƒ¨åˆ†
        if text.lower().startswith('unknown'):
            text = re.sub(r'^unknown[,\s]*', '', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def _extract_log_level(self, text: str) -> str:
        """æå–æ—¥å¿—çº§åˆ«"""
        log_levels = ['DEBUG', 'INFO', 'WARN', 'WARNING', 'ERROR', 'FATAL', 'TRACE']
        text_upper = text.upper()
        for level in log_levels:
            if level in text_upper:
                return level
        return 'UNKNOWN'
    
    def _is_valid_log(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆæ—¥å¿—"""
        if not text or len(text.strip()) < 10:
            return False
        
        log_indicators = ['error', 'warn', 'info', 'debug', 'exception', 'failed', 'success']
        text_lower = text.lower()
        
        return any(indicator in text_lower for indicator in log_indicators)
    
    def _extract_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æå–é«˜çº§ç‰¹å¾"""
        logger.info("ğŸ” æå–é«˜çº§ç»“æ„åŒ–ç‰¹å¾...")
        
        # æå–ç‰¹å¾
        features_list = []
        for idx, row in df.iterrows():
            features = self.extractor.extract_advanced_features(row['cleaned_log'])
            features_list.append(features)
        
        # è½¬æ¢ä¸ºDataFrame
        features_df = pd.DataFrame(features_list)
        
        # åˆå¹¶ç‰¹å¾
        df_enhanced = pd.concat([df.reset_index(drop=True), features_df], axis=1)
        
        # ç»Ÿè®¡ç‰¹å¾æå–ç»“æœ
        logger.info("ğŸ“Š é«˜çº§ç‰¹å¾æå–ç»Ÿè®¡:")
        for col in features_df.columns:
            non_empty = (features_df[col] != '').sum()
            percentage = (non_empty / len(features_df)) * 100
            logger.info(f"  {col}: {non_empty} æ¡ ({percentage:.1f}%)")
        
        return df_enhanced
    
    def _assess_quality(self, df: pd.DataFrame) -> None:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        logger.info("ğŸ“ˆ æ•°æ®è´¨é‡è¯„ä¼°:")
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        total_logs = len(df)
        valid_logs = len(df[df['cleaned_log'].str.len() > 10])
        logs_with_features = len(df[df['error_codes'] != ''])
        logs_with_paths = len(df[df['paths'] != ''])
        
        logger.info(f"  ğŸ“Š æ€»æ—¥å¿—æ•°: {total_logs}")
        logger.info(f"  âœ… æœ‰æ•ˆæ—¥å¿—æ•°: {valid_logs} ({valid_logs/total_logs*100:.1f}%)")
        logger.info(f"  ğŸ” åŒ…å«é”™è¯¯ç : {logs_with_features} ({logs_with_features/total_logs*100:.1f}%)")
        logger.info(f"  ğŸ“ åŒ…å«è·¯å¾„: {logs_with_paths} ({logs_with_paths/total_logs*100:.1f}%)")
        
        # ç±»åˆ«åˆ†å¸ƒ
        category_counts = df['category'].value_counts()
        logger.info("ğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in category_counts.items():
            percentage = (count / total_logs) * 100
            logger.info(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
    
    def create_advanced_enhanced_dataset(self, input_path: str, output_path: str, balance_data: bool = True) -> pd.DataFrame:
        """åˆ›å»ºé«˜çº§å¢å¼ºæ•°æ®é›†"""
        logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {input_path}")
        df = pd.read_csv(input_path)
        
        # å¢å¼ºæ•°æ®è´¨é‡
        df_enhanced = self.enhance_data_quality(df, balance_data)
        
        # ä¿å­˜ç»“æœ
        df_enhanced.to_csv(output_path, index=False)
        logger.info(f"ğŸ’¾ ä¿å­˜é«˜çº§å¢å¼ºæ•°æ®: {output_path}")
        
        return df_enhanced


def main():
    """ä¸»å‡½æ•°"""
    enhancer = AdvancedDataQualityEnhancer()
    
    input_path = "data/processed_logs_final_cleaned.csv"
    output_path = "data/processed_logs_advanced_enhanced.csv"
    
    # åˆ›å»ºé«˜çº§å¢å¼ºæ•°æ®é›†
    df_enhanced = enhancer.create_advanced_enhanced_dataset(input_path, output_path, balance_data=True)
    
    logger.info("âœ… é«˜çº§æ•°æ®è´¨é‡å¢å¼ºå®Œæˆ!")
    
    # æ˜¾ç¤ºå¢å¼ºåçš„æ•°æ®æ ·ä¾‹
    logger.info("ğŸ“‹ é«˜çº§å¢å¼ºæ•°æ®æ ·ä¾‹:")
    sample_cols = ['category', 'cleaned_log', 'log_level', 'error_codes', 'paths', 'methods']
    print(df_enhanced[sample_cols].head(3).to_string())


if __name__ == "__main__":
    main() 