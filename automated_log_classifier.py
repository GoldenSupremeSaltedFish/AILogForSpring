#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨åŒ–æ—¥å¿—åˆ†ç±»å™¨
æ•´åˆæ‰€æœ‰åˆ†ç±»è§„åˆ™å’Œå­˜å‚¨ç»“æ„ï¼Œæä¾›ç»Ÿä¸€çš„åˆ†ç±»æœåŠ¡
"""

import os
import sys
import json
import pandas as pd
import numpy as np
import re
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥æœºå™¨å­¦ä¹ åº“
try:
    import joblib
    import lightgbm as lgb
    from sklearn.feature_extraction.text import TfidfVectorizer
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸  æœºå™¨å­¦ä¹ åº“æœªå®‰è£…ï¼Œå°†ä»…ä½¿ç”¨è§„åˆ™åˆ†ç±»")

class AutomatedLogClassifier:
    """è‡ªåŠ¨åŒ–æ—¥å¿—åˆ†ç±»å™¨"""
    
    def __init__(self, config_file: str = None):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.config = self._load_config(config_file)
        self.classification_rules = self._init_classification_rules()
        self.model = None
        self.vectorizer = None
        self.label_encoder = None
        
        # æ•°æ®è·¯å¾„é…ç½®
        self.data_paths = {
            'raw': Path("DATA_OUTPUT/åŸå§‹é¡¹ç›®æ•°æ®_original"),
            'processed': Path("DATA_OUTPUT"),
            'models': Path("logsense-xpu/models"),
            'output': Path("log-processing-OUTPUT")
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.data_paths['output'].mkdir(exist_ok=True, parents=True)
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self._load_model()
    
    def _load_config(self, config_file: str = None) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if config_file and Path(config_file).exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # é»˜è®¤é…ç½®
        return {
            "classification_rules": {
                "priority_order": [
                    "stack_exception",
                    "spring_boot_startup_failure", 
                    "auth_authorization",
                    "database_exception",
                    "connection_issue",
                    "timeout",
                    "memory_performance",
                    "config_environment",
                    "business_logic",
                    "normal_operation",
                    "monitoring_heartbeat"
                ]
            },
            "quality_thresholds": {
                "min_classification_coverage": 80.0,
                "max_manual_annotation_ratio": 30.0,
                "confidence_threshold": 0.7
            }
        }
    
    def _init_classification_rules(self) -> Dict:
        """åˆå§‹åŒ–åˆ†ç±»è§„åˆ™"""
        return {
            'stack_exception': {
                'keywords': ['Exception', 'Error', 'at java.', 'at org.', 'at com.', 'Caused by', 'stack trace', 'NullPointerException', 'RuntimeException'],
                'patterns': [r'\w+Exception:', r'\w+Error:', r'at \w+\.\w+\.\w+', r'Caused by:'],
                'priority': 1,
                'description': 'å †æ ˆå¼‚å¸¸'
            },
            'spring_boot_startup_failure': {
                'keywords': ['APPLICATION FAILED TO START', 'SpringApplication', 'startup failed', 'bean creation', 'BeanCreationException'],
                'patterns': [r'APPLICATION FAILED TO START', r'Error creating bean', r'BeanCreationException'],
                'priority': 2,
                'description': 'Spring Bootå¯åŠ¨å¤±è´¥'
            },
            'auth_authorization': {
                'keywords': ['authentication', 'authorization', 'login', 'token', 'permission', 'access denied', 'unauthorized', '401', '403'],
                'patterns': [r'Authentication.*failed', r'Access.*denied', r'Unauthorized', r'401', r'403'],
                'priority': 3,
                'description': 'è®¤è¯æˆæƒ'
            },
            'database_exception': {
                'keywords': ['SQLException', 'database', 'DB', 'mysql', 'oracle', 'postgresql', 'jdbc', 'DataAccessException'],
                'patterns': [r'SQL.*Exception', r'Database.*error', r'JDBC.*error', r'DataAccessException'],
                'priority': 4,
                'description': 'æ•°æ®åº“å¼‚å¸¸'
            },
            'connection_issue': {
                'keywords': ['Connection', 'refused', 'timeout', 'unreachable', 'network', 'socket', 'ConnectException'],
                'patterns': [r'Connection.*refused', r'Connection.*timeout', r'Network.*unreachable', r'ConnectException'],
                'priority': 5,
                'description': 'è¿æ¥é—®é¢˜'
            },
            'timeout': {
                'keywords': ['timeout', 'timed out', 'TimeoutException', 'read timeout', 'connect timeout'],
                'patterns': [r'timeout.*exceeded', r'\d+ms.*timeout', r'TimeoutException'],
                'priority': 6,
                'description': 'è¶…æ—¶é”™è¯¯'
            },
            'memory_performance': {
                'keywords': ['OutOfMemoryError', 'memory', 'heap', 'GC', 'garbage collection', 'performance'],
                'patterns': [r'OutOfMemoryError', r'GC.*overhead', r'heap.*space'],
                'priority': 7,
                'description': 'å†…å­˜æ€§èƒ½'
            },
            'config_environment': {
                'keywords': ['configuration', 'property', 'environment', 'profile', 'yaml', 'properties'],
                'patterns': [r'Property.*not.*found', r'Configuration.*error'],
                'priority': 8,
                'description': 'é…ç½®ç¯å¢ƒ'
            },
            'business_logic': {
                'keywords': ['business', 'validation', 'rule', 'constraint', 'invalid'],
                'patterns': [r'Validation.*failed', r'Business.*rule', r'Constraint.*violation'],
                'priority': 9,
                'description': 'ä¸šåŠ¡é€»è¾‘'
            },
            'normal_operation': {
                'keywords': ['started', 'completed', 'success', 'finished', 'initialized', 'INFO'],
                'patterns': [r'Started.*in.*seconds', r'Completed.*successfully'],
                'priority': 10,
                'description': 'æ­£å¸¸æ“ä½œ'
            },
            'monitoring_heartbeat': {
                'keywords': ['health', 'heartbeat', 'ping', 'status', 'alive', 'actuator'],
                'patterns': [r'Health.*check', r'Heartbeat.*received'],
                'priority': 11,
                'description': 'ç›‘æ§å¿ƒè·³'
            }
        }
    
    def _load_model(self):
        """åŠ è½½æœºå™¨å­¦ä¹ æ¨¡å‹"""
        if not ML_AVAILABLE:
            return
        
        try:
            # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
            model_files = list(self.data_paths['models'].glob("lightgbm_model_*.txt"))
            if not model_files:
                print("âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œå°†ä»…ä½¿ç”¨è§„åˆ™åˆ†ç±»")
                return
            
            # è·å–æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
            latest_model = max(model_files, key=os.path.getctime)
            timestamp = latest_model.stem.split('_')[-1]
            
            # åŠ è½½æ¨¡å‹ç»„ä»¶
            model_file = self.data_paths['models'] / f"lightgbm_model_{timestamp}.txt"
            vectorizer_file = self.data_paths['models'] / f"tfidf_vectorizer_{timestamp}.joblib"
            encoder_file = self.data_paths['models'] / f"label_encoder_{timestamp}.joblib"
            
            if all(f.exists() for f in [model_file, vectorizer_file, encoder_file]):
                self.model = lgb.Booster(model_file=str(model_file))
                self.vectorizer = joblib.load(vectorizer_file)
                self.label_encoder = joblib.load(encoder_file)
                print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {timestamp}")
            else:
                print("âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´ï¼Œå°†ä»…ä½¿ç”¨è§„åˆ™åˆ†ç±»")
                
        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä»…ä½¿ç”¨è§„åˆ™åˆ†ç±»")
    
    def classify_log_level(self, log_line: str) -> str:
        """åˆ†ç±»æ—¥å¿—çº§åˆ«"""
        log_upper = log_line.upper()
        if 'ERROR' in log_upper or 'FATAL' in log_upper or 'SEVERE' in log_upper:
            return 'ERROR'
        elif 'WARN' in log_upper or 'WARNING' in log_upper:
            return 'WARN'
        elif 'INFO' in log_upper:
            return 'INFO'
        elif 'DEBUG' in log_upper or 'TRACE' in log_upper:
            return 'DEBUG'
        else:
            return 'UNKNOWN'
    
    def classify_by_rules(self, log_line: str) -> Tuple[str, float, str]:
        """ä½¿ç”¨è§„åˆ™åˆ†ç±»æ—¥å¿—"""
        log_lower = log_line.lower()
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºæ£€æŸ¥
        for category, rules in sorted(self.classification_rules.items(), 
                                    key=lambda x: x[1]['priority']):
            # æ£€æŸ¥å…³é”®è¯
            for keyword in rules['keywords']:
                if keyword.lower() in log_lower:
                    return category, 0.8, f"keyword: {keyword}"
            
            # æ£€æŸ¥æ­£åˆ™æ¨¡å¼
            for pattern in rules['patterns']:
                if re.search(pattern, log_line, re.IGNORECASE):
                    return category, 0.8, f"pattern: {pattern}"
        
        return 'other', 0.0, 'no match'
    
    def classify_by_ml(self, log_line: str) -> Tuple[str, float]:
        """ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹åˆ†ç±»"""
        if not self.model or not self.vectorizer or not self.label_encoder:
            return 'other', 0.0
        
        try:
            # ç‰¹å¾æå–
            features = self.vectorizer.transform([log_line])
            
            # é¢„æµ‹
            prediction = self.model.predict(features)
            probabilities = self.model.predict(features, pred_leaf=False)
            
            # è·å–æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«
            max_prob_idx = np.argmax(probabilities)
            confidence = probabilities[0][max_prob_idx]
            
            # è§£ç æ ‡ç­¾
            if hasattr(self.label_encoder, 'classes_'):
                predicted_class = self.label_encoder.classes_[max_prob_idx]
            else:
                predicted_class = str(max_prob_idx)
            
            return predicted_class, confidence
            
        except Exception as e:
            print(f"âš ï¸  MLåˆ†ç±»å¤±è´¥: {e}")
            return 'other', 0.0
    
    def classify_single_log(self, log_line: str, use_ml: bool = True) -> Dict:
        """åˆ†ç±»å•æ¡æ—¥å¿—"""
        # åŸºç¡€ä¿¡æ¯æå–
        log_level = self.classify_log_level(log_line)
        
        # è§„åˆ™åˆ†ç±»
        rule_category, rule_confidence, rule_reason = self.classify_by_rules(log_line)
        
        # æœºå™¨å­¦ä¹ åˆ†ç±»
        if use_ml and ML_AVAILABLE:
            ml_category, ml_confidence = self.classify_by_ml(log_line)
        else:
            ml_category, ml_confidence = 'other', 0.0
        
        # é€‰æ‹©æœ€ç»ˆåˆ†ç±»ç»“æœ
        if rule_confidence > 0.7:
            final_category = rule_category
            final_confidence = rule_confidence
            method = 'rules'
        elif ml_confidence > 0.5:
            final_category = ml_category
            final_confidence = ml_confidence
            method = 'ml'
        else:
            final_category = rule_category
            final_confidence = rule_confidence
            method = 'rules_fallback'
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦äººå·¥æ ‡æ³¨
        needs_manual = self._needs_manual_annotation(log_level, final_category, final_confidence)
        
        return {
            'original_log': log_line,
            'log_level': log_level,
            'category': final_category,
            'confidence': final_confidence,
            'method': method,
            'rule_reason': rule_reason,
            'needs_manual_annotation': needs_manual,
            'timestamp': datetime.now().isoformat()
        }
    
    def _needs_manual_annotation(self, log_level: str, category: str, confidence: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦äººå·¥æ ‡æ³¨"""
        # é«˜ä¼˜å…ˆçº§é—®é¢˜éœ€è¦äººå·¥æ ‡æ³¨
        if category in ['stack_exception', 'spring_boot_startup_failure', 'auth_authorization', 'database_exception']:
            return True
        
        # ERRORçº§åˆ«æ—¥å¿—éœ€è¦äººå·¥æ ‡æ³¨
        if log_level == 'ERROR':
            return True
        
        # ä½ç½®ä¿¡åº¦éœ€è¦äººå·¥æ ‡æ³¨
        if confidence < self.config['quality_thresholds']['confidence_threshold']:
            return True
        
        return False
    
    def classify_file(self, input_file: str, output_file: str = None, use_ml: bool = True) -> Dict:
        """åˆ†ç±»æ•´ä¸ªæ–‡ä»¶"""
        print(f"ğŸ”„ å¼€å§‹åˆ†ç±»æ–‡ä»¶: {Path(input_file).name}")
        
        try:
            # è¯»å–æ–‡ä»¶
            if input_file.endswith('.csv'):
                df = pd.read_csv(input_file, encoding='utf-8-sig')
                # å°è¯•ä¸åŒçš„åˆ—å
                log_column = None
                for col in ['original_log', 'message', 'content', 'text', 'log']:
                    if col in df.columns:
                        log_column = col
                        break
                
                if log_column is None:
                    print(f"âŒ æœªæ‰¾åˆ°æ—¥å¿—åˆ—ï¼Œå¯ç”¨åˆ—: {list(df.columns)}")
                    return {}
                
                log_lines = df[log_column].fillna('').astype(str).tolist()
            else:
                # çº¯æ–‡æœ¬æ–‡ä»¶
                with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:
                    log_lines = [line.strip() for line in f if line.strip()]
            
            print(f"ğŸ“Š åŠ è½½äº† {len(log_lines)} æ¡æ—¥å¿—")
            
            # åˆ†ç±»æ¯æ¡æ—¥å¿—
            results = []
            for i, log_line in enumerate(log_lines):
                if i % 1000 == 0:
                    print(f"  å¤„ç†è¿›åº¦: {i}/{len(log_lines)}")
                
                result = self.classify_single_log(log_line, use_ml)
                results.append(result)
            
            # ä¿å­˜ç»“æœ
            if output_file is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                input_name = Path(input_file).stem
                output_file = self.data_paths['output'] / f"{input_name}_classified_{timestamp}.csv"
            
            result_df = pd.DataFrame(results)
            result_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            stats = self._generate_classification_stats(results)
            self._save_classification_report(output_file, stats)
            
            print(f"âœ… åˆ†ç±»å®Œæˆ: {output_file}")
            return {
                'input_file': input_file,
                'output_file': str(output_file),
                'total_logs': len(log_lines),
                'stats': stats
            }
            
        except Exception as e:
            print(f"âŒ åˆ†ç±»å¤±è´¥: {e}")
            return {}
    
    def _generate_classification_stats(self, results: List[Dict]) -> Dict:
        """ç”Ÿæˆåˆ†ç±»ç»Ÿè®¡"""
        total_logs = len(results)
        category_counts = {}
        confidence_stats = []
        manual_needed = 0
        
        for result in results:
            category = result['category']
            confidence = result['confidence']
            
            category_counts[category] = category_counts.get(category, 0) + 1
            confidence_stats.append(confidence)
            
            if result['needs_manual_annotation']:
                manual_needed += 1
        
        return {
            'total_logs': total_logs,
            'category_distribution': category_counts,
            'avg_confidence': np.mean(confidence_stats),
            'manual_annotation_needed': manual_needed,
            'manual_annotation_ratio': (manual_needed / total_logs) * 100,
            'classification_coverage': ((total_logs - category_counts.get('other', 0)) / total_logs) * 100
        }
    
    def _save_classification_report(self, output_file: str, stats: Dict):
        """ä¿å­˜åˆ†ç±»æŠ¥å‘Š"""
        report_file = Path(output_file).with_suffix('.txt')
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("æ—¥å¿—åˆ†ç±»ç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ—¥å¿—æ•°: {stats['total_logs']}\n")
            f.write(f"å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.3f}\n")
            f.write(f"åˆ†ç±»è¦†ç›–ç‡: {stats['classification_coverage']:.1f}%\n")
            f.write(f"éœ€è¦äººå·¥æ ‡æ³¨: {stats['manual_annotation_needed']} æ¡ ({stats['manual_annotation_ratio']:.1f}%)\n\n")
            
            f.write("ç±»åˆ«åˆ†å¸ƒ:\n")
            f.write("-" * 30 + "\n")
            for category, count in sorted(stats['category_distribution'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['total_logs']) * 100
                description = self.classification_rules.get(category, {}).get('description', category)
                f.write(f"{description}: {count} æ¡ ({percentage:.1f}%)\n")
        
        print(f"ğŸ“„ ç»Ÿè®¡æŠ¥å‘Š: {report_file}")
    
    def batch_classify(self, input_dir: str, output_dir: str = None, use_ml: bool = True) -> Dict:
        """æ‰¹é‡åˆ†ç±»ç›®å½•ä¸­çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
        input_path = Path(input_dir)
        if not input_path.exists():
            print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return {}
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = self.data_paths['output'] / f"batch_classified_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(exist_ok=True, parents=True)
        
        # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
        log_extensions = ['.log', '.txt', '.csv', '.out', '.err']
        log_files = []
        
        for ext in log_extensions:
            log_files.extend(input_path.rglob(f"*{ext}"))
        
        if not log_files:
            print("âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
            return {}
        
        print(f"ğŸ“ æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        results = []
        for i, log_file in enumerate(log_files, 1):
            print(f"\n{'='*50}")
            print(f"å¤„ç†è¿›åº¦: {i}/{len(log_files)} - {log_file.name}")
            print('='*50)
            
            try:
                result = self.classify_file(str(log_file), str(output_dir / f"{log_file.stem}_classified.csv"), use_ml)
                if result:
                    results.append(result)
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
        
        # ç”Ÿæˆæ‰¹é‡å¤„ç†æŠ¥å‘Š
        self._save_batch_report(output_dir, results)
        
        success_count = len(results)
        print(f"\nğŸ‰ æ‰¹é‡åˆ†ç±»å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {success_count}/{len(log_files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        
        return {
            'total_files': len(log_files),
            'success_count': success_count,
            'results': results,
            'output_dir': str(output_dir)
        }
    
    def _save_batch_report(self, output_dir: Path, results: List[Dict]):
        """ä¿å­˜æ‰¹é‡å¤„ç†æŠ¥å‘Š"""
        report_file = output_dir / "batch_classification_report.txt"
        
        total_logs = sum(r.get('total_logs', 0) for r in results)
        total_categories = {}
        
        for result in results:
            stats = result.get('stats', {})
            for category, count in stats.get('category_distribution', {}).items():
                total_categories[category] = total_categories.get(category, 0) + count
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("æ‰¹é‡æ—¥å¿—åˆ†ç±»æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"å¤„ç†æ–‡ä»¶æ•°: {len(results)}\n")
            f.write(f"æ€»æ—¥å¿—æ•°: {total_logs}\n\n")
            
            f.write("æ€»ä½“ç±»åˆ«åˆ†å¸ƒ:\n")
            f.write("-" * 30 + "\n")
            for category, count in sorted(total_categories.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_logs) * 100 if total_logs > 0 else 0
                description = self.classification_rules.get(category, {}).get('description', category)
                f.write(f"{description}: {count} æ¡ ({percentage:.1f}%)\n")
        
        print(f"ğŸ“„ æ‰¹é‡å¤„ç†æŠ¥å‘Š: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨åŒ–æ—¥å¿—åˆ†ç±»å™¨')
    parser.add_argument('--input-file', help='è¾“å…¥æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--input-dir', help='è¾“å…¥ç›®å½•è·¯å¾„ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰')
    parser.add_argument('--output-file', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-ml', action='store_true', help='ä¸ä½¿ç”¨æœºå™¨å­¦ä¹ åˆ†ç±»')
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡å¤„ç†æ¨¡å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = AutomatedLogClassifier(args.config)
    
    use_ml = not args.no_ml
    
    if args.batch or args.input_dir:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        if not args.input_dir:
            print("âŒ æ‰¹é‡æ¨¡å¼éœ€è¦æŒ‡å®š --input-dir")
            return
        
        classifier.batch_classify(args.input_dir, args.output_dir, use_ml)
    
    elif args.input_file:
        # å•æ–‡ä»¶æ¨¡å¼
        if not Path(args.input_file).exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
            return
        
        classifier.classify_file(args.input_file, args.output_file, use_ml)
    
    else:
        print("âŒ è¯·æŒ‡å®š --input-file æˆ–ä½¿ç”¨ --batch --input-dir è¿›è¡Œæ‰¹é‡å¤„ç†")
        parser.print_help()

if __name__ == "__main__":
    main()
