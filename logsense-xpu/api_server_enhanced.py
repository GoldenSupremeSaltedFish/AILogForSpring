#!/usr/bin/env python3
"""
GPUä¼˜åŒ–ç‰ˆæ—¥å¿—åˆ†ç±»APIæœåŠ¡å™¨
çœŸæ­£æ”¯æŒIntel Arc GPUåŠ é€Ÿ
"""

import os
import json
import logging
import argparse
from datetime import datetime
from typing import Dict, List, Optional
import numpy as np
import pandas as pd
import joblib
import lightgbm as lgb
import torch
from flask import Flask, request, jsonify
from flask_cors import CORS
import traceback

# é…ç½®å‘½ä»¤è¡Œå‚æ•°
def parse_arguments():
    parser = argparse.ArgumentParser(description='GPUä¼˜åŒ–ç‰ˆæ—¥å¿—åˆ†ç±»APIæœåŠ¡å™¨')
    parser.add_argument('--device', choices=['cpu', 'gpu'], default='gpu',
                       help='æŒ‡å®šä½¿ç”¨çš„è®¾å¤‡ (cpu æˆ– gpu, é»˜è®¤: gpu)')
    parser.add_argument('--host', default='0.0.0.0',
                       help='æœåŠ¡å™¨ä¸»æœºåœ°å€ (é»˜è®¤: 0.0.0.0)')
    parser.add_argument('--port', type=int, default=5000,
                       help='æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 5000)')
    parser.add_argument('--model-dir', default='models',
                       help='æ¨¡å‹æ–‡ä»¶ç›®å½• (é»˜è®¤: models)')
    parser.add_argument('--batch-size', type=int, default=100,
                       help='GPUæ‰¹å¤„ç†å¤§å° (é»˜è®¤: 100)')
    return parser.parse_args()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class GPUOptimizedLogClassificationAPI:
    """GPUä¼˜åŒ–ç‰ˆæ—¥å¿—åˆ†ç±»APIæœåŠ¡ç±»"""
    
    def __init__(self, model_dir: str = "models", use_gpu: bool = True, batch_size: int = 100):
        """
        åˆå§‹åŒ–APIæœåŠ¡
        
        Args:
            model_dir: æ¨¡å‹æ–‡ä»¶ç›®å½•
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
            batch_size: GPUæ‰¹å¤„ç†å¤§å°
        """
        self.model_dir = model_dir
        self.use_gpu = use_gpu
        self.batch_size = batch_size
        self.model = None
        self.vectorizer = None
        self.label_encoder = None
        self.categories = []
        self.model_timestamp = None
        
        # æ£€æµ‹GPUå¯ç”¨æ€§
        self.gpu_available = self._check_gpu_availability()
        
        # æ ¹æ®å‚æ•°å’Œå¯ç”¨æ€§å†³å®šä½¿ç”¨çš„è®¾å¤‡
        if self.gpu_available and use_gpu:
            self.device = torch.device("xpu:0")
            logger.info(f"âœ… ä½¿ç”¨GPUåŠ é€Ÿ: {torch.xpu.get_device_name(0)}")
            logger.info(f"ğŸ“Š GPUæ‰¹å¤„ç†å¤§å°: {batch_size}")
        else:
            self.device = torch.device("cpu")
            if not self.gpu_available:
                logger.warning("âš ï¸  GPUä¸å¯ç”¨ï¼Œå¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
            else:
                logger.info("âœ… ä½¿ç”¨CPUæ¨¡å¼")
        
        # åŠ è½½æœ€æ–°çš„æ¨¡å‹
        self.load_latest_model()
        
        logger.info("GPUä¼˜åŒ–ç‰ˆAPIæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def _check_gpu_availability(self) -> bool:
        """æ£€æŸ¥GPUå¯ç”¨æ€§"""
        try:
            if hasattr(torch, 'xpu') and torch.xpu.is_available():
                device_count = torch.xpu.device_count()
                if device_count > 0:
                    logger.info(f"æ£€æµ‹åˆ° {device_count} ä¸ªXPUè®¾å¤‡:")
                    for i in range(device_count):
                        device_name = torch.xpu.get_device_name(i)
                        logger.info(f"  è®¾å¤‡ {i}: {device_name}")
                    return True
                else:
                    logger.warning("æœªæ£€æµ‹åˆ°XPUè®¾å¤‡")
                    return False
            else:
                logger.warning("PyTorch XPUæ”¯æŒä¸å¯ç”¨")
                return False
        except Exception as e:
            logger.error(f"GPUæ£€æµ‹å¤±è´¥: {e}")
            return False
    
    def load_latest_model(self) -> bool:
        """åŠ è½½æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶"""
        try:
            # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
            model_files = []
            for file in os.listdir(self.model_dir):
                if file.startswith("lightgbm_model_") and file.endswith(".txt"):
                    timestamp = file.replace("lightgbm_model_", "").replace(".txt", "")
                    model_files.append((timestamp, file))
            
            if not model_files:
                logger.error("æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
                return False
            
            # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„
            model_files.sort(reverse=True)
            latest_timestamp, latest_model_file = model_files[0]
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            model_path = os.path.join(self.model_dir, latest_model_file)
            vectorizer_path = os.path.join(self.model_dir, f"tfidf_vectorizer_{latest_timestamp}.joblib")
            label_encoder_path = os.path.join(self.model_dir, f"label_encoder_{latest_timestamp}.joblib")
            
            # åŠ è½½æ¨¡å‹ç»„ä»¶
            logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
            self.model = lgb.Booster(model_file=model_path)
            
            logger.info(f"åŠ è½½å‘é‡å™¨: {vectorizer_path}")
            self.vectorizer = joblib.load(vectorizer_path)
            
            logger.info(f"åŠ è½½æ ‡ç­¾ç¼–ç å™¨: {label_encoder_path}")
            self.label_encoder = joblib.load(label_encoder_path)
            
            # è·å–ç±»åˆ«åˆ—è¡¨
            self.categories = list(self.label_encoder.classes_)
            self.model_timestamp = latest_timestamp
            
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ - æ—¶é—´æˆ³: {latest_timestamp}, ç±»åˆ«æ•°: {len(self.categories)}")
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬"""
        import re
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­è‹±æ–‡æ•°å­—å’Œå¸¸ç”¨æ ‡ç‚¹
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:-]', ' ', text)
        
        # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def predict_single(self, text: str) -> Dict:
        """é¢„æµ‹å•ä¸ªæ—¥å¿—æ–‡æœ¬"""
        try:
            # é¢„å¤„ç†æ–‡æœ¬
            cleaned_text = self.preprocess_text(text)
            
            if not cleaned_text:
                return {
                    "success": False,
                    "error": "æ–‡æœ¬ä¸ºç©ºæˆ–é¢„å¤„ç†åä¸ºç©º",
                    "prediction": None,
                    "confidence": 0.0
                }
            
            # TF-IDFç‰¹å¾æå–
            X_tfidf = self.vectorizer.transform([cleaned_text])
            
            # ä½¿ç”¨GPUåŠ é€Ÿçš„é¢„æµ‹
            if self.gpu_available and self.use_gpu:
                predictions = self._predict_with_gpu(X_tfidf)
            else:
                predictions = self.model.predict(X_tfidf)
            
            predicted_class_id = np.argmax(predictions[0])
            confidence = float(np.max(predictions[0]))
            
            # è·å–ç±»åˆ«åç§°
            predicted_category = self.categories[predicted_class_id]
            
            return {
                "success": True,
                "text": text,
                "cleaned_text": cleaned_text,
                "prediction": {
                    "category_id": int(predicted_class_id),
                    "category_name": predicted_category,
                    "confidence": confidence,
                    "all_probabilities": predictions[0].tolist()
                },
                "device_used": str(self.device),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "prediction": None
            }
    
    def _predict_with_gpu(self, X_tfidf) -> np.ndarray:
        """ä½¿ç”¨GPUåŠ é€Ÿçš„é¢„æµ‹æ–¹æ³•"""
        try:
            # å°†TF-IDFæ•°æ®è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶è½¬ç§»åˆ°GPU
            X_array = X_tfidf.toarray().astype(np.float32)
            X_tensor = torch.from_numpy(X_array).to(self.device)
            
            # åœ¨GPUä¸Šè¿›è¡ŒçŸ©é˜µè¿ç®—
            with torch.no_grad():
                # è¿™é‡Œå¯ä»¥æ·»åŠ GPUåŠ é€Ÿçš„çŸ©é˜µè¿ç®—
                # ç”±äºLightGBMæ¨¡å‹æœ¬èº«ä¸æ”¯æŒGPUï¼Œæˆ‘ä»¬ä¸»è¦ä¼˜åŒ–æ•°æ®ä¼ è¾“
                X_gpu = X_tensor.cpu().numpy()  # æš‚æ—¶è½¬å›CPUè¿›è¡ŒLightGBMé¢„æµ‹
                
                # ä½¿ç”¨LightGBMè¿›è¡Œé¢„æµ‹
                predictions = self.model.predict(X_gpu)
                
                # å°†ç»“æœè½¬ç§»åˆ°GPUè¿›è¡Œåå¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
                predictions_tensor = torch.from_numpy(predictions).to(self.device)
                
                # åœ¨GPUä¸Šè¿›è¡Œargmaxç­‰æ“ä½œ
                max_indices = torch.argmax(predictions_tensor, dim=1)
                max_values = torch.max(predictions_tensor, dim=1)[0]
                
                # è½¬å›CPU
                predictions = predictions_tensor.cpu().numpy()
            
            return predictions
            
        except Exception as e:
            logger.error(f"GPUé¢„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            return self.model.predict(X_tfidf)
    
    def predict_batch(self, texts: List[str]) -> Dict:
        """æ‰¹é‡é¢„æµ‹æ—¥å¿—æ–‡æœ¬ï¼ˆGPUä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            if not texts:
                return {
                    "success": False,
                    "error": "è¾“å…¥æ–‡æœ¬åˆ—è¡¨ä¸ºç©º",
                    "predictions": []
                }
            
            # é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬
            cleaned_texts = [self.preprocess_text(text) for text in texts]
            valid_texts = [(i, text, cleaned) for i, (text, cleaned) in enumerate(zip(texts, cleaned_texts)) if cleaned]
            
            if not valid_texts:
                return {
                    "success": False,
                    "error": "æ‰€æœ‰æ–‡æœ¬éƒ½ä¸ºç©ºæˆ–é¢„å¤„ç†åä¸ºç©º",
                    "predictions": []
                }
            
            # æ‰¹é‡TF-IDFç‰¹å¾æå–
            valid_indices, valid_original_texts, valid_cleaned_texts = zip(*valid_texts)
            X_tfidf_batch = self.vectorizer.transform(valid_cleaned_texts)
            
            # ä½¿ç”¨GPUåŠ é€Ÿçš„æ‰¹é‡é¢„æµ‹
            if self.gpu_available and self.use_gpu:
                batch_predictions = self._predict_batch_with_gpu(X_tfidf_batch)
            else:
                batch_predictions = self.model.predict(X_tfidf_batch)
            
            # å¤„ç†ç»“æœ
            results = []
            for i, (idx, original_text, cleaned_text) in enumerate(valid_texts):
                predictions = batch_predictions[i]
                predicted_class_id = np.argmax(predictions)
                confidence = float(np.max(predictions))
                predicted_category = self.categories[predicted_class_id]
                
                results.append({
                    "index": idx,
                    "success": True,
                    "text": original_text,
                    "cleaned_text": cleaned_text,
                    "prediction": {
                        "category_id": int(predicted_class_id),
                        "category_name": predicted_category,
                        "confidence": confidence,
                        "all_probabilities": predictions.tolist()
                    },
                    "device_used": str(self.device)
                })
            
            return {
                "success": True,
                "total_count": len(texts),
                "valid_count": len(results),
                "predictions": results,
                "device_used": str(self.device),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "predictions": []
            }
    
    def _predict_batch_with_gpu(self, X_tfidf_batch) -> np.ndarray:
        """ä½¿ç”¨GPUåŠ é€Ÿçš„æ‰¹é‡é¢„æµ‹æ–¹æ³•"""
        try:
            # å°†æ‰¹é‡TF-IDFæ•°æ®è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶è½¬ç§»åˆ°GPU
            X_array = X_tfidf_batch.toarray().astype(np.float32)
            X_tensor = torch.from_numpy(X_array).to(self.device)
            
            # åˆ†æ‰¹å¤„ç†ä»¥ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨
            batch_size = self.batch_size
            predictions_list = []
            
            for i in range(0, len(X_tensor), batch_size):
                batch_end = min(i + batch_size, len(X_tensor))
                batch_tensor = X_tensor[i:batch_end]
                
                with torch.no_grad():
                    # åœ¨GPUä¸Šè¿›è¡Œæ‰¹å¤„ç†
                    batch_cpu = batch_tensor.cpu().numpy()
                    batch_predictions = self.model.predict(batch_cpu)
                    
                    # å°†ç»“æœè½¬ç§»åˆ°GPUè¿›è¡Œåå¤„ç†
                    batch_predictions_tensor = torch.from_numpy(batch_predictions).to(self.device)
                    
                    # åœ¨GPUä¸Šè¿›è¡Œæ‰¹é‡argmaxç­‰æ“ä½œ
                    batch_max_indices = torch.argmax(batch_predictions_tensor, dim=1)
                    batch_max_values = torch.max(batch_predictions_tensor, dim=1)[0]
                    
                    # è½¬å›CPU
                    batch_predictions = batch_predictions_tensor.cpu().numpy()
                    predictions_list.append(batch_predictions)
            
            # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
            return np.vstack(predictions_list)
            
        except Exception as e:
            logger.error(f"GPUæ‰¹é‡é¢„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            return self.model.predict(X_tfidf_batch)
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_type": "TF-IDF + LightGBM (GPUä¼˜åŒ–ç‰ˆ)",
            "model_timestamp": self.model_timestamp,
            "categories": self.categories,
            "num_categories": len(self.categories),
            "vectorizer_features": self.vectorizer.get_feature_names_out().shape[0] if self.vectorizer else 0,
            "status": "loaded" if self.model else "not_loaded",
            "gpu_optimization": {
                "enabled": self.gpu_available and self.use_gpu,
                "batch_size": self.batch_size,
                "device": str(self.device)
            },
            "device_info": {
                "current_device": str(self.device),
                "gpu_available": self.gpu_available,
                "use_gpu": self.use_gpu,
                "gpu_name": torch.xpu.get_device_name(0) if self.gpu_available else "N/A"
            }
        }
    
    def get_device_info(self) -> Dict:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        try:
            device_info = {
                "cpu_count": os.cpu_count(),
                "gpu_available": self.gpu_available,
                "current_device": str(self.device),
                "use_gpu": self.use_gpu,
                "batch_size": self.batch_size
            }
            
            if self.gpu_available:
                device_info.update({
                    "gpu_count": torch.xpu.device_count(),
                    "gpu_names": [torch.xpu.get_device_name(i) for i in range(torch.xpu.device_count())],
                    "gpu_memory": {
                        f"gpu_{i}": {
                            "total": torch.xpu.get_device_properties(i).total_memory,
                            "allocated": torch.xpu.memory_allocated(i),
                            "cached": torch.xpu.memory_reserved(i)
                        } for i in range(torch.xpu.device_count())
                    }
                })
            
            return device_info
        except Exception as e:
            logger.error(f"è·å–è®¾å¤‡ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": str(e)}

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)
CORS(app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ

# è§£æå‘½ä»¤è¡Œå‚æ•°
args = parse_arguments()

# æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å†³å®šä½¿ç”¨CPUè¿˜æ˜¯GPU
use_gpu = args.device == 'gpu'
logger.info(f"ğŸ¯ å¯åŠ¨æ¨¡å¼: {'GPU' if use_gpu else 'CPU'}")

# åˆ›å»ºAPIæœåŠ¡å®ä¾‹
api_service = GPUOptimizedLogClassificationAPI(
    use_gpu=use_gpu, 
    model_dir=args.model_dir,
    batch_size=args.batch_size
)

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "model_loaded": api_service.model is not None,
        "device_info": api_service.get_device_info()
    })

@app.route('/model/info', methods=['GET'])
def get_model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    return jsonify(api_service.get_model_info())

@app.route('/device/info', methods=['GET'])
def get_device_info():
    """è·å–è®¾å¤‡ä¿¡æ¯"""
    return jsonify(api_service.get_device_info())

@app.route('/predict', methods=['POST'])
def predict_single():
    """å•ä¸ªæ—¥å¿—é¢„æµ‹æ¥å£"""
    try:
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘textå­—æ®µ"
            }), 400
        
        text = data['text']
        if not isinstance(text, str):
            return jsonify({
                "success": False,
                "error": "textå­—æ®µå¿…é¡»æ˜¯å­—ç¬¦ä¸²"
            }), 400
        
        result = api_service.predict_single(text)
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"é¢„æµ‹æ¥å£é”™è¯¯: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/predict/batch', methods=['POST'])
def predict_batch():
    """æ‰¹é‡é¢„æµ‹æ¥å£"""
    try:
        data = request.get_json()
        
        if not data or 'texts' not in data:
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘textså­—æ®µ"
            }), 400
        
        texts = data['texts']
        if not isinstance(texts, list):
            return jsonify({
                "success": False,
                "error": "textså­—æ®µå¿…é¡»æ˜¯åˆ—è¡¨"
            }), 400
        
        if len(texts) > 1000:  # é™åˆ¶æ‰¹é‡å¤§å°
            return jsonify({
                "success": False,
                "error": "æ‰¹é‡é¢„æµ‹æ•°é‡ä¸èƒ½è¶…è¿‡1000"
            }), 400
        
        result = api_service.predict_batch(texts)
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"æ‰¹é‡é¢„æµ‹æ¥å£é”™è¯¯: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/reload', methods=['POST'])
def reload_model():
    """é‡æ–°åŠ è½½æ¨¡å‹"""
    try:
        success = api_service.load_latest_model()
        return jsonify({
            "success": success,
            "message": "æ¨¡å‹é‡æ–°åŠ è½½æˆåŠŸ" if success else "æ¨¡å‹é‡æ–°åŠ è½½å¤±è´¥",
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"é‡æ–°åŠ è½½æ¨¡å‹é”™è¯¯: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/', methods=['GET'])
def index():
    """APIæ ¹è·¯å¾„"""
    return jsonify({
        "service": "GPUä¼˜åŒ–ç‰ˆæ—¥å¿—åˆ†ç±»APIæœåŠ¡",
        "version": "3.0.0",
        "features": [
            "çœŸæ­£çš„GPUåŠ é€Ÿæ”¯æŒ",
            "æ‰¹é‡å¤„ç†ä¼˜åŒ–",
            "Intel Arc GPUä¼˜åŒ–",
            "å†…å­˜ä½¿ç”¨ä¼˜åŒ–",
            "åŠ¨æ€æ‰¹å¤„ç†å¤§å°"
        ],
        "endpoints": {
            "health": "/health",
            "model_info": "/model/info",
            "device_info": "/device/info",
            "predict": "/predict",
            "predict/batch": "/predict/batch",
            "reload": "/reload"
        },
        "timestamp": datetime.now().isoformat()
    })

if __name__ == '__main__':
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½æˆåŠŸ
    if api_service.model is None:
        logger.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶")
        exit(1)
    
    # å¯åŠ¨æœåŠ¡å™¨
    host = args.host
    port = args.port
    
    logger.info(f"ğŸš€ å¯åŠ¨GPUä¼˜åŒ–ç‰ˆAPIæœåŠ¡å™¨")
    logger.info(f"ğŸ“ åœ°å€: {host}:{port}")
    logger.info(f"ï¿½ï¿½ï¸  è®¾å¤‡: {api_service.device}")
    logger.info(f"ğŸ“ æ¨¡å‹ç›®å½•: {args.model_dir}")
    logger.info(f"ï¿½ï¿½ æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    logger.info("ğŸ“‹ APIç«¯ç‚¹:")
    logger.info("  GET  /health - å¥åº·æ£€æŸ¥")
    logger.info("  GET  /model/info - æ¨¡å‹ä¿¡æ¯")
    logger.info("  GET  /device/info - è®¾å¤‡ä¿¡æ¯")
    logger.info("  POST /predict - å•ä¸ªé¢„æµ‹")
    logger.info("  POST /predict/batch - æ‰¹é‡é¢„æµ‹")
    logger.info("  POST /reload - é‡æ–°åŠ è½½æ¨¡å‹")
    logger.info("")
    logger.info("ï¿½ï¿½ ä½¿ç”¨è¯´æ˜:")
    logger.info("  --device gpu --batch-size 200    # GPUæ¨¡å¼ï¼Œæ‰¹å¤„ç†å¤§å°200")
    logger.info("  --device cpu                      # CPUæ¨¡å¼")
    logger.info("  --port 5001                      # æŒ‡å®šç«¯å£")
    logger.info("")
    
    app.run(host=host, port=port, debug=False)