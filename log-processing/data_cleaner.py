#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ¸…æ´—è„šæœ¬
åŠŸèƒ½ï¼š
1. è¯»å–DATA_OUTPUTç›®å½•ä¸­çš„åˆ†ç±»æ•°æ®æ–‡ä»¶
2. å»é™¤'other'ç±»åˆ«æ•°æ®
3. å¹³è¡¡å„ç±»åˆ«æ•°æ®é‡ï¼ˆé¿å…æ•°æ®åæ–œï¼‰
4. ç”Ÿæˆç”¨äºæ¨¡å‹è®­ç»ƒçš„æ•°æ®é›†
5. æ”¯æŒå¤šç§æ•°æ®æ ¼å¼å’Œæ¥æº

ä½¿ç”¨æ–¹æ³•:
python data_cleaner.py                           # å¤„ç†DATA_OUTPUTç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶
python data_cleaner.py <è¾“å…¥æ–‡ä»¶è·¯å¾„>            # å¤„ç†æŒ‡å®šæ–‡ä»¶
python data_cleaner.py --max-per-class 500      # è®¾ç½®æ¯ç±»æœ€å¤šä¿ç•™æ¡æ•°
python data_cleaner.py --output-dir <è¾“å‡ºç›®å½•>   # æŒ‡å®šè¾“å‡ºç›®å½•
"""

import pandas as pd
import numpy as np
import argparse
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')


class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self):
        # å®šä¹‰ç±»åˆ«æ˜ å°„ï¼ˆç”¨äºç»Ÿä¸€ç±»åˆ«åç§°ï¼‰
        self.category_mapping = {
            'stack_exception': 'stack_exception',
            'spring_boot_startup_failure': 'startup_failure',
            'auth_authorization': 'auth_error',
            'database_exception': 'db_error',
            'connection_issue': 'connection_issue',
            'timeout': 'timeout',
            'memory_performance': 'performance',
            'config_environment': 'config',
            'business_logic': 'business',
            'normal_operation': 'normal',
            'monitoring_heartbeat': 'heartbeat',
            'other': 'other'  # å°†è¢«è¿‡æ»¤
        }
        
        # ç±»åˆ«ä¸­æ–‡æè¿°
        self.category_descriptions = {
            'stack_exception': 'å †æ ˆå¼‚å¸¸',
            'startup_failure': 'å¯åŠ¨å¤±è´¥',
            'auth_error': 'è®¤è¯é”™è¯¯',
            'db_error': 'æ•°æ®åº“é”™è¯¯',
            'connection_issue': 'è¿æ¥é—®é¢˜',
            'timeout': 'è¶…æ—¶é”™è¯¯',
            'performance': 'æ€§èƒ½é—®é¢˜',
            'config': 'é…ç½®é—®é¢˜',
            'business': 'ä¸šåŠ¡é€»è¾‘',
            'normal': 'æ­£å¸¸æ“ä½œ',
            'heartbeat': 'ç›‘æ§å¿ƒè·³'
        }
    
    def load_data(self, input_file: str) -> pd.DataFrame:
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        try:
            df = pd.read_csv(input_file)
            print(f"ğŸ“Š åŠ è½½æ•°æ®: {len(df)} æ¡è®°å½•")
            return df
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def detect_label_column(self, df: pd.DataFrame) -> str:
        """æ£€æµ‹æ ‡ç­¾åˆ—å"""
        possible_labels = ['content_type', 'final_label', 'label', 'category']
        for col in possible_labels:
            if col in df.columns:
                return col
        return None
    
    def clean_data(self, df: pd.DataFrame, max_per_class: int = None) -> pd.DataFrame:
        """æ¸…æ´—æ•°æ®"""
        if df.empty:
            return df
        
        # æ£€æµ‹æ ‡ç­¾åˆ—
        label_column = self.detect_label_column(df)
        if not label_column:
            print("âŒ æœªæ‰¾åˆ°æ ‡ç­¾åˆ—")
            return pd.DataFrame()
        
        print(f"ğŸ” ä½¿ç”¨æ ‡ç­¾åˆ—: {label_column}")
        
        # è¿‡æ»¤æ‰'other'ç±»åˆ«
        original_count = len(df)
        df_cleaned = df[df[label_column] != 'other'].copy()
        filtered_count = len(df_cleaned)
        
        print(f"ğŸ” è¿‡æ»¤åæ•°æ®: {filtered_count} æ¡è®°å½• (ç§»é™¤äº† {original_count - filtered_count} æ¡'other'è®°å½•)")
        
        # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
        category_counts = df_cleaned[label_column].value_counts()
        print("\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in category_counts.items():
            percentage = (count / filtered_count) * 100
            print(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
        
        # å¹³è¡¡æ•°æ®ï¼ˆå¦‚æœæŒ‡å®šäº†max_per_classï¼‰
        if max_per_class and max_per_class > 0:
            df_balanced = df_cleaned.groupby(label_column).head(max_per_class).copy()
            balanced_count = len(df_balanced)
            
            print(f"\nâš–ï¸ å¹³è¡¡åæ•°æ®: {balanced_count} æ¡è®°å½• (æ¯ç±»æœ€å¤š {max_per_class} æ¡)")
            
            # é‡æ–°ç»Ÿè®¡
            balanced_counts = df_balanced[label_column].value_counts()
            print("\nğŸ“ˆ å¹³è¡¡åç±»åˆ«åˆ†å¸ƒ:")
            for category, count in balanced_counts.items():
                percentage = (count / balanced_count) * 100
                print(f"  {category}: {count} æ¡ ({percentage:.1f}%)")
            
            return df_balanced
        
        return df_cleaned
    
    def prepare_training_data(self, df: pd.DataFrame, output_file: str) -> bool:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        try:
            # æ£€æµ‹å¿…è¦çš„åˆ—
            label_column = self.detect_label_column(df)
            text_column = None
            
            # æ£€æµ‹æ–‡æœ¬åˆ—
            possible_text_columns = ['original_log', 'message', 'content', 'text']
            for col in possible_text_columns:
                if col in df.columns:
                    text_column = col
                    break
            
            if not text_column:
                print("âŒ æœªæ‰¾åˆ°æ–‡æœ¬åˆ—")
                return False
            
            # é€‰æ‹©ç”¨äºè®­ç»ƒçš„åˆ—
            training_columns = [text_column, label_column]
            
            # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„åˆ—éƒ½å­˜åœ¨
            missing_columns = [col for col in training_columns if col not in df.columns]
            if missing_columns:
                print(f"âŒ ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
                return False
            
            # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
            training_df = df[training_columns].copy()
            
            # é‡å‘½ååˆ—ä»¥ä¾¿æ¨¡å‹è®­ç»ƒ
            training_df.columns = ['text', 'label']
            
            # ä¿å­˜è®­ç»ƒæ•°æ®
            training_df.to_csv(output_file, index=False, encoding='utf-8')
            print(f"ğŸ’¾ å·²ä¿å­˜è®­ç»ƒæ•°æ®åˆ°: {output_file}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å‡†å¤‡è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            return False
    
    def process_single_file(self, input_file: str, output_dir: Path, 
                          max_per_class: int = None) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        print(f"\nğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file}")
        print("-" * 60)
        
        # åŠ è½½æ•°æ®
        df = self.load_data(input_file)
        if df.empty:
            return False
        
        # æ¸…æ´—æ•°æ®
        df_cleaned = self.clean_data(df, max_per_class)
        if df_cleaned.empty:
            return False
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        input_path = Path(input_file)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        cleaned_file = output_dir / f"{input_path.stem}_cleaned_{timestamp}.csv"
        training_file = output_dir / f"{input_path.stem}_training_{timestamp}.csv"
        
        # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        df_cleaned.to_csv(cleaned_file, index=False, encoding='utf-8')
        print(f"ğŸ’¾ å·²ä¿å­˜æ¸…æ´—æ•°æ®åˆ°: {cleaned_file}")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        success = self.prepare_training_data(df_cleaned, str(training_file))
        
        return success
    
    def find_data_files(self) -> List[Path]:
        """æŸ¥æ‰¾DATA_OUTPUTç›®å½•ä¸‹çš„æ•°æ®æ–‡ä»¶"""
        data_dir = Path("DATA_OUTPUT")
        if not data_dir.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return []
        
        # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = list(data_dir.glob("*.csv"))
        return csv_files
    
    def batch_process(self, output_dir: Path = None, max_per_class: int = None):
        """æ‰¹é‡å¤„ç†æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        if output_dir is None:
            output_dir = Path("DATA_OUTPUT")
        
        data_files = self.find_data_files()
        if not data_files:
            print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        print("=" * 60)
        
        success_count = 0
        total_records = 0
        
        for data_file in data_files:
            try:
                if self.process_single_file(str(data_file), output_dir, max_per_class):
                    success_count += 1
                    # ç»Ÿè®¡è®°å½•æ•°
                    df = pd.read_csv(data_file)
                    label_column = self.detect_label_column(df)
                    if label_column:
                        total_records += len(df[df[label_column] != 'other'])
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {data_file} æ—¶å‡ºé”™: {e}")
        
        print("\n" + "=" * 60)
        print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: {success_count}/{len(data_files)} ä¸ªæ–‡ä»¶å¤„ç†æˆåŠŸ")
        print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {total_records} æ¡æœ‰æ•ˆè®°å½•")
    
    def create_combined_dataset(self, output_dir: Path, max_per_class: int = None):
        """åˆ›å»ºåˆå¹¶çš„æ•°æ®é›†"""
        print("\nğŸ”„ åˆ›å»ºåˆå¹¶æ•°æ®é›†...")
        
        data_files = self.find_data_files()
        if not data_files:
            print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
            return
        
        all_data = []
        
        for data_file in data_files:
            try:
                df = self.load_data(str(data_file))
                if not df.empty:
                    label_column = self.detect_label_column(df)
                    if label_column:
                        # è¿‡æ»¤otherç±»åˆ«
                        df_cleaned = df[df[label_column] != 'other'].copy()
                        all_data.append(df_cleaned)
                        print(f"  âœ“ {data_file.name}: {len(df_cleaned)} æ¡è®°å½•")
            except Exception as e:
                print(f"  âŒ {data_file.name}: {e}")
        
        if not all_data:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            return
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        print(f"\nğŸ“Š åˆå¹¶åæ€»æ•°æ®: {len(combined_df)} æ¡è®°å½•")
        
        # å¹³è¡¡æ•°æ®
        if max_per_class and max_per_class > 0:
            label_column = self.detect_label_column(combined_df)
            combined_df = combined_df.groupby(label_column).head(max_per_class).copy()
            print(f"âš–ï¸ å¹³è¡¡åæ•°æ®: {len(combined_df)} æ¡è®°å½•")
        
        # ä¿å­˜åˆå¹¶æ•°æ®é›†
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        combined_file = output_dir / f"combined_dataset_{timestamp}.csv"
        training_file = output_dir / f"training_dataset_{timestamp}.csv"
        
        combined_df.to_csv(combined_file, index=False, encoding='utf-8')
        print(f"ğŸ’¾ å·²ä¿å­˜åˆå¹¶æ•°æ®é›†åˆ°: {combined_file}")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        self.prepare_training_data(combined_df, str(training_file))
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        label_column = self.detect_label_column(combined_df)
        if label_column:
            category_counts = combined_df[label_column].value_counts()
            print("\nğŸ“ˆ æœ€ç»ˆç±»åˆ«åˆ†å¸ƒ:")
            for category, count in category_counts.items():
                percentage = (count / len(combined_df)) * 100
                print(f"  {category}: {count} æ¡ ({percentage:.1f}%)")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®æ¸…æ´—è„šæœ¬")
    parser.add_argument("input_file", nargs="?", help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output-dir", help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--max-per-class", type=int, default=500, 
                       help="æ¯ç±»æœ€å¤šä¿ç•™æ¡æ•° (é»˜è®¤: 500)")
    parser.add_argument("--combined", action="store_true", 
                       help="åˆ›å»ºåˆå¹¶çš„æ•°æ®é›†")
    
    args = parser.parse_args()
    
    cleaner = DataCleaner()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = Path("DATA_OUTPUT")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(exist_ok=True)
    
    if args.combined:
        # åˆ›å»ºåˆå¹¶æ•°æ®é›†
        cleaner.create_combined_dataset(output_dir, args.max_per_class)
    elif args.input_file:
        # å¤„ç†æŒ‡å®šæ–‡ä»¶
        success = cleaner.process_single_file(
            args.input_file, 
            output_dir, 
            args.max_per_class
        )
        sys.exit(0 if success else 1)
    else:
        # æ‰¹é‡å¤„ç†
        cleaner.batch_process(output_dir, args.max_per_class)


if __name__ == "__main__":
    main() 