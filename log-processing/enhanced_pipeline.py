#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„æ—¥å¿—åŠè‡ªåŠ¨åˆ†ç±»æµæ°´çº¿
æ•´åˆæ¨¡æ¿åŒ–ã€ç‰¹å¾å·¥ç¨‹ã€æœºå™¨å­¦ä¹ åˆ†ç±»å’Œäººå·¥å®¡æŸ¥çš„å®Œæ•´æµç¨‹

ä½¿ç”¨æ–¹æ³•:
python enhanced_pipeline.py --input-file logs.csv --mode full
python enhanced_pipeline.py --input-dir logs/ --mode batch --skip-human-review
python enhanced_pipeline.py --input-file logs.csv --mode template-only
"""

import pandas as pd
import numpy as np
import json
import argparse
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from log_templater import LogTemplater
from feature_engineer import FeatureEngineer
from enhanced_pre_classifier import EnhancedPreClassifier
from auto_labeler import LogAutoLabeler
from log_reviewer import LogReviewer
from quality_analyzer import QualityAnalyzer

class EnhancedLogPipeline:
    """å¢å¼ºçš„æ—¥å¿—åŠè‡ªåŠ¨åˆ†ç±»æµæ°´çº¿"""
    
    def __init__(self):
        self.pipeline_config = {
            'enable_templating': True,
            'enable_feature_engineering': True,
            'enable_ml_classification': True,
            'enable_human_review': True,
            'enable_quality_analysis': True,
            'max_per_class': 500,
            'confidence_threshold': 0.7
        }
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.templater = LogTemplater()
        self.feature_engineer = FeatureEngineer()
        self.pre_classifier = EnhancedPreClassifier()
        self.auto_labeler = LogAutoLabeler()
        self.quality_analyzer = QualityAnalyzer()
        
        # è¾“å‡ºç›®å½•é…ç½®
        self.output_base_dir = Path(__file__).parent.parent / "log-processing-OUTPUT"
        
        # æµæ°´çº¿çŠ¶æ€
        self.pipeline_state = {
            'current_step': 0,
            'total_steps': 0,
            'results': {},
            'errors': []
        }
    
    def configure_pipeline(self, config: Dict):
        """é…ç½®æµæ°´çº¿å‚æ•°"""
        self.pipeline_config.update(config)
        print(f"ğŸ”§ æµæ°´çº¿é…ç½®æ›´æ–°: {config}")
    
    def step_1_template_logs(self, input_file: str, output_dir: Path) -> Dict:
        """æ­¥éª¤1: æ—¥å¿—æ¨¡æ¿åŒ–"""
        print("\n" + "="*60)
        print("ğŸ“‹ æ­¥éª¤1: æ—¥å¿—æ¨¡æ¿åŒ–")
        print("="*60)
        
        try:
            result = self.templater.process_file(input_file, output_dir)
            if result:
                self.pipeline_state['results']['templating'] = result
                print(f"âœ… æ¨¡æ¿åŒ–å®Œæˆ: {result['total_templates']} ä¸ªæ¨¡æ¿")
                return result
            else:
                raise Exception("æ¨¡æ¿åŒ–å¤„ç†å¤±è´¥")
                
        except Exception as e:
            error_msg = f"æ¨¡æ¿åŒ–æ­¥éª¤å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            self.pipeline_state['errors'].append(error_msg)
            return {}
    
    def step_2_extract_features(self, templated_file: str, output_dir: Path) -> Dict:
        """æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹"""
        print("\n" + "="*60)
        print("ğŸ”§ æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹")
        print("="*60)
        
        try:
            result = self.feature_engineer.process_file(templated_file, output_dir)
            if result:
                self.pipeline_state['results']['feature_engineering'] = result
                print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {result['feature_count']} ä¸ªç‰¹å¾")
                return result
            else:
                raise Exception("ç‰¹å¾å·¥ç¨‹å¤„ç†å¤±è´¥")
                
        except Exception as e:
            error_msg = f"ç‰¹å¾å·¥ç¨‹æ­¥éª¤å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            self.pipeline_state['errors'].append(error_msg)
            return {}
    
    def step_3_pre_classify(self, input_file: str, output_dir: Path) -> Dict:
        """æ­¥éª¤3: é¢„åˆ†ç±»"""
        print("\n" + "="*60)
        print("ğŸ·ï¸ æ­¥éª¤3: é¢„åˆ†ç±»")
        print("="*60)
        
        try:
            result = self.pre_classifier.process_log_file(input_file, str(output_dir))
            if result:
                self.pipeline_state['results']['pre_classification'] = result
                print(f"âœ… é¢„åˆ†ç±»å®Œæˆ: {result['classified_lines']} æ¡å·²åˆ†ç±»")
                return result
            else:
                raise Exception("é¢„åˆ†ç±»å¤„ç†å¤±è´¥")
                
        except Exception as e:
            error_msg = f"é¢„åˆ†ç±»æ­¥éª¤å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            self.pipeline_state['errors'].append(error_msg)
            return {}
    
    def step_4_auto_label(self, input_file: str, output_dir: Path) -> Dict:
        """æ­¥éª¤4: è‡ªåŠ¨æ ‡ç­¾"""
        print("\n" + "="*60)
        print("ğŸ¤– æ­¥éª¤4: è‡ªåŠ¨æ ‡ç­¾")
        print("="*60)
        
        try:
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            temp_output_dir = output_dir / "temp_auto_label"
            temp_output_dir.mkdir(exist_ok=True)
            
            success = self.auto_labeler.process_single_file(input_file, temp_output_dir, use_ml=True)
            if success:
                # æŸ¥æ‰¾ç”Ÿæˆçš„æ–‡ä»¶
                labeled_files = list(temp_output_dir.glob("*_labeled_*.csv"))
                if labeled_files:
                    result = {
                        'output_file': str(labeled_files[0]),
                        'success': True
                    }
                    self.pipeline_state['results']['auto_labeling'] = result
                    print(f"âœ… è‡ªåŠ¨æ ‡ç­¾å®Œæˆ: {labeled_files[0].name}")
                    return result
            
            raise Exception("è‡ªåŠ¨æ ‡ç­¾å¤„ç†å¤±è´¥")
                
        except Exception as e:
            error_msg = f"è‡ªåŠ¨æ ‡ç­¾æ­¥éª¤å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            self.pipeline_state['errors'].append(error_msg)
            return {}
    
    def step_5_human_review(self, labeled_file: str, output_dir: Path) -> Dict:
        """æ­¥éª¤5: äººå·¥å®¡æŸ¥"""
        print("\n" + "="*60)
        print("ğŸ‘¥ æ­¥éª¤5: äººå·¥å®¡æŸ¥")
        print("="*60)
        
        if not self.pipeline_config['enable_human_review']:
            print("â­ï¸ è·³è¿‡äººå·¥å®¡æŸ¥æ­¥éª¤")
            return {'skipped': True}
        
        try:
            print("ğŸ” å¯åŠ¨äººå·¥å®¡æŸ¥å·¥å…·...")
            print("ğŸ’¡ æç¤º: åœ¨å®¡æŸ¥å·¥å…·ä¸­æŒ‰ 'q' å¯ä»¥é€€å‡ºå¹¶ä¿å­˜è¿›åº¦")
            
            # åˆ›å»ºå®¡æŸ¥å™¨
            reviewer = LogReviewer(labeled_file, str(output_dir))
            
            # è¿è¡Œå®¡æŸ¥
            reviewer.run()
            
            result = {
                'output_file': str(reviewer.output_file),
                'stats': reviewer.stats,
                'success': True
            }
            
            self.pipeline_state['results']['human_review'] = result
            print(f"âœ… äººå·¥å®¡æŸ¥å®Œæˆ")
            return result
            
        except Exception as e:
            error_msg = f"äººå·¥å®¡æŸ¥æ­¥éª¤å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            self.pipeline_state['errors'].append(error_msg)
            return {}
    
    def step_6_quality_analysis(self, final_file: str, output_dir: Path) -> Dict:
        """æ­¥éª¤6: è´¨é‡åˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ“Š æ­¥éª¤6: è´¨é‡åˆ†æ")
        print("="*60)
        
        try:
            result = self.quality_analyzer.generate_report(final_file, str(output_dir))
            if result:
                self.pipeline_state['results']['quality_analysis'] = result
                print(f"âœ… è´¨é‡åˆ†æå®Œæˆ")
                return result
            else:
                raise Exception("è´¨é‡åˆ†æå¤„ç†å¤±è´¥")
                
        except Exception as e:
            error_msg = f"è´¨é‡åˆ†ææ­¥éª¤å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            self.pipeline_state['errors'].append(error_msg)
            return {}
    
    def run_full_pipeline(self, input_file: str, output_dir: Path) -> Dict:
        """è¿è¡Œå®Œæ•´çš„æµæ°´çº¿"""
        print("ğŸš€ å¯åŠ¨å¢å¼ºçš„æ—¥å¿—åŠè‡ªåŠ¨åˆ†ç±»æµæ°´çº¿")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        pipeline_output_dir = output_dir / f"pipeline_{timestamp}"
        pipeline_output_dir.mkdir(exist_ok=True, parents=True)
        
        # è®¾ç½®æµæ°´çº¿çŠ¶æ€
        self.pipeline_state = {
            'current_step': 0,
            'total_steps': 6,
            'results': {},
            'errors': [],
            'start_time': datetime.now().isoformat(),
            'input_file': input_file,
            'output_dir': str(pipeline_output_dir)
        }
        
        try:
            # æ­¥éª¤1: æ¨¡æ¿åŒ–
            if self.pipeline_config['enable_templating']:
                self.pipeline_state['current_step'] = 1
                templating_result = self.step_1_template_logs(input_file, pipeline_output_dir)
                if not templating_result:
                    raise Exception("æ¨¡æ¿åŒ–æ­¥éª¤å¤±è´¥")
                
                # ä½¿ç”¨æ¨¡æ¿åŒ–ç»“æœä½œä¸ºä¸‹ä¸€æ­¥è¾“å…¥
                next_input = templating_result['output_file']
            else:
                next_input = input_file
            
            # æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹
            if self.pipeline_config['enable_feature_engineering']:
                self.pipeline_state['current_step'] = 2
                feature_result = self.step_2_extract_features(next_input, pipeline_output_dir)
                if not feature_result:
                    raise Exception("ç‰¹å¾å·¥ç¨‹æ­¥éª¤å¤±è´¥")
            
            # æ­¥éª¤3: é¢„åˆ†ç±»
            self.pipeline_state['current_step'] = 3
            preclass_result = self.step_3_pre_classify(input_file, pipeline_output_dir)
            if not preclass_result:
                raise Exception("é¢„åˆ†ç±»æ­¥éª¤å¤±è´¥")
            
            # æ­¥éª¤4: è‡ªåŠ¨æ ‡ç­¾
            if self.pipeline_config['enable_ml_classification']:
                self.pipeline_state['current_step'] = 4
                # ä½¿ç”¨é¢„åˆ†ç±»ç»“æœ
                preclass_files = list(pipeline_output_dir.rglob("*_classified.csv"))
                if preclass_files:
                    auto_label_result = self.step_4_auto_label(str(preclass_files[0]), pipeline_output_dir)
                    if not auto_label_result:
                        raise Exception("è‡ªåŠ¨æ ‡ç­¾æ­¥éª¤å¤±è´¥")
                    
                    # ä½¿ç”¨è‡ªåŠ¨æ ‡ç­¾ç»“æœä½œä¸ºä¸‹ä¸€æ­¥è¾“å…¥
                    review_input = auto_label_result['output_file']
                else:
                    raise Exception("æœªæ‰¾åˆ°é¢„åˆ†ç±»ç»“æœæ–‡ä»¶")
            else:
                # è·³è¿‡è‡ªåŠ¨æ ‡ç­¾ï¼Œä½¿ç”¨é¢„åˆ†ç±»ç»“æœ
                preclass_files = list(pipeline_output_dir.rglob("*_classified.csv"))
                if preclass_files:
                    review_input = str(preclass_files[0])
                else:
                    raise Exception("æœªæ‰¾åˆ°é¢„åˆ†ç±»ç»“æœæ–‡ä»¶")
            
            # æ­¥éª¤5: äººå·¥å®¡æŸ¥
            self.pipeline_state['current_step'] = 5
            review_result = self.step_5_human_review(review_input, pipeline_output_dir)
            
            # ç¡®å®šæœ€ç»ˆæ–‡ä»¶
            if review_result.get('success'):
                final_file = review_result['output_file']
            else:
                final_file = review_input
            
            # æ­¥éª¤6: è´¨é‡åˆ†æ
            if self.pipeline_config['enable_quality_analysis']:
                self.pipeline_state['current_step'] = 6
                quality_result = self.step_6_quality_analysis(final_file, pipeline_output_dir)
            
            # å®Œæˆæµæ°´çº¿
            self.pipeline_state['end_time'] = datetime.now().isoformat()
            self.pipeline_state['status'] = 'completed'
            
            # ç”Ÿæˆæµæ°´çº¿æŠ¥å‘Š
            self.generate_pipeline_report(pipeline_output_dir)
            
            print("\nğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {pipeline_output_dir}")
            
            return self.pipeline_state
            
        except Exception as e:
            self.pipeline_state['status'] = 'failed'
            self.pipeline_state['end_time'] = datetime.now().isoformat()
            self.pipeline_state['errors'].append(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            
            print(f"\nâŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            print(f"ğŸ“ éƒ¨åˆ†ç»“æœä¿å­˜åœ¨: {pipeline_output_dir}")
            
            # ç”Ÿæˆé”™è¯¯æŠ¥å‘Š
            self.generate_pipeline_report(pipeline_output_dir)
            
            return self.pipeline_state
    
    def run_template_only(self, input_file: str, output_dir: Path) -> Dict:
        """ä»…è¿è¡Œæ¨¡æ¿åŒ–æ­¥éª¤"""
        print("ğŸš€ å¯åŠ¨æ¨¡æ¿åŒ–æµæ°´çº¿")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        pipeline_output_dir = output_dir / f"template_only_{timestamp}"
        pipeline_output_dir.mkdir(exist_ok=True, parents=True)
        
        try:
            result = self.step_1_template_logs(input_file, pipeline_output_dir)
            if result:
                print(f"\nğŸ‰ æ¨¡æ¿åŒ–å®Œæˆï¼")
                print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {pipeline_output_dir}")
                return result
            else:
                raise Exception("æ¨¡æ¿åŒ–å¤±è´¥")
                
        except Exception as e:
            print(f"\nâŒ æ¨¡æ¿åŒ–å¤±è´¥: {e}")
            return {}
    
    def run_batch_pipeline(self, input_dir: str, output_dir: Path) -> Dict:
        """æ‰¹é‡è¿è¡Œæµæ°´çº¿"""
        print("ğŸš€ å¯åŠ¨æ‰¹é‡æµæ°´çº¿")
        
        input_path = Path(input_dir)
        if not input_path.exists():
            print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return {}
        
        # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
        log_extensions = ['.log', '.txt', '.csv', '.out', '.err']
        log_files = []
        
        for ext in log_extensions:
            log_files.extend(input_path.rglob(f"*{ext}"))
        
        if not log_files:
            print("âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
            return {}
        
        print(f"ğŸ“ æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
        
        # åˆ›å»ºæ‰¹é‡è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        batch_output_dir = output_dir / f"batch_pipeline_{timestamp}"
        batch_output_dir.mkdir(exist_ok=True, parents=True)
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        results = []
        for i, log_file in enumerate(log_files, 1):
            print(f"\n{'='*60}")
            print(f"å¤„ç†è¿›åº¦: {i}/{len(log_files)} - {log_file.name}")
            print('='*60)
            
            try:
                result = self.run_full_pipeline(str(log_file), batch_output_dir)
                if result.get('status') == 'completed':
                    results.append({
                        'file': str(log_file),
                        'status': 'success',
                        'result': result
                    })
                else:
                    results.append({
                        'file': str(log_file),
                        'status': 'failed',
                        'errors': result.get('errors', [])
                    })
                    
            except Exception as e:
                results.append({
                    'file': str(log_file),
                    'status': 'error',
                    'error': str(e)
                })
        
        # ç”Ÿæˆæ‰¹é‡å¤„ç†æŠ¥å‘Š
        self.generate_batch_report(batch_output_dir, results)
        
        success_count = len([r for r in results if r['status'] == 'success'])
        print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {success_count}/{len(log_files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {batch_output_dir}")
        
        return {
            'total_files': len(log_files),
            'success_count': success_count,
            'results': results,
            'output_dir': str(batch_output_dir)
        }
    
    def generate_pipeline_report(self, output_dir: Path):
        """ç”Ÿæˆæµæ°´çº¿æ‰§è¡ŒæŠ¥å‘Š"""
        report_file = output_dir / "pipeline_execution_report.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("å¢å¼ºæ—¥å¿—åŠè‡ªåŠ¨åˆ†ç±»æµæ°´çº¿æ‰§è¡ŒæŠ¥å‘Š\n")
            f.write("=" * 60 + "\n")
            f.write(f"æ‰§è¡Œæ—¶é—´: {self.pipeline_state.get('start_time', 'N/A')} - {self.pipeline_state.get('end_time', 'N/A')}\n")
            f.write(f"è¾“å…¥æ–‡ä»¶: {self.pipeline_state.get('input_file', 'N/A')}\n")
            f.write(f"è¾“å‡ºç›®å½•: {self.pipeline_state.get('output_dir', 'N/A')}\n")
            f.write(f"æ‰§è¡ŒçŠ¶æ€: {self.pipeline_state.get('status', 'N/A')}\n")
            f.write(f"å½“å‰æ­¥éª¤: {self.pipeline_state.get('current_step', 0)}/{self.pipeline_state.get('total_steps', 0)}\n\n")
            
            # å„æ­¥éª¤ç»“æœ
            f.write("æ­¥éª¤æ‰§è¡Œç»“æœ:\n")
            f.write("-" * 30 + "\n")
            
            steps = [
                ('templating', 'æ¨¡æ¿åŒ–'),
                ('feature_engineering', 'ç‰¹å¾å·¥ç¨‹'),
                ('pre_classification', 'é¢„åˆ†ç±»'),
                ('auto_labeling', 'è‡ªåŠ¨æ ‡ç­¾'),
                ('human_review', 'äººå·¥å®¡æŸ¥'),
                ('quality_analysis', 'è´¨é‡åˆ†æ')
            ]
            
            for step_key, step_name in steps:
                if step_key in self.pipeline_state['results']:
                    result = self.pipeline_state['results'][step_key]
                    f.write(f"âœ… {step_name}: æˆåŠŸ\n")
                    if isinstance(result, dict):
                        for key, value in result.items():
                            if key not in ['output_file', 'success']:
                                f.write(f"   {key}: {value}\n")
                else:
                    f.write(f"â­ï¸ {step_name}: è·³è¿‡\n")
            
            # é”™è¯¯ä¿¡æ¯
            if self.pipeline_state['errors']:
                f.write(f"\né”™è¯¯ä¿¡æ¯:\n")
                f.write("-" * 30 + "\n")
                for i, error in enumerate(self.pipeline_state['errors'], 1):
                    f.write(f"{i}. {error}\n")
            
            # é…ç½®ä¿¡æ¯
            f.write(f"\næµæ°´çº¿é…ç½®:\n")
            f.write("-" * 30 + "\n")
            for key, value in self.pipeline_config.items():
                f.write(f"{key}: {value}\n")
        
        print(f"ğŸ“„ æµæ°´çº¿æŠ¥å‘Š: {report_file}")
    
    def generate_batch_report(self, output_dir: Path, results: List[Dict]):
        """ç”Ÿæˆæ‰¹é‡å¤„ç†æŠ¥å‘Š"""
        report_file = output_dir / "batch_processing_report.txt"
        
        success_count = len([r for r in results if r['status'] == 'success'])
        failed_count = len([r for r in results if r['status'] == 'failed'])
        error_count = len([r for r in results if r['status'] == 'error'])
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("æ‰¹é‡æµæ°´çº¿å¤„ç†æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ–‡ä»¶æ•°: {len(results)}\n")
            f.write(f"æˆåŠŸå¤„ç†: {success_count}\n")
            f.write(f"å¤„ç†å¤±è´¥: {failed_count}\n")
            f.write(f"æ‰§è¡Œé”™è¯¯: {error_count}\n\n")
            
            # è¯¦ç»†ç»“æœ
            f.write("è¯¦ç»†ç»“æœ:\n")
            f.write("-" * 30 + "\n")
            for i, result in enumerate(results, 1):
                f.write(f"{i}. {Path(result['file']).name}\n")
                f.write(f"   çŠ¶æ€: {result['status']}\n")
                if result['status'] == 'error':
                    f.write(f"   é”™è¯¯: {result['error']}\n")
                elif result['status'] == 'failed':
                    f.write(f"   é”™è¯¯: {', '.join(result['errors'])}\n")
                f.write("\n")
        
        print(f"ğŸ“„ æ‰¹é‡å¤„ç†æŠ¥å‘Š: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºçš„æ—¥å¿—åŠè‡ªåŠ¨åˆ†ç±»æµæ°´çº¿')
    parser.add_argument('--input-file', help='è¾“å…¥æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--input-dir', help='è¾“å…¥ç›®å½•è·¯å¾„ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰')
    parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--mode', choices=['full', 'template-only', 'batch'], 
                       default='full', help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--skip-human-review', action='store_true', 
                       help='è·³è¿‡äººå·¥å®¡æŸ¥æ­¥éª¤')
    parser.add_argument('--skip-templating', action='store_true', 
                       help='è·³è¿‡æ¨¡æ¿åŒ–æ­¥éª¤')
    parser.add_argument('--skip-feature-engineering', action='store_true', 
                       help='è·³è¿‡ç‰¹å¾å·¥ç¨‹æ­¥éª¤')
    parser.add_argument('--skip-ml', action='store_true', 
                       help='è·³è¿‡æœºå™¨å­¦ä¹ åˆ†ç±»')
    parser.add_argument('--skip-quality-analysis', action='store_true', 
                       help='è·³è¿‡è´¨é‡åˆ†æ')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµæ°´çº¿
    pipeline = EnhancedLogPipeline()
    
    # é…ç½®æµæ°´çº¿
    config = {
        'enable_human_review': not args.skip_human_review,
        'enable_templating': not args.skip_templating,
        'enable_feature_engineering': not args.skip_feature_engineering,
        'enable_ml_classification': not args.skip_ml,
        'enable_quality_analysis': not args.skip_quality_analysis
    }
    pipeline.configure_pipeline(config)
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = pipeline.output_base_dir / "enhanced_pipeline"
    
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # æ ¹æ®æ¨¡å¼è¿è¡Œ
    if args.mode == 'full':
        if not args.input_file:
            print("âŒ å®Œæ•´æ¨¡å¼éœ€è¦æŒ‡å®š --input-file")
            return
        
        if not Path(args.input_file).exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
            return
        
        pipeline.run_full_pipeline(args.input_file, output_dir)
    
    elif args.mode == 'template-only':
        if not args.input_file:
            print("âŒ æ¨¡æ¿åŒ–æ¨¡å¼éœ€è¦æŒ‡å®š --input-file")
            return
        
        if not Path(args.input_file).exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
            return
        
        pipeline.run_template_only(args.input_file, output_dir)
    
    elif args.mode == 'batch':
        if not args.input_dir:
            print("âŒ æ‰¹é‡æ¨¡å¼éœ€è¦æŒ‡å®š --input-dir")
            return
        
        if not Path(args.input_dir).exists():
            print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
            return
        
        pipeline.run_batch_pipeline(args.input_dir, output_dir)
    
    else:
        print("âŒ æ— æ•ˆçš„è¿è¡Œæ¨¡å¼")
        parser.print_help()

if __name__ == "__main__":
    main()
