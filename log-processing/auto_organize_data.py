#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†è„šæœ¬
åŠŸèƒ½ï¼š
1. è¯»å–DATA_OUTPUT/classified_dataä¸­çš„å·²åˆ†ç±»æ•°æ®æ–‡ä»¶
2. å»é™¤otherç±»åˆ«ï¼ŒæŒ‰ç±»åˆ«æ’åº
3. å°†åˆ†ç±»åçš„æ•°æ®å­˜å‚¨åˆ°å¯¹åº”çš„ç›®å½•ç»“æ„ä¸­
4. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
python auto_organize_data.py                           # å¤„ç†æ‰€æœ‰classified_dataæ–‡ä»¶
python auto_organize_data.py <æ–‡ä»¶å>                  # å¤„ç†æŒ‡å®šæ–‡ä»¶
python auto_organize_data.py --update-existing        # æ›´æ–°ç°æœ‰æ–‡ä»¶
"""

import pandas as pd
import numpy as np
import argparse
import sys
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')


class DataOrganizer:
    """æ•°æ®æ•´ç†å™¨"""
    
    def __init__(self):
        # å®šä¹‰ç›®å½•ç»“æ„
        self.base_dir = Path("DATA_OUTPUT")
        self.classified_data_dir = self.base_dir / "classified_data"
        self.categorized_logs_dir = self.base_dir / "categorized_logs"
        self.training_data_dir = self.base_dir / "training_data"
        self.summary_reports_dir = self.base_dir / "summary_reports"
        self.raw_data_dir = self.base_dir / "raw_data"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self._ensure_directories()
        
        # ç±»åˆ«ä¸­æ–‡æè¿°
        self.category_descriptions = {
            'stack_exception': 'å †æ ˆå¼‚å¸¸',
            'spring_boot_startup_failure': 'Spring Bootå¯åŠ¨å¤±è´¥',
            'auth_authorization': 'è®¤è¯æˆæƒ',
            'database_exception': 'æ•°æ®åº“å¼‚å¸¸',
            'connection_issue': 'è¿æ¥é—®é¢˜',
            'timeout': 'è¶…æ—¶é”™è¯¯',
            'memory_performance': 'å†…å­˜æ€§èƒ½',
            'config_environment': 'é…ç½®ç¯å¢ƒ',
            'business_logic': 'ä¸šåŠ¡é€»è¾‘',
            'normal_operation': 'æ­£å¸¸æ“ä½œ',
            'monitoring_heartbeat': 'ç›‘æ§å¿ƒè·³',
            'other': 'å…¶ä»–'
        }
    
    def _ensure_directories(self):
        """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        directories = [
            self.classified_data_dir,
            self.categorized_logs_dir,
            self.training_data_dir,
            self.summary_reports_dir,
            self.raw_data_dir
        ]
        
        for directory in directories:
            directory.mkdir(exist_ok=True)
            print(f"âœ… ç¡®ä¿ç›®å½•å­˜åœ¨: {directory}")
    
    def find_classified_files(self) -> List[Path]:
        """æŸ¥æ‰¾classified_dataç›®å½•ä¸­çš„å·²åˆ†ç±»æ–‡ä»¶"""
        if not self.classified_data_dir.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {self.classified_data_dir}")
            return []
        
        # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = list(self.classified_data_dir.glob("*.csv"))
        
        # è¿‡æ»¤æ‰å·²ç»å¤„ç†è¿‡çš„æ–‡ä»¶ï¼ˆåŒ…å«categorizedçš„æ–‡ä»¶ï¼‰
        classified_files = [f for f in csv_files if "categorized" not in f.name]
        
        print(f"ğŸ“ æ‰¾åˆ° {len(classified_files)} ä¸ªå¾…å¤„ç†çš„å·²åˆ†ç±»æ–‡ä»¶")
        for file in classified_files:
            print(f"  - {file.name}")
        
        return classified_files
    
    def detect_label_column(self, df: pd.DataFrame) -> str:
        """æ£€æµ‹æ ‡ç­¾åˆ—å"""
        possible_labels = ['content_type', 'final_label', 'label', 'category']
        for col in possible_labels:
            if col in df.columns:
                return col
        return None
    
    def process_single_file(self, file_path: Path, update_existing: bool = False) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        print(f"\nğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path.name}")
        print("-" * 60)
        
        try:
            # è¯»å–æ•°æ®
            df = pd.read_csv(file_path)
            print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
            
            # æ£€æµ‹æ ‡ç­¾åˆ—
            label_column = self.detect_label_column(df)
            if not label_column:
                print(f"âŒ æœªæ‰¾åˆ°æ ‡ç­¾åˆ—: {file_path.name}")
                return False
            
            print(f"ğŸ” ä½¿ç”¨æ ‡ç­¾åˆ—: {label_column}")
            
            # è¿‡æ»¤æ‰otherç±»åˆ«
            original_count = len(df)
            df_filtered = df[df[label_column] != 'other'].copy()
            filtered_count = len(df_filtered)
            
            print(f"ğŸ” è¿‡æ»¤åæ•°æ®: {filtered_count} æ¡è®°å½• (ç§»é™¤äº† {original_count - filtered_count} æ¡'other'è®°å½•)")
            
            if filtered_count == 0:
                print("âš ï¸ è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                return False
            
            # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
            category_counts = df_filtered[label_column].value_counts()
            print("\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:")
            for category, count in category_counts.items():
                percentage = (count / filtered_count) * 100
                description = self.category_descriptions.get(category, category)
                print(f"  {description} ({category}): {count} æ¡ ({percentage:.1f}%)")
            
            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # 1. ä¿å­˜åˆ†ç±»åçš„ä¸»æ–‡ä»¶åˆ°classified_data
            categorized_file = self.classified_data_dir / f"{file_path.stem}_categorized_{timestamp}.csv"
            df_filtered.to_csv(categorized_file, index=False, encoding='utf-8')
            print(f"ğŸ’¾ å·²ä¿å­˜åˆ†ç±»æ•°æ®åˆ°: {categorized_file.name}")
            
            # 2. æŒ‰ç±»åˆ«åˆ†åˆ«ä¿å­˜åˆ°categorized_logs
            self._save_categorized_files(df_filtered, label_column, timestamp)
            
            # 3. ç”Ÿæˆè®­ç»ƒæ•°æ®é›†åˆ°training_data
            self._save_training_data(df_filtered, label_column, timestamp)
            
            # 4. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šåˆ°summary_reports
            self._save_summary_report(df_filtered, label_column, file_path.name, timestamp)
            
            return True
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _save_categorized_files(self, df: pd.DataFrame, label_column: str, timestamp: str):
        """æŒ‰ç±»åˆ«åˆ†åˆ«ä¿å­˜æ–‡ä»¶"""
        print("\nğŸ“ ä¿å­˜åˆ†ç±»æ–‡ä»¶...")
        
        for category in df[label_column].unique():
            if category == 'other':
                continue
            
            category_df = df[df[label_column] == category].copy()
            description = self.category_descriptions.get(category, category)
            
            # åˆ›å»ºæ–‡ä»¶å
            category_file = self.categorized_logs_dir / f"{category}_{description}_{timestamp}.csv"
            
            # ä¿å­˜ç±»åˆ«æ•°æ®
            category_df.to_csv(category_file, index=False, encoding='utf-8')
            print(f"  âœ“ {description}: {category_file.name} ({len(category_df)} æ¡)")
    
    def _save_training_data(self, df: pd.DataFrame, label_column: str, timestamp: str):
        """ä¿å­˜è®­ç»ƒæ•°æ®é›†"""
        print("\nğŸ“š ç”Ÿæˆè®­ç»ƒæ•°æ®é›†...")
        
        # æ£€æµ‹æ–‡æœ¬åˆ—
        text_column = None
        possible_text_columns = ['original_log', 'message', 'content', 'text']
        for col in possible_text_columns:
            if col in df.columns:
                text_column = col
                break
        
        if not text_column:
            print("âš ï¸ æœªæ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œè·³è¿‡è®­ç»ƒæ•°æ®ç”Ÿæˆ")
            return
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        training_df = df[[text_column, label_column]].copy()
        training_df.columns = ['text', 'label']
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        training_file = self.training_data_dir / f"training_dataset_{timestamp}.csv"
        training_df.to_csv(training_file, index=False, encoding='utf-8')
        print(f"  âœ“ è®­ç»ƒæ•°æ®é›†: {training_file.name} ({len(training_df)} æ¡)")
        
        # ä¿å­˜åˆå¹¶æ•°æ®é›†
        combined_file = self.training_data_dir / f"combined_dataset_{timestamp}.csv"
        df.to_csv(combined_file, index=False, encoding='utf-8')
        print(f"  âœ“ åˆå¹¶æ•°æ®é›†: {combined_file.name} ({len(df)} æ¡)")
    
    def _save_summary_report(self, df: pd.DataFrame, label_column: str, original_filename: str, timestamp: str):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_count = len(df)
        category_counts = df[label_column].value_counts()
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_lines = []
        report_lines.append("ğŸ“ˆ æ—¥å¿—åˆ†ç±»æ±‡æ€»æŠ¥å‘Š")
        report_lines.append("=" * 50)
        report_lines.append(f"åŸå§‹æ–‡ä»¶: {original_filename}")
        report_lines.append(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"æ€»è®°å½•æ•°: {total_count}")
        report_lines.append("")
        report_lines.append("ç±»åˆ«åˆ†å¸ƒ:")
        
        for category, count in category_counts.items():
            percentage = (count / total_count) * 100
            description = self.category_descriptions.get(category, category)
            report_lines.append(f"  {description} ({category}): {count} æ¡ ({percentage:.1f}%)")
        
        report_lines.append("")
        report_lines.append("=" * 50)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.summary_reports_dir / f"{Path(original_filename).stem}_categorized_{timestamp}_summary.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"  âœ“ æ±‡æ€»æŠ¥å‘Š: {report_file.name}")
    
    def process_all_files(self, update_existing: bool = False):
        """å¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        classified_files = self.find_classified_files()
        
        if not classified_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¾…å¤„ç†çš„æ–‡ä»¶")
            return
        
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(classified_files)} ä¸ªæ–‡ä»¶")
        print("=" * 60)
        
        success_count = 0
        total_records = 0
        
        for file_path in classified_files:
            try:
                if self.process_single_file(file_path, update_existing):
                    success_count += 1
                    # ç»Ÿè®¡è®°å½•æ•°
                    df = pd.read_csv(file_path)
                    label_column = self.detect_label_column(df)
                    if label_column:
                        total_records += len(df[df[label_column] != 'other'])
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")
        
        print("\n" + "=" * 60)
        print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: {success_count}/{len(classified_files)} ä¸ªæ–‡ä»¶å¤„ç†æˆåŠŸ")
        print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {total_records} æ¡æœ‰æ•ˆè®°å½•")
        
        # ç”Ÿæˆæ€»ä½“æ±‡æ€»æŠ¥å‘Š
        self._generate_overall_summary()
    
    def _generate_overall_summary(self):
        """ç”Ÿæˆæ€»ä½“æ±‡æ€»æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ€»ä½“æ±‡æ€»æŠ¥å‘Š...")
        
        # ç»Ÿè®¡å„ä¸ªç›®å½•çš„æ–‡ä»¶
        categorized_files = list(self.categorized_logs_dir.glob("*.csv"))
        training_files = list(self.training_data_dir.glob("*.csv"))
        summary_files = list(self.summary_reports_dir.glob("*.txt"))
        
        # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
        overall_report = []
        overall_report.append("ğŸ“Š æ•°æ®æ•´ç†æ€»ä½“æ±‡æ€»")
        overall_report.append("=" * 50)
        overall_report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        overall_report.append("")
        overall_report.append("ğŸ“ ç›®å½•ç»Ÿè®¡:")
        overall_report.append(f"  åˆ†ç±»æ—¥å¿—æ–‡ä»¶: {len(categorized_files)} ä¸ª")
        overall_report.append(f"  è®­ç»ƒæ•°æ®æ–‡ä»¶: {len(training_files)} ä¸ª")
        overall_report.append(f"  æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶: {len(summary_files)} ä¸ª")
        overall_report.append("")
        overall_report.append("ğŸ“ˆ æœ€è¿‘å¤„ç†çš„æ–‡ä»¶:")
        
        # æ˜¾ç¤ºæœ€è¿‘çš„æ–‡ä»¶
        all_files = categorized_files + training_files + summary_files
        all_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        for file in all_files[:5]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ªæ–‡ä»¶
            mtime = datetime.fromtimestamp(file.stat().st_mtime)
            overall_report.append(f"  {file.name} ({mtime.strftime('%Y-%m-%d %H:%M')})")
        
        overall_report.append("")
        overall_report.append("=" * 50)
        
        # ä¿å­˜æ€»ä½“æŠ¥å‘Š
        overall_file = self.summary_reports_dir / f"overall_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(overall_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(overall_report))
        
        print(f"âœ… æ€»ä½“æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {overall_file.name}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†è„šæœ¬")
    parser.add_argument("file", nargs="?", help="æŒ‡å®šè¦å¤„ç†çš„æ–‡ä»¶")
    parser.add_argument("--update-existing", action="store_true", 
                       help="æ›´æ–°ç°æœ‰æ–‡ä»¶")
    
    args = parser.parse_args()
    
    organizer = DataOrganizer()
    
    if args.file:
        # å¤„ç†æŒ‡å®šæ–‡ä»¶
        file_path = Path(args.file)
        if not file_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            sys.exit(1)
        
        success = organizer.process_single_file(file_path, args.update_existing)
        sys.exit(0 if success else 1)
    else:
        # å¤„ç†æ‰€æœ‰æ–‡ä»¶
        organizer.process_all_files(args.update_existing)


if __name__ == "__main__":
    main() 