import re
import json
import argparse
from pathlib import Path
from typing import Optional, List, Dict
import pandas as pd
from datetime import datetime
import os
import glob

# ----------------------
# æ—¥å¿—è§£ææ­£åˆ™æ¨¡å¼
# ----------------------
# Listeneræ ¼å¼: æ—¶é—´æˆ³ [çº¿ç¨‹] çº§åˆ« ç±»è·¯å¾„ - æ¶ˆæ¯
LOG_PATTERN_LISTENER = re.compile(
    r'^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\s+\[(?P<thread>[^\]]+)\]\s+(?P<level>[A-Z]+)\s+(?P<classpath>.+?)\s+-\s+(?P<message>.+)$'
)

# Gatewayæ ¼å¼: æ—¶é—´æˆ³.æ¯«ç§’ çº§åˆ« è¿›ç¨‹ID --- [çº¿ç¨‹] ç±»è·¯å¾„ : æ¶ˆæ¯
LOG_PATTERN_GATEWAY = re.compile(
    r'^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3})\s+(?P<level>[A-Z]+)\s+\d+\s+---\s+\[(?P<thread>[^\]]+)\]\s+(?P<classpath>.+?)\s+:\s+(?P<message>.+)$'
)

# æ”¯æŒçš„æ—¥å¿—æ ¼å¼æ¨¡å¼åˆ—è¡¨
LOG_PATTERNS = [LOG_PATTERN_LISTENER, LOG_PATTERN_GATEWAY]

# ----------------------
# è‡ªå®šä¹‰è¿‡æ»¤é…ç½®
# ----------------------
ALLOWED_LEVELS = {"INFO", "ERROR", "WARN", "DEBUG", "TRACE"}  # å¯ä¿ç•™çš„æ—¥å¿—çº§åˆ«
INCLUDE_KEYWORDS = [
    # ç”¨æˆ·ç›¸å…³
    "user", "account", "login", "auth", "jwt", "token",
    # ç³»ç»Ÿç»„ä»¶
    "controller", "service", "repository", "config", "gateway", "security",
    # APIç›¸å…³
    "http", "api", "request", "response", "filter",
    # æ•°æ®ç›¸å…³
    "data", "database", "sql", "asset",
    # ç›‘æ§ç›¸å…³
    "alert", "dashboard", "audit", "runtime",
    # é€šç”¨å…³é”®è¯
    "getting", "creating", "updating", "deleting", "processing",
    # Gatewayç‰¹å®šå…³é”®è¯
    "validation", "verify", "success", "failed"
]  # ç±»è·¯å¾„æˆ–æ¶ˆæ¯ä¸­åŒ…å«ä»»ä¸€å…³é”®è¯å³ä¿ç•™


def parse_line(line: str) -> Optional[dict]:
    """å°†å•è¡Œæ—¥å¿—è§£æä¸ºç»“æ„åŒ–å­—æ®µ"""
    line_stripped = line.strip()
    
    # å°è¯•æ‰€æœ‰æ”¯æŒçš„æ—¥å¿—æ ¼å¼
    for pattern in LOG_PATTERNS:
        match = pattern.match(line_stripped)
        if match:
            result = match.groupdict()
            # ç»Ÿä¸€æ—¶é—´æˆ³æ ¼å¼ï¼ˆå»æ‰æ¯«ç§’éƒ¨åˆ†ä»¥ä¾¿ç»Ÿä¸€å¤„ç†ï¼‰
            if '.' in result['timestamp']:
                result['timestamp'] = result['timestamp'].split('.')[0]
            return result
    
    return None


def is_relevant(log: dict) -> bool:
    """åˆ¤æ–­æ—¥å¿—æ˜¯å¦æ»¡è¶³ä¿ç•™æ¡ä»¶"""
    if log["level"] not in ALLOWED_LEVELS:
        return False
    
    # æ£€æŸ¥ç±»è·¯å¾„æˆ–æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
    classpath_lower = log["classpath"].lower()
    message_lower = log["message"].lower()
    
    # æ›´å®½æ¾çš„è¿‡æ»¤æ¡ä»¶ï¼šåªè¦åŒ…å«ä»»ä¸€å…³é”®è¯å°±ä¿ç•™
    if any(k in classpath_lower or k in message_lower for k in INCLUDE_KEYWORDS):
        return True
    
    # é¢å¤–ä¿ç•™ä¸€äº›é‡è¦çš„æ—¥å¿—æ¨¡å¼
    if any(pattern in message_lower for pattern in [
        "error", "exception", "failed", "timeout", "connection",
        "started", "stopped", "shutdown", "startup", "success"
    ]):
        return True
    
    return False


def clean_log_file(input_path: Path, output_path: Path, to_json: bool = False):
    """ä¸»å¤„ç†å‡½æ•°ï¼šè§£ææ—¥å¿—ã€è¿‡æ»¤ã€è¾“å‡º"""
    parsed_logs = []

    with input_path.open("r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            parsed = parse_line(line)
            if parsed and is_relevant(parsed):
                # æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯
                parsed['source_file'] = input_path.name
                parsed['source_directory'] = input_path.parent.name
                parsed_logs.append(parsed)

    if not parsed_logs:
        print(f"âŒ {input_path.name}: æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ—¥å¿—")
        return 0

    df = pd.DataFrame(parsed_logs)

    if to_json:
        df.to_json(output_path, orient="records", force_ascii=False, indent=2)
    else:
        df.to_csv(output_path, index=False, encoding="utf-8-sig")
    
    print(f"âœ… {input_path.name}: å¤„ç†æœ‰æ•ˆæ—¥å¿— {len(df)} æ¡")
    return len(df)


def scan_data_directory(data_dir: Path) -> List[Path]:
    """æ‰«ædataç›®å½•ä¸‹çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
    log_files = []
    
    # æ”¯æŒçš„æ—¥å¿—æ–‡ä»¶æ‰©å±•å
    log_extensions = ['*.log', '*.txt', '*.out']
    
    for subdir in data_dir.iterdir():
        if subdir.is_dir() and subdir.name != "__pycache__":
            print(f"ğŸ“‚ æ‰«æç›®å½•: {subdir.name}")
            for ext in log_extensions:
                files = list(subdir.glob(ext))
                log_files.extend(files)
                if files:
                    print(f"   æ‰¾åˆ° {len(files)} ä¸ª {ext} æ–‡ä»¶")
    
    print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
    return log_files


def scan_specific_directory(target_dir: Path) -> List[Path]:
    """æ‰«ææŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
    log_files = []
    
    # æ”¯æŒçš„æ—¥å¿—æ–‡ä»¶æ‰©å±•å
    log_extensions = ['*.log', '*.txt', '*.out']
    
    if not target_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {target_dir}")
        return []
    
    print(f"ğŸ“‚ æ‰«æç›®å½•: {target_dir}")
    for ext in log_extensions:
        files = list(target_dir.glob(ext))
        log_files.extend(files)
        if files:
            print(f"   æ‰¾åˆ° {len(files)} ä¸ª {ext} æ–‡ä»¶")
    
    print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
    return log_files


def process_all_logs(data_dir: Path = None, to_json: bool = False):
    """å¤„ç†dataç›®å½•ä¸‹çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
    if data_dir is None:
        # é»˜è®¤ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•ä¸‹çš„dataæ–‡ä»¶å¤¹
        script_dir = Path(__file__).parent
        data_dir = script_dir.parent / "data"
    
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä»¥å½“å‰æ—¶é—´æˆ³å‘½åï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = data_dir / f"processed_{timestamp}"
    output_dir.mkdir(exist_ok=True)
    
    print(f"ğŸš€ å¼€å§‹å¤„ç†æ—¥å¿—æ–‡ä»¶...")
    print(f"ğŸ“ æ•°æ®ç›®å½•: {data_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("-" * 50)
    
    # æ‰«ææ‰€æœ‰æ—¥å¿—æ–‡ä»¶
    log_files = scan_data_directory(data_dir)
    
    if not log_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ—¥å¿—æ–‡ä»¶")
        return
    
    # å¤„ç†æ¯ä¸ªæ—¥å¿—æ–‡ä»¶
    total_logs = 0
    processed_count = 0
    summary = {}
    
    for log_file in log_files:
        source_name = f"{log_file.parent.name}_{log_file.stem}"
        output_file = output_dir / f"{source_name}_cleaned.{'json' if to_json else 'csv'}"
        
        try:
            count = clean_log_file(log_file, output_file, to_json)
            if count > 0:
                total_logs += count
                processed_count += 1
                summary[source_name] = count
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {log_file.name}: {e}")
    
    # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
    generate_summary_report(output_dir, summary, total_logs, processed_count)
    
    print("-" * 50)
    print(f"ğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š å¤„ç†äº† {processed_count} ä¸ªæ–‡ä»¶ï¼Œå…± {total_logs} æ¡æœ‰æ•ˆæ—¥å¿—")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")


def process_specific_directory(target_dir: Path, to_json: bool = False, output_base_dir: Path = None):
    """å¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
    if not target_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {target_dir}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if output_base_dir is None:
        # é»˜è®¤åœ¨ç›®æ ‡ç›®å½•çš„çˆ¶ç›®å½•ä¸‹åˆ›å»º
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = target_dir.parent / f"processed_{target_dir.name}_{timestamp}"
    else:
        # ä½¿ç”¨æŒ‡å®šçš„è¾“å‡ºåŸºç¡€ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = output_base_dir / f"processed_{target_dir.name}_{timestamp}"
    
    output_dir.mkdir(exist_ok=True, parents=True)
    
    print(f"ğŸš€ å¼€å§‹å¤„ç†æ—¥å¿—æ–‡ä»¶...")
    print(f"ğŸ“ ç›®æ ‡ç›®å½•: {target_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("-" * 50)
    
    # æ‰«ææŒ‡å®šç›®å½•çš„æ—¥å¿—æ–‡ä»¶
    log_files = scan_specific_directory(target_dir)
    
    if not log_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ—¥å¿—æ–‡ä»¶")
        return
    
    # å¤„ç†æ¯ä¸ªæ—¥å¿—æ–‡ä»¶
    total_logs = 0
    processed_count = 0
    summary = {}
    
    for log_file in log_files:
        source_name = f"{log_file.parent.name}_{log_file.stem}"
        output_file = output_dir / f"{source_name}_cleaned.{'json' if to_json else 'csv'}"
        
        try:
            count = clean_log_file(log_file, output_file, to_json)
            if count > 0:
                total_logs += count
                processed_count += 1
                summary[source_name] = count
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {log_file.name}: {e}")
    
    # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
    generate_summary_report(output_dir, summary, total_logs, processed_count)
    
    print("-" * 50)
    print(f"ğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š å¤„ç†äº† {processed_count} ä¸ªæ–‡ä»¶ï¼Œå…± {total_logs} æ¡æœ‰æ•ˆæ—¥å¿—")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")


def generate_summary_report(output_dir: Path, summary: Dict[str, int], total_logs: int, processed_count: int):
    """ç”Ÿæˆå¤„ç†æ‘˜è¦æŠ¥å‘Š"""
    report_file = output_dir / "processing_summary.txt"
    
    with report_file.open("w", encoding="utf-8") as f:
        f.write("=" * 60 + "\n")
        f.write("LogSense-XPU æ—¥å¿—å¤„ç†æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n")
        f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"å¤„ç†æ–‡ä»¶æ•°: {processed_count}\n")
        f.write(f"æœ‰æ•ˆæ—¥å¿—æ€»æ•°: {total_logs}\n\n")
        
        f.write("å„æ–‡ä»¶å¤„ç†è¯¦æƒ…:\n")
        f.write("-" * 40 + "\n")
        for source, count in summary.items():
            f.write(f"{source:<30} {count:>6} æ¡\n")
        
        f.write("\nè¿‡æ»¤æ¡ä»¶:\n")
        f.write(f"- æ—¥å¿—çº§åˆ«: {', '.join(ALLOWED_LEVELS)}\n")
        f.write(f"- å…³é”®è¯: {', '.join(INCLUDE_KEYWORDS)}\n")
    
    print(f"ğŸ“„ å¤„ç†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file.name}")


# ----------------------
# å‘½ä»¤è¡Œæ”¯æŒ
# ----------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="LogSense-XPU æ—¥å¿—æ¸…æ´—ä¸è¿‡æ»¤è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¤„ç†dataç›®å½•ä¸‹çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
  python clean_and_filter_logs.py --auto
  
  # å¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
  python clean_and_filter_logs.py --dir /path/to/target/directory
  
  # å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰
  python clean_and_filter_logs.py --input file.log --output result.csv
        """
    )
    
    # åˆ›å»ºäº’æ–¥å‚æ•°ç»„
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--auto", action="store_true", 
                      help="è‡ªåŠ¨å¤„ç†dataç›®å½•ä¸‹çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶")
    group.add_argument("--dir", type=Path,
                      help="å¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶")
    group.add_argument("--input", help="è¾“å…¥æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå•æ–‡ä»¶æ¨¡å¼ï¼‰")
    
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå•æ–‡ä»¶æ¨¡å¼å¿…éœ€ï¼‰")
    parser.add_argument("--output-dir", type=Path,
                       help="æŒ‡å®šè¾“å‡ºç›®å½•è·¯å¾„ï¼ˆç”¨äº--diræ¨¡å¼ï¼‰")
    parser.add_argument("--data-dir", type=Path, 
                       help="æŒ‡å®šdataç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ä¸º../dataï¼‰")
    parser.add_argument("--json", action="store_true", help="è¾“å‡ºä¸º JSON æ ¼å¼")
    
    args = parser.parse_args()
    
    if args.auto:
        # è‡ªåŠ¨æ¨¡å¼ï¼šå¤„ç†dataç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
        data_dir = args.data_dir if args.data_dir else None
        process_all_logs(data_dir, args.json)
    elif args.dir:
        # æŒ‡å®šç›®å½•æ¨¡å¼ï¼šå¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
        process_specific_directory(args.dir, args.json, args.output_dir)
    else:
        # å•æ–‡ä»¶æ¨¡å¼
        if not args.output:
            parser.error("å•æ–‡ä»¶æ¨¡å¼éœ€è¦æŒ‡å®š --output å‚æ•°")
        clean_log_file(Path(args.input), Path(args.output), args.json) 