#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠè‡ªåŠ¨æ—¥å¿—æ ‡ç­¾è¾…åŠ©å™¨
åŠŸèƒ½ï¼š
1. ä½¿ç”¨å…³é”®è¯è§„åˆ™è¿›è¡Œåˆæ­¥åˆ†ç±»
2. å¯é€‰åœ°ä½¿ç”¨TF-IDF + ç®€å•åˆ†ç±»å™¨
3. è¾“å‡ºå¸¦æœ‰predicted_labelåˆ—çš„CSVæ–‡ä»¶
4. æ”¯æŒäººå·¥æ ¡æ­£åçš„è¿­ä»£è®­ç»ƒ
5. ä¸å¸¦å‚æ•°æ—¶è‡ªåŠ¨æ‰¹é‡å¤„ç†DATA_OUTPUTç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶

ä½¿ç”¨æ–¹æ³•:
python auto_labeler.py                           # è‡ªåŠ¨æ‰¹é‡å¤„ç†DATA_OUTPUTç›®å½•
python auto_labeler.py <è¾“å…¥CSVæ–‡ä»¶è·¯å¾„>          # å¤„ç†æŒ‡å®šæ–‡ä»¶
python auto_labeler.py <æ–‡ä»¶è·¯å¾„> --use-ml       # ä½¿ç”¨æœºå™¨å­¦ä¹ 
"""

import pandas as pd
import numpy as np
import re
import argparse
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥æœºå™¨å­¦ä¹ åº“
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, accuracy_score
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸  scikit-learnæœªå®‰è£…ï¼Œå°†ä»…ä½¿ç”¨è§„åˆ™åˆ†ç±»")


class LogAutoLabeler:
    """æ—¥å¿—è‡ªåŠ¨æ ‡ç­¾å™¨"""
    
    def __init__(self):
        # å®šä¹‰æ ‡ç­¾åˆ†ç±»è§„åˆ™
        self.label_rules = {
            'auth_error': {
                'keywords': [
                    'token', 'unauthorized', 'authentication', 'auth', 'jwt', 
                    'login', 'logout', 'permission', 'access denied', '401', 
                    '403', 'forbidden', 'credentials', 'è®¤è¯', 'æˆæƒ', 'ä»¤ç‰Œ',
                    'security', 'authz', 'authn', 'oauth', 'credential'
                ],
                'description': 'ç™»å½•ã€æƒé™å¼‚å¸¸'
            },
            'db_error': {
                'keywords': [
                    'sqlexception', 'dataaccess', 'database', 'connection', 'sql',
                    'hibernate', 'mybatis', 'jdbc', 'mysql', 'oracle', 'postgres',
                    'deadlock', 'constraint', 'æ•°æ®åº“', 'DB', 'datasource',
                    'connectionpool', 'transaction', 'rollback', 'commit'
                ],
                'description': 'æ•°æ®åº“å¼‚å¸¸'
            },
            'timeout': {
                'keywords': [
                    'timeout', 'timed out', 'time out', 'timeoutexception',
                    'read timeout', 'connect timeout', 'socket timeout',
                    'connection timeout', 'è¶…æ—¶', 'socketread', 'sockettimeout'
                ],
                'description': 'è¶…æ—¶ç±»å¼‚å¸¸'
            },
            'api_success': {
                'keywords': [
                    'response=success', 'status=200', 'completed successfully',
                    'request processed', 'operation successful', 'æˆåŠŸ',
                    'response success', '200 ok', 'successfully processed'
                ],
                'level_filter': ['INFO'],
                'description': 'æ­£å¸¸APIè¯·æ±‚'
            },
            'ignore': {
                'keywords': [
                    'heartbeat', 'healthcheck', 'ping', 'metrics', 'actuator',
                    'health', 'monitoring', 'probe', 'keepalive', 'å¿ƒè·³',
                    'status check', 'alive', 'health-check', 'prometheus'
                ],
                'description': 'å¯å¿½ç•¥çš„å¿ƒè·³æ£€æµ‹'
            },
            'system_error': {
                'keywords': [
                    'nullpointerexception', 'nullpointer', 'npe', '500', 'exception',
                    'error', 'runtimeexception', 'illegalargument', 'outofmemory',
                    'stacktrace', 'caused by', 'java.lang', 'ç³»ç»Ÿé”™è¯¯',
                    'internal server error', 'server error'
                ],
                'level_filter': ['ERROR', 'FATAL'],
                'description': 'ç³»ç»Ÿçº§é”™è¯¯'
            },
            'network_error': {
                'keywords': [
                    'connection refused', 'connection reset', 'network', 'socket',
                    'host unreachable', 'connection failed', 'è¿æ¥å¤±è´¥', 'ç½‘ç»œ',
                    'connectexception', 'unknownhost', 'no route to host'
                ],
                'description': 'ç½‘ç»œè¿æ¥å¼‚å¸¸'
            },
            'performance': {
                'keywords': [
                    'slow query', 'performance', 'latency', 'response time',
                    'execution time', 'memory usage', 'cpu', 'æ€§èƒ½', 'ç¼“æ…¢',
                    'gc', 'garbage collection', 'memory leak'
                ],
                'description': 'æ€§èƒ½ç›¸å…³'
            }
        }
        
        self.ml_model = None
        self.vectorizer = None
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        self.output_base_dir = Path(__file__).parent.parent / "log-processing-OUTPUT"
        self.data_output_dir = Path(__file__).parent.parent / "DATA_OUTPUT"
        
    def classify_by_rules(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨è§„åˆ™å¯¹æ—¥å¿—è¿›è¡Œåˆ†ç±»"""
        print("ğŸ” å¼€å§‹åŸºäºè§„åˆ™çš„åˆ†ç±»...")
        
        # åˆ›å»ºpredicted_labelåˆ—
        df['predicted_label'] = 'other'
        df['confidence'] = 0.0
        df['rule_matched'] = ''
        
        # é¢„å¤„ç†æ–‡æœ¬ç”¨äºåŒ¹é…
        df['text_for_matching'] = ''
        for col in ['message', 'classpath', 'level']:
            if col in df.columns:
                df['text_for_matching'] += ' ' + df[col].fillna('').astype(str)
        df['text_for_matching'] = df['text_for_matching'].str.lower()
        
        # ç»Ÿè®¡å„æ ‡ç­¾å‘½ä¸­æ¬¡æ•°
        label_counts = {}
        
        # æŒ‰ä¼˜å…ˆçº§é¡ºåºåº”ç”¨è§„åˆ™ï¼ˆå…ˆå¤„ç†ç‰¹æ®Šæƒ…å†µï¼‰
        priority_order = ['ignore', 'api_success', 'auth_error', 'db_error', 
                         'timeout', 'network_error', 'performance', 'system_error']
        
        for label in priority_order:
            if label not in self.label_rules:
                continue
                
            rules = self.label_rules[label]
            keywords = rules['keywords']
            level_filter = rules.get('level_filter', [])
            
            # åˆ›å»ºå…³é”®è¯åŒ¹é…æ¡ä»¶
            keyword_pattern = '|'.join(re.escape(kw) for kw in keywords)
            mask = df['text_for_matching'].str.contains(keyword_pattern, regex=True, na=False)
            
            # å¦‚æœæœ‰çº§åˆ«è¿‡æ»¤ï¼Œæ·»åŠ çº§åˆ«æ¡ä»¶
            if level_filter and 'level' in df.columns:
                level_mask = df['level'].isin(level_filter)
                mask = mask & level_mask
            
            # åªæ›´æ–°è¿˜æœªåˆ†ç±»çš„è®°å½•
            update_mask = mask & (df['predicted_label'] == 'other')
            df.loc[update_mask, 'predicted_label'] = label
            df.loc[update_mask, 'confidence'] = 0.8  # è§„åˆ™åŒ¹é…ç»™è¾ƒé«˜ç½®ä¿¡åº¦
            df.loc[update_mask, 'rule_matched'] = f"keywords: {', '.join(keywords[:3])}"
            
            count = update_mask.sum()
            label_counts[label] = count
            if count > 0:
                print(f"  âœ… {label}: {count} æ¡")
        
        # ç»Ÿè®¡ç»“æœ
        other_count = (df['predicted_label'] == 'other').sum()
        label_counts['other'] = other_count
        
        print(f"\nğŸ“Š è§„åˆ™åˆ†ç±»ç»“æœ:")
        for label, count in label_counts.items():
            percentage = count / len(df) * 100
            print(f"  {label}: {count} æ¡ ({percentage:.1f}%)")
        
        return df
    
    def train_ml_model(self, train_df: pd.DataFrame, text_column: str = 'message') -> bool:
        """è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹"""
        if not ML_AVAILABLE:
            print("âŒ scikit-learnæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æœºå™¨å­¦ä¹ åŠŸèƒ½")
            return False
            
        if 'label' not in train_df.columns:
            print("âŒ è®­ç»ƒæ•°æ®å¿…é¡»åŒ…å«'label'åˆ—")
            return False
        
        print("ğŸ¤– å¼€å§‹è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        texts = train_df[text_column].fillna('').astype(str)
        labels = train_df['label']
        
        # è¿‡æ»¤æ‰'other'æ ‡ç­¾ï¼ˆæ•°é‡å¯èƒ½å¤ªå¤šï¼‰
        mask = labels != 'other'
        texts = texts[mask]
        labels = labels[mask]
        
        if len(texts) < 10:
            print("âŒ è®­ç»ƒæ•°æ®å¤ªå°‘ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
            return False
        
        # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                texts, labels, test_size=0.2, random_state=42, stratify=labels
            )
        except ValueError:
            print("âš ï¸  æŸäº›ç±»åˆ«æ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•åˆ†å‰²æ•°æ®ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ")
            X_train, y_train = texts, labels
            X_test, y_test = texts[:min(10, len(texts))], labels[:min(10, len(labels))]
        
        # åˆ›å»ºç®¡é“
        self.ml_model = Pipeline([
            ('tfidf', TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))),
            ('classifier', MultinomialNB(alpha=0.1))
        ])
        
        # è®­ç»ƒæ¨¡å‹
        self.ml_model.fit(X_train, y_train)
        
        # è¯„ä¼°æ¨¡å‹
        y_pred = self.ml_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"  âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"  ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(X_train)} æ¡ï¼Œæµ‹è¯•æ ·æœ¬: {len(X_test)} æ¡")
        
        return True
    
    def predict_with_ml(self, df: pd.DataFrame, text_column: str = 'message') -> pd.DataFrame:
        """ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹"""
        if not self.ml_model:
            print("âŒ æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•è¿›è¡ŒMLé¢„æµ‹")
            return df
        
        print("ğŸ¤– ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹...")
        
        # åªå¯¹è§„åˆ™æœªåˆ†ç±»çš„è®°å½•ä½¿ç”¨ML
        other_mask = df['predicted_label'] == 'other'
        other_texts = df.loc[other_mask, text_column].fillna('').astype(str)
        
        if len(other_texts) == 0:
            print("  â„¹ï¸  æ‰€æœ‰è®°å½•å·²é€šè¿‡è§„åˆ™åˆ†ç±»ï¼Œæ— éœ€MLé¢„æµ‹")
            return df
        
        # é¢„æµ‹
        ml_predictions = self.ml_model.predict(other_texts)
        ml_probabilities = self.ml_model.predict_proba(other_texts)
        
        # æ›´æ–°é¢„æµ‹ç»“æœ
        df.loc[other_mask, 'predicted_label'] = ml_predictions
        df.loc[other_mask, 'confidence'] = ml_probabilities.max(axis=1)
        df.loc[other_mask, 'rule_matched'] = 'ML_model'
        
        ml_count = len(ml_predictions)
        print(f"  âœ… MLæ¨¡å‹é¢„æµ‹äº† {ml_count} æ¡è®°å½•")
        
        return df
    
    def generate_label_summary(self, df: pd.DataFrame, file_name: str = "") -> str:
        """ç”Ÿæˆæ ‡ç­¾åˆ†ç±»æ‘˜è¦"""
        summary = []
        summary.append("=" * 60)
        summary.append("ğŸ“Š æ—¥å¿—è‡ªåŠ¨æ ‡ç­¾åˆ†ç±»æ‘˜è¦")
        summary.append("=" * 60)
        if file_name:
            summary.append(f"æ–‡ä»¶: {file_name}")
        summary.append(f"æ€»æ—¥å¿—æ•°: {len(df)}")
        summary.append(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        summary.append("")
        
        # æ ‡ç­¾åˆ†å¸ƒ
        label_counts = df['predicted_label'].value_counts()
        summary.append("æ ‡ç­¾åˆ†å¸ƒ:")
        summary.append("-" * 30)
        for label, count in label_counts.items():
            percentage = count / len(df) * 100
            description = self.label_rules.get(label, {}).get('description', 'å…¶ä»–ç±»å‹')
            summary.append(f"{label:<15} {count:>6} æ¡ ({percentage:>5.1f}%) - {description}")
        
        summary.append("")
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        summary.append("ç½®ä¿¡åº¦ç»Ÿè®¡:")
        summary.append("-" * 30)
        high_conf = (df['confidence'] >= 0.7).sum()
        medium_conf = ((df['confidence'] >= 0.4) & (df['confidence'] < 0.7)).sum()
        low_conf = (df['confidence'] < 0.4).sum()
        
        summary.append(f"é«˜ç½®ä¿¡åº¦ (â‰¥0.7): {high_conf} æ¡ ({high_conf/len(df)*100:.1f}%)")
        summary.append(f"ä¸­ç½®ä¿¡åº¦ (0.4-0.7): {medium_conf} æ¡ ({medium_conf/len(df)*100:.1f}%)")
        summary.append(f"ä½ç½®ä¿¡åº¦ (<0.4): {low_conf} æ¡ ({low_conf/len(df)*100:.1f}%)")
        
        summary.append("")
        summary.append("ğŸ’¡ å»ºè®®:")
        summary.append("- é‡ç‚¹æ£€æŸ¥ä½ç½®ä¿¡åº¦çš„è®°å½•")
        summary.append("- 'other'ç±»å‹å¯èƒ½éœ€è¦è¡¥å……åˆ†ç±»è§„åˆ™")
        summary.append("- äººå·¥æ ¡æ­£åå¯ç”¨äºè®­ç»ƒæ›´å¥½çš„æ¨¡å‹")
        
        return "\n".join(summary)
    
    def process_single_file(self, input_file: str, output_dir: Path, 
                           use_ml: bool = False, train_data_file: str = None) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            print(f"\nğŸ“ å¤„ç†æ–‡ä»¶: {Path(input_file).name}")
            
            # è¯»å–æ•°æ®
            df = pd.read_csv(input_file, encoding='utf-8-sig')
            print(f"ğŸ“Š åŠ è½½äº† {len(df)} æ¡æ—¥å¿—è®°å½•")
            
            if len(df) == 0:
                print("âš ï¸  æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                return False
            
            # å¦‚æœæœ‰è®­ç»ƒæ•°æ®ï¼Œå…ˆè®­ç»ƒMLæ¨¡å‹
            if use_ml and train_data_file and Path(train_data_file).exists():
                try:
                    train_df = pd.read_csv(train_data_file, encoding='utf-8-sig')
                    print(f"ğŸ“š åŠ è½½è®­ç»ƒæ•°æ®: {len(train_df)} æ¡")
                    self.train_ml_model(train_df)
                except Exception as e:
                    print(f"âš ï¸  åŠ è½½è®­ç»ƒæ•°æ®å¤±è´¥ï¼Œä»…ä½¿ç”¨è§„åˆ™åˆ†ç±»: {e}")
            
            # è§„åˆ™åˆ†ç±»
            df = self.classify_by_rules(df)
            
            # MLåˆ†ç±»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if use_ml and self.ml_model:
                df = self.predict_with_ml(df)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            input_path = Path(input_file)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = output_dir / f"{input_path.stem}_labeled_{timestamp}.csv"
            
            # ä¿å­˜ç»“æœ
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"âœ… ç»“æœå·²ä¿å­˜: {output_file}")
            
            # ç”Ÿæˆæ‘˜è¦
            summary = self.generate_label_summary(df, input_path.name)
            
            # ä¿å­˜æ‘˜è¦æ–‡ä»¶
            summary_file = output_dir / f"{input_path.stem}_labeled_{timestamp}_summary.txt"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(summary)
            print(f"ğŸ“„ æ‘˜è¦å·²ä¿å­˜: {summary_file}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def find_csv_files(self) -> List[Path]:
        """æŸ¥æ‰¾DATA_OUTPUTç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶"""
        csv_files = []
        
        if not self.data_output_dir.exists():
            print(f"âŒ DATA_OUTPUTç›®å½•ä¸å­˜åœ¨: {self.data_output_dir}")
            return csv_files
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
        for csv_file in self.data_output_dir.rglob("*.csv"):
            # æ’é™¤å·²ç»æ ‡æ³¨è¿‡çš„æ–‡ä»¶
            if "_labeled_" not in csv_file.name:
                csv_files.append(csv_file)
        
        return csv_files
    
    def batch_process(self, use_ml: bool = False, train_data_file: str = None):
        """æ‰¹é‡å¤„ç†DATA_OUTPUTç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶"""
        print("ğŸš€ å¯åŠ¨æ‰¹é‡æ—¥å¿—æ ‡ç­¾å¤„ç†...")
        print(f"ğŸ“ æ‰«æç›®å½•: {self.data_output_dir}")
        
        # æŸ¥æ‰¾CSVæ–‡ä»¶
        csv_files = self.find_csv_files()
        
        if not csv_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„CSVæ–‡ä»¶")
            return
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶å¾…å¤„ç†")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = self.output_base_dir / f"batch_labeled_{timestamp}"
        output_dir.mkdir(exist_ok=True, parents=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        success_count = 0
        total_logs = 0
        
        for i, csv_file in enumerate(csv_files, 1):
            print(f"\n{'='*50}")
            print(f"å¤„ç†è¿›åº¦: {i}/{len(csv_files)}")
            
            if self.process_single_file(str(csv_file), output_dir, use_ml, train_data_file):
                success_count += 1
                # ç®€å•ç»Ÿè®¡æ—¥å¿—æ•°é‡
                try:
                    df = pd.read_csv(csv_file, encoding='utf-8-sig')
                    total_logs += len(df)
                except:
                    pass
        
        # ç”Ÿæˆæ€»ä½“æ‘˜è¦
        self.generate_batch_summary(output_dir, success_count, len(csv_files), total_logs)
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {success_count}/{len(csv_files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“Š æ€»æ—¥å¿—æ•°: {total_logs} æ¡")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    
    def generate_batch_summary(self, output_dir: Path, success_count: int, 
                              total_count: int, total_logs: int):
        """ç”Ÿæˆæ‰¹é‡å¤„ç†æ‘˜è¦"""
        summary = []
        summary.append("=" * 60)
        summary.append("ğŸ“Š æ‰¹é‡æ—¥å¿—æ ‡ç­¾å¤„ç†æ‘˜è¦")
        summary.append("=" * 60)
        summary.append(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        summary.append(f"æ‰«æç›®å½•: {self.data_output_dir}")
        summary.append(f"è¾“å‡ºç›®å½•: {output_dir}")
        summary.append(f"æˆåŠŸå¤„ç†: {success_count}/{total_count} ä¸ªæ–‡ä»¶")
        summary.append(f"æ€»æ—¥å¿—æ•°: {total_logs} æ¡")
        summary.append("")
        
        # åˆ—å‡ºå¤„ç†çš„æ–‡ä»¶
        labeled_files = list(output_dir.glob("*_labeled_*.csv"))
        summary.append("å¤„ç†ç»“æœæ–‡ä»¶:")
        summary.append("-" * 30)
        for labeled_file in labeled_files:
            summary.append(f"- {labeled_file.name}")
        
        summary.append("")
        summary.append("ğŸ’¡ åç»­æ­¥éª¤:")
        summary.append("1. æ£€æŸ¥å„ä¸ª *_labeled_*.csv æ–‡ä»¶")
        summary.append("2. äººå·¥æ ¡æ­£é”™è¯¯çš„æ ‡ç­¾")
        summary.append("3. ä½¿ç”¨æ ¡æ­£åçš„æ•°æ®è¿›è¡ŒMLè®­ç»ƒ")
        summary.append("4. é‡å¤å¤„ç†ä»¥æé«˜å‡†ç¡®ç‡")
        
        # ä¿å­˜æ‘˜è¦
        summary_file = output_dir / "batch_processing_summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("\n".join(summary))
        
        print(f"ğŸ“‹ æ‰¹é‡å¤„ç†æ‘˜è¦: {summary_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŠè‡ªåŠ¨æ—¥å¿—æ ‡ç­¾è¾…åŠ©å™¨')
    parser.add_argument('input_file', nargs='?', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™æ‰¹é‡å¤„ç†ï¼‰')
    parser.add_argument('--use-ml', action='store_true', help='ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹')
    parser.add_argument('--train-data', help='è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºMLæ¨¡å‹ï¼‰')
    parser.add_argument('--batch', action='store_true', help='å¼ºåˆ¶æ‰¹é‡å¤„ç†æ¨¡å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ ‡ç­¾å™¨
    labeler = LogAutoLabeler()
    
    # ç¡®å®šè¿è¡Œæ¨¡å¼
    if args.input_file and not args.batch:
        # å•æ–‡ä»¶æ¨¡å¼
        if not Path(args.input_file).exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
            return
        
        print("ğŸš€ å¯åŠ¨å•æ–‡ä»¶æ—¥å¿—æ ‡ç­¾å™¨...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = labeler.output_base_dir / f"single_labeled_{timestamp}"
        output_dir.mkdir(exist_ok=True, parents=True)
        
        # å¤„ç†æ–‡ä»¶
        success = labeler.process_single_file(
            args.input_file, output_dir, args.use_ml, args.train_data
        )
        
        if success:
            print("\nğŸ‰ å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    else:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        labeler.batch_process(args.use_ml, args.train_data)
    
    print("\nğŸ’¡ æ¥ä¸‹æ¥çš„æ­¥éª¤:")
    print("1. æ£€æŸ¥ç”Ÿæˆçš„ *_labeled_*.csv æ–‡ä»¶")
    print("2. åœ¨Excelä¸­äººå·¥æ ¡æ­£é”™è¯¯æ ‡ç­¾")
    print("3. å°†æ ¡æ­£åçš„æ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®æ”¹è¿›æ¨¡å‹")
    print("4. é‡å¤æ­¤è¿‡ç¨‹ä»¥æŒç»­ä¼˜åŒ–æ ‡ç­¾è´¨é‡")


if __name__ == "__main__":
    main() 