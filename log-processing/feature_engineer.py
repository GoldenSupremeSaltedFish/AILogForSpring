#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„ç‰¹å¾å·¥ç¨‹å·¥å…·
å®ç°ç»“æ„ç‰¹å¾+è¯­ä¹‰ç‰¹å¾çš„åŒé‡ç‰¹å¾æå–
æ”¯æŒæ¨¡æ¿ID embeddingã€å¼‚å¸¸å…³é”®å­—embeddingã€TF-IDFç­‰

ä½¿ç”¨æ–¹æ³•:
python feature_engineer.py --input-file templated_logs.csv --output-dir output/
python feature_engineer.py --batch --input-dir templated_logs/ --output-dir output/
"""

import pandas as pd
import numpy as np
import re
import json
import pickle
import argparse
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥æœºå™¨å­¦ä¹ åº“
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.preprocessing import LabelEncoder, OneHotEncoder
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸  scikit-learnæœªå®‰è£…ï¼Œå°†ä»…ä½¿ç”¨åŸºç¡€ç‰¹å¾å·¥ç¨‹")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸  LightGBMæœªå®‰è£…ï¼Œå°†ä½¿ç”¨å…¶ä»–åˆ†ç±»å™¨")

class FeatureEngineer:
    """ç‰¹å¾å·¥ç¨‹å™¨"""
    
    def __init__(self):
        # ç»“æ„ç‰¹å¾å®šä¹‰
        self.structural_features = {
            'log_level': ['ERROR', 'WARN', 'INFO', 'DEBUG', 'TRACE', 'FATAL'],
            'contains_stack': [True, False],
            'exception_type': [],  # åŠ¨æ€å¡«å……
            'file_path': [],  # åŠ¨æ€å¡«å……
            'function_name': [],  # åŠ¨æ€å¡«å……
        }
        
        # è¯­ä¹‰ç‰¹å¾é…ç½®
        self.semantic_config = {
            'tfidf_max_features': 1000,
            'tfidf_ngram_range': (1, 2),
            'template_id_buckets': 100,
            'exception_keyword_buckets': 50,
        }
        
        # ç‰¹å¾å­˜å‚¨
        self.feature_encoders = {}
        self.feature_stats = {}
        self.template_id_mapping = {}
        self.exception_keyword_mapping = {}
        
        # è¾“å‡ºç›®å½•é…ç½®
        self.output_base_dir = Path(__file__).parent.parent / "log-processing-OUTPUT"
    
    def extract_structural_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æå–ç»“æ„ç‰¹å¾"""
        print("ğŸ”§ æå–ç»“æ„ç‰¹å¾...")
        
        # 1. æ—¥å¿—çº§åˆ«ç‰¹å¾
        df['log_level'] = df['original_log'].apply(self._extract_log_level)
        
        # 2. å †æ ˆè·Ÿè¸ªç‰¹å¾
        df['contains_stack'] = df['original_log'].apply(self._has_stack_trace)
        
        # 3. å¼‚å¸¸ç±»å‹ç‰¹å¾
        df['exception_type'] = df['original_log'].apply(self._extract_exception_type)
        
        # 4. æ–‡ä»¶è·¯å¾„ç‰¹å¾
        df['file_path'] = df['original_log'].apply(self._extract_file_path)
        
        # 5. å‡½æ•°åç‰¹å¾
        df['function_name'] = df['original_log'].apply(self._extract_function_name)
        
        # 6. è¡Œå·ç‰¹å¾
        df['line_number'] = df['original_log'].apply(self._extract_line_number)
        
        # 7. æ—¥å¿—é•¿åº¦ç‰¹å¾
        df['log_length'] = df['original_log'].str.len()
        df['cleaned_length'] = df['cleaned_log'].str.len()
        df['compression_ratio'] = df['cleaned_length'] / (df['log_length'] + 1e-6)
        
        # 8. ç‰¹æ®Šå­—ç¬¦ç‰¹å¾
        df['has_quotes'] = df['original_log'].str.contains(r'["\']')
        df['has_brackets'] = df['original_log'].str.contains(r'[\[\]{}()]')
        df['has_numbers'] = df['original_log'].str.contains(r'\d')
        df['has_urls'] = df['original_log'].str.contains(r'https?://')
        df['has_emails'] = df['original_log'].str.contains(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        
        print(f"  âœ… æå–äº† {len([col for col in df.columns if col.startswith(('log_', 'contains_', 'has_', 'exception_', 'file_', 'function_', 'line_', 'compression_'))])} ä¸ªç»“æ„ç‰¹å¾")
        
        return df
    
    def extract_semantic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æå–è¯­ä¹‰ç‰¹å¾"""
        print("ğŸ§  æå–è¯­ä¹‰ç‰¹å¾...")
        
        # 1. æ¨¡æ¿IDç‰¹å¾
        df = self._create_template_id_features(df)
        
        # 2. å¼‚å¸¸å…³é”®å­—ç‰¹å¾
        df = self._create_exception_keyword_features(df)
        
        # 3. TF-IDFç‰¹å¾
        if ML_AVAILABLE:
            df = self._create_tfidf_features(df)
        
        # 4. æ–‡æœ¬ç»Ÿè®¡ç‰¹å¾
        df = self._create_text_statistics_features(df)
        
        print(f"  âœ… æå–äº†è¯­ä¹‰ç‰¹å¾")
        
        return df
    
    def _extract_log_level(self, log_line: str) -> str:
        """æå–æ—¥å¿—çº§åˆ«"""
        log_upper = log_line.upper()
        for level in ['ERROR', 'FATAL', 'WARN', 'INFO', 'DEBUG', 'TRACE']:
            if level in log_upper:
                return level
        return 'UNKNOWN'
    
    def _has_stack_trace(self, log_line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å †æ ˆè·Ÿè¸ª"""
        stack_indicators = ['at ', 'caused by', 'stack trace', 'exception in thread']
        return any(indicator in log_line.lower() for indicator in stack_indicators)
    
    def _extract_exception_type(self, log_line: str) -> str:
        """æå–å¼‚å¸¸ç±»å‹"""
        # åŒ¹é…Javaå¼‚å¸¸
        java_exception = re.search(r'(\w+Exception|\w+Error):', log_line)
        if java_exception:
            return java_exception.group(1)
        
        # åŒ¹é…å…¶ä»–å¼‚å¸¸æ¨¡å¼
        exception_patterns = [
            r'(\w+Exception)',
            r'(\w+Error)',
            r'Exception:\s*(\w+)',
            r'Error:\s*(\w+)'
        ]
        
        for pattern in exception_patterns:
            match = re.search(pattern, log_line)
            if match:
                return match.group(1)
        
        return 'None'
    
    def _extract_file_path(self, log_line: str) -> str:
        """æå–æ–‡ä»¶è·¯å¾„"""
        # åŒ¹é…Javaæ–‡ä»¶è·¯å¾„
        java_path = re.search(r'at\s+([a-zA-Z0-9_.]+\.java)', log_line)
        if java_path:
            return java_path.group(1)
        
        # åŒ¹é…å…¶ä»–æ–‡ä»¶è·¯å¾„
        path_patterns = [
            r'([a-zA-Z0-9_.]+\.(java|py|js|ts|go|rs|cpp|c|h))',
            r'([a-zA-Z]:\\[^:]+)',
            r'(/[^:]+)'
        ]
        
        for pattern in path_patterns:
            match = re.search(pattern, log_line)
            if match:
                return match.group(1)
        
        return 'None'
    
    def _extract_function_name(self, log_line: str) -> str:
        """æå–å‡½æ•°å"""
        # åŒ¹é…Javaæ–¹æ³•è°ƒç”¨
        java_method = re.search(r'at\s+[a-zA-Z0-9_.]+\.([a-zA-Z0-9_]+)', log_line)
        if java_method:
            return java_method.group(1)
        
        # åŒ¹é…å…¶ä»–å‡½æ•°æ¨¡å¼
        function_patterns = [
            r'([a-zA-Z0-9_]+)\s*\(',
            r'function\s+([a-zA-Z0-9_]+)',
            r'method\s+([a-zA-Z0-9_]+)'
        ]
        
        for pattern in function_patterns:
            match = re.search(pattern, log_line)
            if match:
                return match.group(1)
        
        return 'None'
    
    def _extract_line_number(self, log_line: str) -> int:
        """æå–è¡Œå·"""
        line_patterns = [
            r':(\d+)\)',
            r'line\s+(\d+)',
            r'at\s+line\s+(\d+)'
        ]
        
        for pattern in line_patterns:
            match = re.search(pattern, log_line)
            if match:
                return int(match.group(1))
        
        return 0
    
    def _create_template_id_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæ¨¡æ¿IDç‰¹å¾"""
        # æ¨¡æ¿ID one-hotç¼–ç 
        if 'template_id' in df.columns:
            # åˆ›å»ºæ¨¡æ¿IDæ˜ å°„
            unique_templates = df['template_id'].unique()
            self.template_id_mapping = {template: i for i, template in enumerate(unique_templates)}
            
            # åˆ›å»ºæ¨¡æ¿IDç‰¹å¾
            df['template_id_encoded'] = df['template_id'].map(self.template_id_mapping)
            
            # æ¨¡æ¿é¢‘ç‡ç‰¹å¾
            template_counts = df['template_id'].value_counts()
            df['template_frequency'] = df['template_id'].map(template_counts)
            df['template_frequency_log'] = np.log1p(df['template_frequency'])
        
        return df
    
    def _create_exception_keyword_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºå¼‚å¸¸å…³é”®å­—ç‰¹å¾"""
        if 'exception_keywords' in df.columns:
            # è§£æå¼‚å¸¸å…³é”®å­—
            all_keywords = set()
            for keywords_str in df['exception_keywords'].fillna('[]'):
                try:
                    if isinstance(keywords_str, str):
                        keywords = eval(keywords_str) if keywords_str.startswith('[') else [keywords_str]
                    else:
                        keywords = keywords_str
                    all_keywords.update(keywords)
                except:
                    continue
            
            # åˆ›å»ºå¼‚å¸¸å…³é”®å­—æ˜ å°„
            self.exception_keyword_mapping = {kw: i for i, kw in enumerate(all_keywords)}
            
            # åˆ›å»ºå¼‚å¸¸å…³é”®å­—ç‰¹å¾
            df['exception_count'] = df['exception_keywords'].apply(self._count_exceptions)
            df['has_exception'] = df['exception_count'] > 0
            
            # ä¸ºæ¯ä¸ªå¼‚å¸¸å…³é”®å­—åˆ›å»ºäºŒè¿›åˆ¶ç‰¹å¾
            for keyword in list(all_keywords)[:20]:  # é™åˆ¶å‰20ä¸ªæœ€å¸¸è§çš„
                df[f'exception_{keyword.replace(":", "_")}'] = df['exception_keywords'].apply(
                    lambda x: self._has_exception_keyword(x, keyword)
                )
        
        return df
    
    def _create_tfidf_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºTF-IDFç‰¹å¾"""
        if not ML_AVAILABLE:
            return df
        
        print("  ğŸ“Š åˆ›å»ºTF-IDFç‰¹å¾...")
        
        # ä½¿ç”¨æ¸…ç†åçš„æ—¥å¿—è¿›è¡ŒTF-IDF
        texts = df['cleaned_log'].fillna('').astype(str)
        
        # åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=self.semantic_config['tfidf_max_features'],
            ngram_range=self.semantic_config['tfidf_ngram_range'],
            stop_words='english',
            min_df=2,
            max_df=0.95
        )
        
        # æ‹Ÿåˆå’Œè½¬æ¢
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        
        # å°†TF-IDFç‰¹å¾æ·»åŠ åˆ°DataFrame
        feature_names = [f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])]
        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=df.index)
        
        # åˆå¹¶ç‰¹å¾
        df = pd.concat([df, tfidf_df], axis=1)
        
        print(f"  âœ… åˆ›å»ºäº† {tfidf_matrix.shape[1]} ä¸ªTF-IDFç‰¹å¾")
        
        return df
    
    def _create_text_statistics_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæ–‡æœ¬ç»Ÿè®¡ç‰¹å¾"""
        # è¯æ•°ç»Ÿè®¡
        df['word_count'] = df['cleaned_log'].str.split().str.len()
        df['char_count'] = df['cleaned_log'].str.len()
        df['avg_word_length'] = df['char_count'] / (df['word_count'] + 1e-6)
        
        # ç‰¹æ®Šå­—ç¬¦ç»Ÿè®¡
        df['digit_count'] = df['cleaned_log'].str.count(r'\d')
        df['uppercase_count'] = df['cleaned_log'].str.count(r'[A-Z]')
        df['lowercase_count'] = df['cleaned_log'].str.count(r'[a-z]')
        df['special_char_count'] = df['cleaned_log'].str.count(r'[^a-zA-Z0-9\s]')
        
        # æ ‡ç‚¹ç¬¦å·ç»Ÿè®¡
        df['comma_count'] = df['cleaned_log'].str.count(',')
        df['period_count'] = df['cleaned_log'].str.count('\.')
        df['colon_count'] = df['cleaned_log'].str.count(':')
        df['semicolon_count'] = df['cleaned_log'].str.count(';')
        
        return df
    
    def _count_exceptions(self, keywords_str) -> int:
        """è®¡ç®—å¼‚å¸¸å…³é”®å­—æ•°é‡"""
        try:
            if isinstance(keywords_str, str):
                keywords = eval(keywords_str) if keywords_str.startswith('[') else [keywords_str]
            else:
                keywords = keywords_str
            return len(keywords) if keywords else 0
        except:
            return 0
    
    def _has_exception_keyword(self, keywords_str, keyword) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šå¼‚å¸¸å…³é”®å­—"""
        try:
            if isinstance(keywords_str, str):
                keywords = eval(keywords_str) if keywords_str.startswith('[') else [keywords_str]
            else:
                keywords = keywords_str
            return keyword in keywords if keywords else False
        except:
            return False
    
    def create_feature_combinations(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºç‰¹å¾ç»„åˆ"""
        print("ğŸ”— åˆ›å»ºç‰¹å¾ç»„åˆ...")
        
        # 1. æ—¥å¿—çº§åˆ« + å †æ ˆè·Ÿè¸ª
        df['error_with_stack'] = (df['log_level'] == 'ERROR') & df['contains_stack']
        df['warn_with_stack'] = (df['log_level'] == 'WARN') & df['contains_stack']
        
        # 2. æ¨¡æ¿ID + TF-IDF (å¦‚æœæœ‰çš„è¯)
        if 'template_id_encoded' in df.columns and any(col.startswith('tfidf_') for col in df.columns):
            # é€‰æ‹©æœ€é‡è¦çš„TF-IDFç‰¹å¾è¿›è¡Œç»„åˆ
            tfidf_cols = [col for col in df.columns if col.startswith('tfidf_')]
            if tfidf_cols:
                top_tfidf = df[tfidf_cols].sum(axis=1)
                df['template_tfidf_interaction'] = df['template_id_encoded'] * top_tfidf
        
        # 3. å¼‚å¸¸ç±»å‹ + å‡½æ•°å
        df['exception_function'] = df['exception_type'] + '_' + df['function_name']
        
        # 4. æ—¥å¿—é•¿åº¦ + å‹ç¼©æ¯”
        df['length_compression'] = df['log_length'] * df['compression_ratio']
        
        print(f"  âœ… åˆ›å»ºäº†ç‰¹å¾ç»„åˆ")
        
        return df
    
    def prepare_ml_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """å‡†å¤‡æœºå™¨å­¦ä¹ ç‰¹å¾"""
        print("ğŸ¤– å‡†å¤‡æœºå™¨å­¦ä¹ ç‰¹å¾...")
        
        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        numeric_features = []
        categorical_features = []
        
        # ç»“æ„ç‰¹å¾
        structural_cols = ['log_length', 'cleaned_length', 'compression_ratio', 'word_count', 
                          'char_count', 'avg_word_length', 'digit_count', 'uppercase_count',
                          'lowercase_count', 'special_char_count', 'comma_count', 'period_count',
                          'colon_count', 'semicolon_count', 'line_number']
        
        for col in structural_cols:
            if col in df.columns:
                numeric_features.append(col)
        
        # å¸ƒå°”ç‰¹å¾
        boolean_cols = ['contains_stack', 'has_quotes', 'has_brackets', 'has_numbers', 
                       'has_urls', 'has_emails', 'has_exception', 'error_with_stack', 'warn_with_stack']
        
        for col in boolean_cols:
            if col in df.columns:
                numeric_features.append(col)
        
        # åˆ†ç±»ç‰¹å¾
        categorical_cols = ['log_level', 'exception_type', 'file_path', 'function_name']
        
        for col in categorical_cols:
            if col in df.columns:
                categorical_features.append(col)
        
        # TF-IDFç‰¹å¾
        tfidf_features = [col for col in df.columns if col.startswith('tfidf_')]
        numeric_features.extend(tfidf_features)
        
        # å¼‚å¸¸å…³é”®å­—ç‰¹å¾
        exception_features = [col for col in df.columns if col.startswith('exception_') and col != 'exception_count']
        numeric_features.extend(exception_features)
        
        # æ¨¡æ¿ç‰¹å¾
        template_features = ['template_id_encoded', 'template_frequency', 'template_frequency_log']
        for col in template_features:
            if col in df.columns:
                numeric_features.append(col)
        
        # åˆ›å»ºç‰¹å¾çŸ©é˜µ
        feature_cols = numeric_features + categorical_features
        available_cols = [col for col in feature_cols if col in df.columns]
        
        print(f"  âœ… é€‰æ‹©äº† {len(available_cols)} ä¸ªç‰¹å¾ç”¨äºæœºå™¨å­¦ä¹ ")
        print(f"  ğŸ“Š æ•°å€¼ç‰¹å¾: {len([col for col in available_cols if col in numeric_features])}")
        print(f"  ğŸ“Š åˆ†ç±»ç‰¹å¾: {len([col for col in available_cols if col in categorical_features])}")
        
        return df[available_cols], available_cols
    
    def train_classifier(self, df: pd.DataFrame, target_column: str = 'content_type') -> Dict:
        """è®­ç»ƒåˆ†ç±»å™¨"""
        if not ML_AVAILABLE:
            print("âŒ scikit-learnæœªå®‰è£…ï¼Œæ— æ³•è®­ç»ƒåˆ†ç±»å™¨")
            return {}
        
        print("ğŸ¯ è®­ç»ƒåˆ†ç±»å™¨...")
        
        # å‡†å¤‡ç‰¹å¾
        X, feature_names = self.prepare_ml_features(df)
        y = df[target_column]
        
        # è¿‡æ»¤æ‰'other'ç±»åˆ«
        mask = y != 'other'
        X = X[mask]
        y = y[mask]
        
        if len(X) < 10:
            print("âŒ è®­ç»ƒæ•°æ®å¤ªå°‘ï¼Œæ— æ³•è®­ç»ƒåˆ†ç±»å™¨")
            return {}
        
        # å¤„ç†åˆ†ç±»ç‰¹å¾
        categorical_features = [col for col in feature_names if col in ['log_level', 'exception_type', 'file_path', 'function_name']]
        
        # ç¼–ç åˆ†ç±»ç‰¹å¾
        for col in categorical_features:
            if col in X.columns:
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))
                self.feature_encoders[f'{col}_encoder'] = le
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # è®­ç»ƒæ¨¡å‹
        if LIGHTGBM_AVAILABLE:
            # ä½¿ç”¨LightGBM
            train_data = lgb.Dataset(X_train, label=y_train)
            val_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
            
            params = {
                'objective': 'multiclass',
                'num_class': len(y.unique()),
                'metric': 'multi_logloss',
                'boosting_type': 'gbdt',
                'num_leaves': 31,
                'learning_rate': 0.05,
                'feature_fraction': 0.9,
                'bagging_fraction': 0.8,
                'bagging_freq': 5,
                'verbose': -1
            }
            
            model = lgb.train(
                params,
                train_data,
                valid_sets=[val_data],
                num_boost_round=100,
                callbacks=[lgb.early_stopping(10)]
            )
            
            # é¢„æµ‹
            y_pred = model.predict(X_test)
            y_pred_labels = [list(y.unique())[i] for i in np.argmax(y_pred, axis=1)]
            
        else:
            # ä½¿ç”¨æœ´ç´ è´å¶æ–¯
            from sklearn.naive_bayes import MultinomialNB
            from sklearn.pipeline import Pipeline
            
            model = Pipeline([
                ('classifier', MultinomialNB(alpha=0.1))
            ])
            
            model.fit(X_train, y_train)
            y_pred_labels = model.predict(X_test)
        
        # è¯„ä¼°
        from sklearn.metrics import accuracy_score, classification_report
        
        accuracy = accuracy_score(y_test, y_pred_labels)
        report = classification_report(y_test, y_pred_labels, output_dict=True)
        
        print(f"  âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {accuracy:.3f}")
        
        return {
            'model': model,
            'feature_names': feature_names,
            'accuracy': accuracy,
            'classification_report': report,
            'X_test': X_test,
            'y_test': y_test,
            'y_pred': y_pred_labels
        }
    
    def process_file(self, input_file: str, output_dir: Path) -> Dict:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        print(f"ğŸ”„ å¤„ç†æ–‡ä»¶: {Path(input_file).name}")
        
        try:
            # è¯»å–æ•°æ®
            df = pd.read_csv(input_file, encoding='utf-8-sig')
            print(f"ğŸ“Š åŠ è½½äº† {len(df)} æ¡è®°å½•")
            
            # æå–ç»“æ„ç‰¹å¾
            df = self.extract_structural_features(df)
            
            # æå–è¯­ä¹‰ç‰¹å¾
            df = self.extract_semantic_features(df)
            
            # åˆ›å»ºç‰¹å¾ç»„åˆ
            df = self.create_feature_combinations(df)
            
            # è®­ç»ƒåˆ†ç±»å™¨ï¼ˆå¦‚æœæœ‰æ ‡ç­¾åˆ—ï¼‰
            model_results = {}
            if 'content_type' in df.columns:
                model_results = self.train_classifier(df)
            
            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            input_name = Path(input_file).stem
            
            # ä¿å­˜ç‰¹å¾æ•°æ®
            output_file = output_dir / f"{input_name}_features_{timestamp}.csv"
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            # ä¿å­˜æ¨¡å‹å’Œç¼–ç å™¨
            if model_results:
                model_file = output_dir / f"{input_name}_model_{timestamp}.pkl"
                with open(model_file, 'wb') as f:
                    pickle.dump({
                        'model': model_results['model'],
                        'feature_encoders': self.feature_encoders,
                        'template_id_mapping': self.template_id_mapping,
                        'exception_keyword_mapping': self.exception_keyword_mapping,
                        'feature_names': model_results['feature_names'],
                        'tfidf_vectorizer': getattr(self, 'tfidf_vectorizer', None)
                    }, f)
                
                # ä¿å­˜æ¨¡å‹è¯„ä¼°æŠ¥å‘Š
                report_file = output_dir / f"{input_name}_model_report_{timestamp}.json"
                with open(report_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'accuracy': model_results['accuracy'],
                        'classification_report': model_results['classification_report'],
                        'feature_count': len(model_results['feature_names']),
                        'training_time': datetime.now().isoformat()
                    }, f, ensure_ascii=False, indent=2)
            
            # ç”Ÿæˆç‰¹å¾ç»Ÿè®¡æŠ¥å‘Š
            self.generate_feature_report(df, output_dir, input_name, timestamp)
            
            print(f"âœ… å¤„ç†å®Œæˆ: {output_file}")
            
            return {
                'input_file': input_file,
                'output_file': str(output_file),
                'total_records': len(df),
                'feature_count': len(df.columns),
                'model_results': model_results
            }
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def generate_feature_report(self, df: pd.DataFrame, output_dir: Path, input_name: str, timestamp: str):
        """ç”Ÿæˆç‰¹å¾ç»Ÿè®¡æŠ¥å‘Š"""
        report_file = output_dir / f"{input_name}_feature_report_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("ç‰¹å¾å·¥ç¨‹ç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è¾“å…¥æ–‡ä»¶: {input_name}\n")
            f.write(f"æ€»è®°å½•æ•°: {len(df)}\n")
            f.write(f"ç‰¹å¾æ•°é‡: {len(df.columns)}\n\n")
            
            # ç‰¹å¾ç±»å‹ç»Ÿè®¡
            f.write("ç‰¹å¾ç±»å‹ç»Ÿè®¡:\n")
            f.write("-" * 30 + "\n")
            
            structural_features = [col for col in df.columns if any(col.startswith(prefix) for prefix in 
                ['log_', 'contains_', 'has_', 'exception_', 'file_', 'function_', 'line_', 'compression_'])]
            
            semantic_features = [col for col in df.columns if col.startswith('tfidf_')]
            
            template_features = [col for col in df.columns if 'template' in col]
            
            f.write(f"ç»“æ„ç‰¹å¾: {len(structural_features)}\n")
            f.write(f"è¯­ä¹‰ç‰¹å¾ (TF-IDF): {len(semantic_features)}\n")
            f.write(f"æ¨¡æ¿ç‰¹å¾: {len(template_features)}\n")
            f.write(f"å…¶ä»–ç‰¹å¾: {len(df.columns) - len(structural_features) - len(semantic_features) - len(template_features)}\n\n")
            
            # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæœ‰æ¨¡å‹ï¼‰
            if hasattr(self, 'feature_importance'):
                f.write("ç‰¹å¾é‡è¦æ€§ (Top 20):\n")
                f.write("-" * 30 + "\n")
                for feature, importance in self.feature_importance[:20]:
                    f.write(f"{feature}: {importance:.4f}\n")
        
        print(f"ğŸ“„ ç‰¹å¾æŠ¥å‘Š: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç‰¹å¾å·¥ç¨‹å·¥å…·')
    parser.add_argument('--input-file', help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--input-dir', help='è¾“å…¥ç›®å½•è·¯å¾„ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰')
    parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡å¤„ç†æ¨¡å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç‰¹å¾å·¥ç¨‹å™¨
    engineer = FeatureEngineer()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = engineer.output_base_dir / "feature_engineered"
    
    output_dir.mkdir(exist_ok=True, parents=True)
    
    if args.batch or args.input_dir:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        if not args.input_dir:
            print("âŒ æ‰¹é‡æ¨¡å¼éœ€è¦æŒ‡å®š --input-dir")
            return
        
        input_path = Path(args.input_dir)
        if not input_path.exists():
            print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
            return
        
        # æŸ¥æ‰¾CSVæ–‡ä»¶
        csv_files = list(input_path.rglob("*.csv"))
        if not csv_files:
            print("âŒ æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        # åˆ›å»ºæ‰¹é‡è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        batch_output_dir = output_dir / f"batch_features_{timestamp}"
        batch_output_dir.mkdir(exist_ok=True, parents=True)
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        results = []
        for i, csv_file in enumerate(csv_files, 1):
            print(f"\n{'='*50}")
            print(f"å¤„ç†è¿›åº¦: {i}/{len(csv_files)}")
            
            result = engineer.process_file(str(csv_file), batch_output_dir)
            if result:
                results.append(result)
        
        print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {batch_output_dir}")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {len(results)}/{len(csv_files)} ä¸ªæ–‡ä»¶")
    
    elif args.input_file:
        # å•æ–‡ä»¶æ¨¡å¼
        if not Path(args.input_file).exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        single_output_dir = output_dir / f"single_features_{timestamp}"
        single_output_dir.mkdir(exist_ok=True, parents=True)
        
        result = engineer.process_file(args.input_file, single_output_dir)
        if result:
            print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {single_output_dir}")
    
    else:
        print("âŒ è¯·æŒ‡å®š --input-file æˆ–ä½¿ç”¨ --batch --input-dir è¿›è¡Œæ‰¹é‡å¤„ç†")
        parser.print_help()

if __name__ == "__main__":
    main()
